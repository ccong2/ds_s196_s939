[
  {
    "objectID": "syllabus/index.html",
    "href": "syllabus/index.html",
    "title": "Applied Data Science for Cities",
    "section": "",
    "text": "Description\nUrban data science draws upon statistics, visualization, and computation to better understand and ultimately shape cities. This course emphasizes telling stories about cities using a set of descriptive approaches, quantitative and spatial analysis in R. We will learn how to describe community characteristics with small area census data, work with local administrative data, and think about how our analysis of quantitative data fit with other forms of data and engagement to fill in gaps in knowledge.\nLearning objectives:\n\nLearn - begin developing core data science competencies such as data wrangling, visualization, analysis, and communication;\nApply - apply concepts introduced in the readings and lectures to analyze datasets drawn from cities around the world;\nVisualize - manipulate tabular and geospatial data to produce intelligible and useful graphics for inclusion in documents and dissemination on the web;\nSynthesize - translate the results of visualization and analysis for use in decision-making and policy development.\n\n\nHow Will We Be Learning\nLecture: Class meetings are generally divided into lecture (Mondays) and laboratory sessions (Wednesdays) that focus on concepts and hand-on applications, respectively.\nLab: We will provide data science tutorials using R. Each lab tutorial aims to solve a specific urban data science problem in addition to building coding skills. Lab reports are due before the subsequent lab session and should be written independently.\nExtension readings: We provide a light material list that focuses on specific topics each week. These resources are meant to expand your knowledge and enhance your project completion capabilities.\nUrban data science project: The term project for the course will focus on integrating the tools of data science to explore a specific real-world planning issue or research question. This is a group project and students will define the scope of the project and identify specific deliverable(s) early in the semester. Reproducing an existing analysis or study using different datasets or an alternate study area is also acceptable for the term project.\n\n\nPrerequisites\nThis is a relatively fast-paced course so students can benefit from some prior knowledge working in R and RStudio. However, this is not a course prerequisite. Our first course sessions will focus on ensuring that we are all familiar with some of the basic work environment and methods which we'll make use of over the semester.\n\n\nAssessment\n\n\n\nAssignment\nWeight\n\n\nLab Reports\n50%\n\n\nProject Proposal\n10%\n\n\nProject Presentation\n30%\n\n\nAttendance\n10%\n\n\n\n\n\nKey Logistics\nLab Reports: You will be working on lab on most Wednesday classes and submit a short report before the next lab session. Details will be specified in each assignment submission page on Canvas.\nProject Proposal: Due Monday, Nov 18.\nThe purpose of this memo is to communicate the scope of your project (what you will do) and your strategies for collecting, visualizing, and analyzing data (how you will do it). A separate guideline will be distributed at the beginning of the course.\nProject Presentation: Week 8 (Dec 9 – Dec 11), class time. Presentations slides will due Dec 9.\n\n\nLate policy\nTo keep all students on a relatively level playing field, 5% will be deducted for late assignments, with an additional 5% deducted for each subsequent day. Late assignments two weeks after the due date will receive no credit and will not be accepted.\n\n\nSoftware\nWe use R and R Studio as the coding environment to develop analysis and applications. You will need to install both software on your personal computers from hereLinks to an external site. and hereLinks to an external site..\n\n\nCommunication\nPlan on using our class Slack channel, email, and office hours to get help with troubleshooting problems as they arise in your work. I also encourage you to work with others in the class to troubleshoot problems - it is highly likely that others in the class have encountered similar problems, and this also allows us to create a repository of our problems and responses.\nEmail: I check emails quite frequently, but I will not always be able to respond to emails right away. Please plan accordingly so that you don't miss deadlines.\nSlack: We have a Slack workspace that is accessible through Canvas for general communication, including homework Q&A, resource exchange, project collaboration, etc.\nOffice hours: Please consult the top of the syllabus for specific times. I will announce if there are any changes or exceptions. I'm happy to answer any specific coding questions, or chat and help shape the objective and scope of your projects.\n\n\nEthics\nAcademic Integrity: Violations to academic integrity are unacceptable at MIT and DUSP. Instances of misconduct include but are not limited to plagiarism, cheating, and deliberately unauthorized use of course data and material.\nCollaboration Policy: While team collaboration is encouraged, students should specify their roles and tasks in a project. A positive and constructive attitude toward teamwork is essential for the successful completion of the course.\nDiversity and Inclusion: MIT highly values a diverse, friendly, respectful, and inclusive learning environment among students, faculty, and staff. We welcome all individuals regardless of their origin, citizenship, gender identity, sexual orientation, or religious and political beliefs. Please contact me or departmental staff if you have any questions / considerations regarding this."
  },
  {
    "objectID": "labs/lab1.html",
    "href": "labs/lab1.html",
    "title": "Get started with ",
    "section": "",
    "text": "This exercise provides some more structured exploration and practice with Quarto Document. We will mix Markdown sections with code chunks, and experiment with working with a tabular dataset.\nLab 1 is not graded, and there’s no need to submit anything after you’ve completed it. But for labs after this one, you will complete them and submit your lab report, details will be specified in each lab document.",
    "crumbs": [
      "Labs",
      "01 Cambridge Building Energy: R, Quarto, dplyr essentials"
    ]
  },
  {
    "objectID": "labs/lab1.html#set-up-a-new-project",
    "href": "labs/lab1.html#set-up-a-new-project",
    "title": "Get started with ",
    "section": "Set up a New Project",
    "text": "Set up a New Project\nWe have talked about files paths in class, and the importance of setting a directory to work out of. Most of the time we can use setwd(), this works fine until when you want to move your working folder to another computer or share your work. If you write out a path in the code, it might not work on another computer if that directory does not exist.\nIn our labs, we are going to use R Projects to organize our works and make sure we don’t lose files. R projects organize all files related to a project in one place and setting up relative file paths. Let’s start by setting up a project for our exercise.\nLaunch RStudio, then click File - New Project… A dialog box will open up. Select New Directory, then New Project. Here, you can create a new folder to save everything related to this project. For example, I navigated to my D:/Fall24 folder and created a new folder there called Lab 1:\n\nClick the button “Create Project”. R will take a second to refresh. Then you will see in your Files tab that you have been directed to your current working directory: D:/Fall24/Lab 1. You will also see a .Rproj file in that folder.\n\nThe .Rproj file serves as a reference point that R uses to locate all files associated with the project. If you save all files related to Lab 1 in this folder, all relative paths remain intact and consistently applied.\nNote: In future sessions, I may provide you with a project folder containing data. As long as you launch RStudio by double-clicking the .Rproj file, you will be taken directly to the project’s home directory.",
    "crumbs": [
      "Labs",
      "01 Cambridge Building Energy: R, Quarto, dplyr essentials"
    ]
  },
  {
    "objectID": "labs/lab1.html#practice-formatting-text-with-quarto",
    "href": "labs/lab1.html#practice-formatting-text-with-quarto",
    "title": "Get started with ",
    "section": "Practice formatting text with Quarto",
    "text": "Practice formatting text with Quarto\nNow go to File - New File - Quarto Document to create a new Quarto document. The prompt shown below will appear. Type in a document title (e.g. Lab 1) and your name. Keep the radio button for HTML selected.\n\nYou will then see a template file. At the very top you will see the YAML (or “Yet Another Markdown Language”) header which begins and ends with three dashes ---. The YAML header determines how your document will be rendered to your desired output format. Now it specifies the title, author, output format and text editor.\nTo get an idea of how everything works, let’s click the “Render” button on top of your toolbar.\nWhen prompted, give this file a name, it will be saved in the folder where your “.Rproj” file is, as a .qmd file.\nYou will now see a formatted document in a web browser. Switch between your code and the document back and forth to see where each part of the code is placed in the rendered HTML file.\nNow we can add a new line to the YAML header:\n\ndate: &lt;insert the date that the file is created&gt;.\n\nRender it again and see where the difference is.\nThere can be other options specified in the YAML, particularly if you are rendering to a format other than HTML (such as pdf, or Word, see all formats).\nOn the very left of this toolbar, click the “Source” button to switch Markdown editing mode. These sections of text typically explain or provide context for the code and graphics, and they are formatted using Markdown syntax. For example:\n\n#: a header element.\n**: bold text.\n*: italic text.\n` : code blocks.\n\nOverall, the Visual interface looks pretty much like a Word document. There is a toolbar that allows you to make bold or italic text, create a bullet list, insert a link or an image, insert a code block, etc.\nNow let’s delete everything below the YAML header in the template file, so that we will start creating our own formatted report.\n\nYour practice\nIn 2014, the City of Cambridge passed a local ordinance on building energy use disclosure. Spend a few moments reviewing this website to become familiar with the ordinance (in general). Then, add a short paragraph below your YAML section that explain the following:\n\nWhat does the Building Energy Use Disclosure Ordinance require?\nWhat kind of data have been compiled and where to find them?\n\nYou may edit your text either in the “Source” or “Visual” panel, or toggle between them to get familiar with both. Make sure to make gratuitous use of bold, italics, bullet points, etc. in your text.\nWhen you finish, save your file and click Render again. You can immediately see your nicely formatted document in a web browser.",
    "crumbs": [
      "Labs",
      "01 Cambridge Building Energy: R, Quarto, dplyr essentials"
    ]
  },
  {
    "objectID": "labs/lab1.html#select-selects-a-subset-of-columns.",
    "href": "labs/lab1.html#select-selects-a-subset-of-columns.",
    "title": "Get started with ",
    "section": "Select: selects a subset of columns.",
    "text": "Select: selects a subset of columns.\nIn the energy dataset, we probably don’t need all of the 46 columns. So we can make it a smaller dataset by specifying a few columns to keep.\ndataset |&gt; select(Column1, Column2)\nInsert a new code chunk in your document like this one below. We will only keep 9 columns in the new dataset. You can type the pipe |&gt; operator in using Shift+Ctrl/Cmd+M.\n\nenergy &lt;- energy |&gt;\n  select(\n    `Data Year`,\n    `BEUDO Category`,\n    Owner,\n    `Year Built`,\n    `Primary Property Type - Self Selected`,\n    `Total GHG Emissions (Metric Tons CO2e)`,\n    `Total GHG Emissions Intensity (kgCO2e/ft2)`,\n    Longitude,\n    Latitude\n  ) \n\nSome of the column names are surrounded by backticks (`), that’s because they include special characters or spaces, which deviate from standard naming conventions. The use of backticks is a means of preserving these unique naming attributes. Just keep typing the column names, dplyr will populate the correct names for you.",
    "crumbs": [
      "Labs",
      "01 Cambridge Building Energy: R, Quarto, dplyr essentials"
    ]
  },
  {
    "objectID": "labs/lab1.html#filter-select-a-subset-of-rows",
    "href": "labs/lab1.html#filter-select-a-subset-of-rows",
    "title": "Get started with ",
    "section": "filter: Select a subset of rows",
    "text": "filter: Select a subset of rows\nNow let’s create a new dataset that only contains energy use records from MIT buildings.\ndataset |&gt; filter(&lt;condition&gt;)\nTake a look at how we achieve this using the following code:\n{r}\nenergy |&gt; \n  filter(Owner == \"MASSACHUSETTS INSTITUTE OF TECHNOLOGY\")\nViewing the result, you’ll notice that some entries are missing records for total GHG emissions, which appear as NA under the “Total GHG Emissions (Metric Tons CO2e)” column. If we want to simplify the dataset by keeping only the rows with valid GHG emission records, we can apply that as a filter condition too.\nProceed to insert a new code chunk in your document like the one below. Now we are filtering MIT buildings that have emission data, and we are assigning the result to a new variable “mit_energy”.\n\nmit_energy &lt;- energy |&gt; \n  filter(Owner == \"MASSACHUSETTS INSTITUTE OF TECHNOLOGY\") |&gt; \n  filter(!is.na(`Total GHG Emissions (Metric Tons CO2e)`))\n\nis.na() is a function commonly used to check whether each value in a column is missing (NA). The ! is a logical negation operator, so !is.na() checks for values that are not missing. It returns TRUE for non-missing values and FALSE for missing values.",
    "crumbs": [
      "Labs",
      "01 Cambridge Building Energy: R, Quarto, dplyr essentials"
    ]
  },
  {
    "objectID": "labs/lab1.html#group_by-summarise",
    "href": "labs/lab1.html#group_by-summarise",
    "title": "Get started with ",
    "section": "Group_by + Summarise",
    "text": "Group_by + Summarise\nSummarise is usually used in conjunction with group_by because the latter changes the scope from operating on the entire dataset to operating on it group-by-group. Go ahead and run the following code and observe the result:\n\nmit_energy |&gt; \n  group_by(`Data Year`) |&gt; \n  summarise(count = n())\n\n# A tibble: 8 × 2\n  `Data Year` count\n        &lt;dbl&gt; &lt;int&gt;\n1        2015   130\n2        2016   135\n3        2017   130\n4        2018   136\n5        2019   113\n6        2020   116\n7        2021   139\n8        2022   136\n\n\nWe now have a summary table that says, in our dataset, there are 130 records in 2015, 135 in 2016, and so on. We use group_by such that rows are grouped according to Data Year, which is the year when the energy record was taken. The result is then passed to summarise to count a total number of records per year. By default, the n() function creates a new column, which we here name as “count”.  \nBelow we are using the same group_by + summarise chain to calculate the average GHG emissions of all buildings, and the average GHG emission intensity. Pay attention to how we are giving new names to each of the new columns.\n\nmit_energy |&gt; \n  group_by(year = `Data Year`) |&gt; \n  summarise(count = n(),\n            avg_emission = mean(`Total GHG Emissions (Metric Tons CO2e)`),\n            avg_intensity = mean(`Total GHG Emissions Intensity (kgCO2e/ft2)`))\n\n# A tibble: 8 × 4\n   year count avg_emission avg_intensity\n  &lt;dbl&gt; &lt;int&gt;        &lt;dbl&gt;         &lt;dbl&gt;\n1  2015   130        1575.          13.4\n2  2016   135        1436.          13.2\n3  2017   130        1524.          13.5\n4  2018   136        1449.          13.1\n5  2019   113        1473.          12.6\n6  2020   116        1384.          10.9\n7  2021   139        1407.          11.4\n8  2022   136        1419.          11.4\n\n\n\nYour practice\nInsert a few new code chunks below this one to document your code and show your results. \n\nFrom the mit_energy dataset, create a subset of all non-residential buildings, which were built before the year 2000. (Hint: which function would you use?). How many such buildings are there?\nFrom the mit_energy dataset, compare the GHG emissions by property type (Hint: which column has that information?), and generate a table that shows the following results:\n\n\nYou can create this table mostly by modifying the sample code, however, there are a few small things you can experiment on:\n\nThe calculated average numbers in this table are rounded to 2 decimals, how to achieve that?\nThe table is arranged in descending order based on the “avg_emission” column, how to do that? (Hint)\n\nWe are already trying to ask questions and find hints of interesting stories from the dataset! If the results so far look interesting/surprising/expected to you, write a few sentences describing what you see from the analysis.\n\nLastly, and for fun, we will insert a map to complete your working document! The dataset we have includes “Longitude” and “Latitude” columns, which I love, because it indicates that location information is readily available and can be visualized.\nAdd the following code to your document, and you should be able to run it and see a map. (If your R says it can’t find mapview, run the line install.packages(\"mapview\"))\n\n\n#install.packages(\"mapview\")\nlibrary(mapview)\nmapview(\n  mit_energy,\n  xcol = \"Longitude\", ycol = \"Latitude\",\n  crs = 4326,\n  grid = FALSE\n)\n\n\n\n\n\n\nNow Save, Render your document again. You have now created a pretty, multi-media document using R!\n------\nIn this lab we have introduced how to create and develop a Quarto Document. We have also introduced some commonly-used dplyr functions including select, filter, group_by and summarise. This is the beginning of our data wrangling and leads to the work in Week 2.",
    "crumbs": [
      "Labs",
      "01 Cambridge Building Energy: R, Quarto, dplyr essentials"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Applied Data Science for Cities",
    "section": "",
    "text": "Urban science draws on statistics, visualization, and spatial analysis techniques to gain deeper insights into cities and actively contribute to their development. In this course, we’ll dive into the dynamic world of urban science by learning how to tell stories about cities and neighborhoods, covering a range of topics including demographic analysis, health and transportation, and using R as our primary quantitative analysis and interactive visualization tool.\n\n\n\n\n\n\n\nCourse Information\n\nSchedule: MW 9:30 - 11:00 AM, H2\nLocation: Mondays: 9-451; Wednesdays: 9-554\nCanvas Site: https://canvas.mit.edu/courses/27065"
  },
  {
    "objectID": "howto/setupr.html",
    "href": "howto/setupr.html",
    "title": "Applied Data Science for Cities",
    "section": "",
    "text": "Urban analytics draws upon statistics, visualization, and computation to better understand and ultimately shape cities. This course emphasizes telling stories about cities and neighborhoods covering a set of fundamental concepts of descriptive approaches, quantitative and spatial analysis in R, and principles of reproducible data analysis. Students learn to communicate the results of visualization and analysis for use in decision-making and policy development and to critique those processes.",
    "crumbs": [
      "Resources",
      "Set Up R and RStudio"
    ]
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "howto/git.html",
    "href": "howto/git.html",
    "title": "Applied Data Science for Cities",
    "section": "",
    "text": "Urban analytics draws upon statistics, visualization, and computation to better understand and ultimately shape cities. This course emphasizes telling stories about cities and neighborhoods covering a set of fundamental concepts of descriptive approaches, quantitative and spatial analysis in R, and principles of reproducible data analysis. Students learn to communicate the results of visualization and analysis for use in decision-making and policy development and to critique those processes.",
    "crumbs": [
      "Resources",
      "Start with GIT"
    ]
  },
  {
    "objectID": "howto/quartoweb.html",
    "href": "howto/quartoweb.html",
    "title": "Applied Data Science for Cities",
    "section": "",
    "text": "Urban analytics draws upon statistics, visualization, and computation to better understand and ultimately shape cities. This course emphasizes telling stories about cities and neighborhoods covering a set of fundamental concepts of descriptive approaches, quantitative and spatial analysis in R, and principles of reproducible data analysis. Students learn to communicate the results of visualization and analysis for use in decision-making and policy development and to critique those processes.",
    "crumbs": [
      "Resources",
      "Publish a Quarto Website"
    ]
  },
  {
    "objectID": "howto/startcode.html",
    "href": "howto/startcode.html",
    "title": "Applied Data Science for Cities",
    "section": "",
    "text": "Urban analytics draws upon statistics, visualization, and computation to better understand and ultimately shape cities. This course emphasizes telling stories about cities and neighborhoods covering a set of fundamental concepts of descriptive approaches, quantitative and spatial analysis in R, and principles of reproducible data analysis. Students learn to communicate the results of visualization and analysis for use in decision-making and policy development and to critique those processes.",
    "crumbs": [
      "Resources",
      "Quick Code Guides"
    ]
  },
  {
    "objectID": "labs/index.html",
    "href": "labs/index.html",
    "title": "Schedule Overview",
    "section": "",
    "text": "Schedule\nW01 (Oct 21 - Oct 25):\n\nCourse Overview\nLab1: Cambridge Building Energy: R, Quarto, dplyr essentials\n\nW02 (Oct 28 - Nov 1):\n\nExploratory Data Analysis\nLab 2: Opportunity Zones: tidyverse, ggplot2 packages\n\nW03 (Nov 4 - Nov 8):\n\nSpatial Analysis; Obtain data from multiple sources\nLab 3: Walkable Environment: sf, osmdata packages\n\nW04 (Nov 11 – Nov 15):\n\nCreate interactive graphs and maps\nLab 4: Airbnb in Chicago: plotly and leaflet packages\n\nW05 (Nov 18 – Nov 22):\n\nCensus Data and Demographic Analysis\nLab 5: Neighborhood Change: tidycensus, tidyr packages\n\nW06 (Nov 25 – Nov 29):\n\nWeb Storytelling I\nLab 6: Build ShinyApps\n\nW07 (Dec 2 – Dec 6):\n\nWeb Storytelling II\nLab 7 (optional): Quarto Website\n\nW08 (Dec 9 – Dec 11):\n\nPresentation",
    "crumbs": [
      "Labs",
      "Schedule"
    ]
  },
  {
    "objectID": "labs/lab1.html#summarise-create-a-summary-of-your-data",
    "href": "labs/lab1.html#summarise-create-a-summary-of-your-data",
    "title": "Get started with ",
    "section": "Summarise: Create a summary of your data",
    "text": "Summarise: Create a summary of your data\nGo ahead and run the following code and observe the result:\n\nmit_energy |&gt; \n  summarise(avg_emission = mean(`Total GHG Emissions (Metric Tons CO2e)`))\n\n# A tibble: 1 × 1\n  avg_emission\n         &lt;dbl&gt;\n1        1458.\n\n\nIt calculates the average of the column “Total GHG Emissions (Metric Tons CO2e)” of the entire dataset, and names the result “avg_emission”. The result says, of all MIT buildings, through all years, the average annual GHG emission is ~1458 MTCO2e.\nsummarise calculates summary statistics, like a total, mean, or count, across all values in the dataset. However, when used with group_by(), it calculates each group separately, collapsing each group into its own summary row.\nFor instance, below we calculate the average GHG emissions by Data Year, which is the year when the energy record was taken.\n\nmit_energy |&gt; \n  group_by(year = `Data Year`) |&gt; \n  summarise(avg_emission = mean(`Total GHG Emissions (Metric Tons CO2e)`))\n\n# A tibble: 8 × 2\n   year avg_emission\n  &lt;dbl&gt;        &lt;dbl&gt;\n1  2015        1575.\n2  2016        1436.\n3  2017        1524.\n4  2018        1449.\n5  2019        1473.\n6  2020        1384.\n7  2021        1407.\n8  2022        1419.\n\n\nThis says, in 2015, the average annual GHG emission was ~1575 MTCO2e., and in 2016, it was ~1436 MTCO2e., so on and so forth.\n\nYour practice\nInsert a few new code chunks below this one to document your code and show your results. \n\nFrom the mit_energy dataset, create a subset of all non-residential buildings, which were built before the year 2000. (Hint: which function would you use?). How many such buildings are there?\nFrom the mit_energy dataset, compare the GHG emissions by property type (Hint: which column has that information?), and generate a table that shows the following results:\n\n\nYou can create this table mostly by modifying the sample code, however, there are a few small things you can experiment on:\n\nThe calculated average numbers in this table are rounded to 2 decimals, how to achieve that?\nThe table is arranged in descending order based on the “avg_emission” column, how to do that? (Hint)\n\nWe are already trying to ask questions and find hints of interesting stories from the dataset! If the results so far look interesting/surprising/expected to you, write a few sentences describing what you see from the analysis.\n\nLastly, and for fun, we will insert a map to complete your working document! The dataset we have includes “Longitude” and “Latitude” columns, which I love, because it indicates that location information is readily available and can be visualized.\nCopy and paste the following code to your document, and you should be able to run it and see a map. (If your R says it can’t find mapview, run the line install.packages(\"mapview\"))\n\n\n#install.packages(\"mapview\")\nlibrary(mapview)\nmapview(\n  mit_energy,\n  xcol = \"Longitude\", ycol = \"Latitude\",\n  crs = 4326,\n  grid = FALSE\n)\n\n\n\n\n\n\nNow Save, Render your document again. You have now created a pretty, multi-media document using R!\n------\nIn this lab we have introduced how to create and develop a Quarto Document. We have also introduced some commonly-used dplyr functions including select, filter, group_by and summarise. This is the beginning of our data wrangling and leads to the work in Week 2.",
    "crumbs": [
      "Labs",
      "01 Cambridge Building Energy: R, Quarto, dplyr essentials"
    ]
  },
  {
    "objectID": "labs/lab2.html",
    "href": "labs/lab2.html",
    "title": "Exploratory Data Analysis with ",
    "section": "",
    "text": "This week’s Lab Exercise focuses on the dplyr package and the ggplot2 package. It also engages with data visualization best practices by demonstrating how to create and interpret a variety of graphics.\nExploratory data analysis (EDA) is a phase of a data science workflow that emphasizes getting to know the data before rushing to analyze it. EDA typically involves the creation and interpretation of summaries and graphics in order to gain insights that can inform more sophisticated analyses later on.",
    "crumbs": [
      "Labs",
      "02 Opportunity Zones: tidyverse, ggplot2 packages"
    ]
  },
  {
    "objectID": "labs/lab2.html#download-data-and-load-packages",
    "href": "labs/lab2.html#download-data-and-load-packages",
    "title": "Exploratory Data Analysis with ",
    "section": "Download data and load packages",
    "text": "Download data and load packages\nCreate a folder, for example, named “Lab 2”. Open RStudio and navigate to File &gt; New Project… When the dialog box will appear, choose Existing Directory. Proceed to create a new R project within your “Lab 2” folder.\nYou can create a new R file to implement the code from the tutorial; you will be asked to start a Quarto Document when you begin your exercises at the end of this tutorial.\nNow navigate to Urban Institute’s website about Opportunity Zones, find the link “Download tract-level data on all Opportunity Zones”, and download this dataset to your Lab 2 project folder. Rename the file if you need.\nTo stay organized, we should load packages at the beginning of our markdown document. These are the three packages we are going to use today. You may want to run install.packages() on readxl and DataExplorer if it’s the first time you use them.\n\nlibrary(readxl)\nlibrary(tidyverse)\nlibrary(DataExplorer)",
    "crumbs": [
      "Labs",
      "02 Opportunity Zones: tidyverse, ggplot2 packages"
    ]
  },
  {
    "objectID": "labs/lab2.html#data-cleaning",
    "href": "labs/lab2.html#data-cleaning",
    "title": "Exploratory Data Analysis with ",
    "section": "Data cleaning",
    "text": "Data cleaning\nThe mutate function in dplyr allows you to modify your dataset by either adding new columns, or updating values in existing columns. It’s a very flexible function because you can transform existing variables using a wide range of operations, such as arithmetic calculations, conditional expressions, or functions.\nFor example, the Urban Institute has coded the designated variable as either taking a value of 1 when designated, or NA when not. Since the NA and 1 here have no mathematical meaning, it would be easier to read if the column simply showed text like “Designated” or “Not Designated.” In the following code, we are updating the column DesignatedOZ.\n\nozs &lt;- data |&gt;\n  mutate(DesignatedOZ =\n           ifelse(is.na(DesignatedOZ), \n                  \"not_designated\", \"designated\"))\n\nThe ifelse(condition, \"not_designated\", \"designated\") is used to set the value of DesignatedOZ based on the condition: If DesignatedOZ is NA, it assigns the text “not_designated”. Otherwise, it assigns “designated”. After the modification, we can make a quick count of both types of tracts.\n\nozs |&gt; \n  count(DesignatedOZ) \n\n# A tibble: 2 × 2\n  DesignatedOZ       n\n  &lt;chr&gt;          &lt;int&gt;\n1 designated      8764\n2 not_designated 33414\n\n\nNote: A common point of confusion is the similarity between &lt;- (assign to) and |&gt;(pipe) and when to use them. To put it briefly:\n\nWhen to Use &lt;-: Use &lt;- to save our object. In the example above, we are keeping the original data intact, but created a new object called ozs.\nWhen Not to Use &lt;-: If you only want to view results without modifying the object.\n\nThere are a few columns (such as SE_Flag) that wont’ be very helpful for this analysis. We can select a subset of columns to work on. If there is a minus sign in front of the column names, that means to drop these specified columns.\n\nozs &lt;- \n  ozs |&gt; \n  select(-c(dec_score, SE_Flag, pctown, Metro, Micro, NoCBSAType))\n\nOne of the characteristics tracked in the Urban Institute data is the median household income for each tract (medhhincome). We might want to question whether there’s a difference in the median household income for designated and not-designated census tracts.\nHowever, if you scroll down to the bottom of the dataset in the data viewer, you will notice there are quite a few of NAs in the Census demographic columns.\nHow many missing values are there, and how many would be a hurdle for my analysis? It will be great to have a sense of completeness in terms of what proportion of a field actually holds data. Below we use is.na to check if each element in ozs is NA, and use colSums to sum up all TRUE values by column.\n\ncolSums(is.na(ozs))\n\n           geoid            state     DesignatedOZ           county \n               0               23                0               98 \n            Type       Population      medhhincome      PovertyRate \n               0              112              249              141 \n       unemprate         medvalue          medrent severerentburden \n             141             1106              395              189 \n     vacancyrate         pctwhite         pctBlack      pctHispanic \n             167              131              131              131 \n    pctAAPIalone       pctunder18        pctover64        HSorlower \n             131              131              131              132 \n      BAorhigher \n             132 \n\n\nAnother way to observe missing values in each column is to use plot_missing in the DataExplorer package.\n\nplot_missing(ozs)\n\n\n\n\n\n\n\n\nplot_missing calculates the proportion of missing values in a given variable, and makes some judgemental calls of whether the missing is significant, indicated by “Good”, “OK”, and “Bad”. (Feel feel to check out ?plot_missing in your console. What are the default ranges for these three categories?) Overall, most of our columns have a very small portion of missing values (less than 1%) and would not create significant representative issues. However, when performing calculations, we need to include the na.rm = TRUE argument, indicating that we are calculating based on the available 99%.",
    "crumbs": [
      "Labs",
      "02 Opportunity Zones: tidyverse, ggplot2 packages"
    ]
  },
  {
    "objectID": "labs/lab2.html#create-summary-tables",
    "href": "labs/lab2.html#create-summary-tables",
    "title": "Exploratory Data Analysis with ",
    "section": "Create Summary tables",
    "text": "Create Summary tables\nWe can calculate the average median household income for designated and not-designated census tracts. That is to collapse the stat summary of median household income summarise(mean(medhhincome)) into two groups group_by(DesignatedOZ) .\n\nozs |&gt; \n  group_by(DesignatedOZ) |&gt; \n  summarise(\n    Tracts = n(),\n    Income = mean(medhhincome, na.rm=TRUE))\n\n# A tibble: 2 × 3\n  DesignatedOZ   Tracts Income\n  &lt;chr&gt;           &lt;int&gt;  &lt;dbl&gt;\n1 designated       8764 33346.\n2 not_designated  33414 44446.\n\n\nWe can also put two columns in the group_by function, for instance, grouping first by state and then by eligibility, allowing for comparisons within each state.\n\nozs |&gt; \n  group_by(state, DesignatedOZ) |&gt; \n  summarise(\n    Income = mean(medhhincome, na.rm=TRUE))\n\n# A tibble: 108 × 3\n# Groups:   state [57]\n   state          DesignatedOZ   Income\n   &lt;chr&gt;          &lt;chr&gt;           &lt;dbl&gt;\n 1 Alabama        designated     30044.\n 2 Alabama        not_designated 36542.\n 3 Alaska         designated     49840.\n 4 Alaska         not_designated 54784.\n 5 American Samoa designated       NaN \n 6 Arizona        designated     34373.\n 7 Arizona        not_designated 40961.\n 8 Arkansas       designated     31254.\n 9 Arkansas       not_designated 37814.\n10 California     designated     36134.\n# ℹ 98 more rows\n\n\n“American Samoa” might have caught our attention at this step, because we’ve got NaN (not a number), indicating that all its values are NA. This prompts us to return to the dataset and further clean our data.\nAre there any other states where all economic variable values are NA, possibly meaning that we have no records for tracts in those areas?\n\nozs |&gt; \n  group_by(state) |&gt; \n  summarize(all_na = all(is.na(Population))) |&gt; \n  filter(all_na == TRUE)\n\n# A tibble: 5 × 2\n  state                    all_na\n  &lt;chr&gt;                    &lt;lgl&gt; \n1 American Samoa           TRUE  \n2 Guam                     TRUE  \n3 Northern Mariana Islands TRUE  \n4 Virgin Islands           TRUE  \n5 &lt;NA&gt;                     TRUE  \n\n\nThe all(is.na(Population)) function checks if all values in the Population column for that state are NA. If they are, all_na will be TRUE. If we aim to produce economic stats and these states are uninformative, we can choose to remove them from our dataset:\n\nozs &lt;- \n  ozs |&gt; \n  filter(!state %in% c(\"American Samoa\", \"Guam\", \"Northern Mariana Islands\", \"Virgin Islands\") & !is.na(state))\n\nThen perform the summary again:\n\nozs |&gt; \n  group_by(state, DesignatedOZ) |&gt; \n  summarise(\n    Income = mean(medhhincome, na.rm=TRUE))\n\n# A tibble: 103 × 3\n# Groups:   state [52]\n   state      DesignatedOZ   Income\n   &lt;chr&gt;      &lt;chr&gt;           &lt;dbl&gt;\n 1 Alabama    designated     30044.\n 2 Alabama    not_designated 36542.\n 3 Alaska     designated     49840.\n 4 Alaska     not_designated 54784.\n 5 Arizona    designated     34373.\n 6 Arizona    not_designated 40961.\n 7 Arkansas   designated     31254.\n 8 Arkansas   not_designated 37814.\n 9 California designated     36134.\n10 California not_designated 50858.\n# ℹ 93 more rows\n\n\nIt might be useful for us to reshape our summary table, arranging it in a way that each state has a single row with separate columns for designated and not-designated income value.\nFunctions pivot_wider() and pivot_longer() are useful for reshaping data. pivot_wider() adds columns to a dataset by transitioning content from rows to columns. pivot_longer() does the opposite - it makes a dataset longer by transitioning columns to rows.\nIn our case, let’s use pivot_wider() to transition our Designated and Not Designated rows into columns.\n\nozs |&gt; \n  group_by(state, DesignatedOZ) |&gt; \n  summarise(\n    Income = mean(medhhincome, na.rm=TRUE)) |&gt; \n  pivot_wider(names_from = DesignatedOZ, values_from = Income)\n\n# A tibble: 52 × 3\n# Groups:   state [52]\n   state                designated not_designated\n   &lt;chr&gt;                     &lt;dbl&gt;          &lt;dbl&gt;\n 1 Alabama                  30044.         36542.\n 2 Alaska                   49840.         54784.\n 3 Arizona                  34373.         40961.\n 4 Arkansas                 31254.         37814.\n 5 California               36134.         50858.\n 6 Colorado                 41138.         49601.\n 7 Connecticut              36760.         51389.\n 8 Delaware                 40971.         50143.\n 9 District of Columbia     38291.         62840.\n10 Florida                  31015.         40931.\n# ℹ 42 more rows\n\n\nAdd one more step, we can create a new column, to calculate and show the difference in income between designated and not designated tracts:\n\nozs |&gt; \n  group_by(state, DesignatedOZ) |&gt; \n  summarise(\n    Income = mean(medhhincome, na.rm=TRUE)) |&gt; \n  pivot_wider(names_from = DesignatedOZ, values_from = Income) |&gt; \n  mutate(Difference = designated - not_designated)\n\n# A tibble: 52 × 4\n# Groups:   state [52]\n   state                designated not_designated Difference\n   &lt;chr&gt;                     &lt;dbl&gt;          &lt;dbl&gt;      &lt;dbl&gt;\n 1 Alabama                  30044.         36542.     -6498.\n 2 Alaska                   49840.         54784.     -4944.\n 3 Arizona                  34373.         40961.     -6588.\n 4 Arkansas                 31254.         37814.     -6560.\n 5 California               36134.         50858.    -14724.\n 6 Colorado                 41138.         49601.     -8463.\n 7 Connecticut              36760.         51389.    -14628.\n 8 Delaware                 40971.         50143.     -9172.\n 9 District of Columbia     38291.         62840.    -24548.\n10 Florida                  31015.         40931.     -9916.\n# ℹ 42 more rows",
    "crumbs": [
      "Labs",
      "02 Opportunity Zones: tidyverse, ggplot2 packages"
    ]
  },
  {
    "objectID": "labs/lab2.html#distribution-of-one-variable",
    "href": "labs/lab2.html#distribution-of-one-variable",
    "title": "Exploratory Data Analysis with ",
    "section": "Distribution of one variable",
    "text": "Distribution of one variable\n\nBoxplot\nThe code below creates a boxplot to contrast the distribution of poverty rates between designated opportunity zones and undesignated zones. We are using grammars of the ggplot function introduced in class, then adding more features with the + operator and other functions listed in the package reference.\n\nozs |&gt; ggplot(): This is the main plotting function. ozs is your dataset we use.\ngeom_boxplot(): Recall that geometric layers are called geoms_*. It tells R what kind of geometry you want to use visualize the data.\naes(x = DesignatedOZ, y = PovertyRate): The aes() function is where you tell ggplot which variable goes on the x axis followed by which variable goes on the y axis.\nThe third aesthetic element is fill, which indicates the filled color of the boxplot. Accordingly, we use the fill argument in the labs function to set the name of the legend.\nWe used a new function scale_y_continuous to specify y axis properties. Here we are making sure the poverty rate are labeled as percentages. If you remove this line, they will by default show as decimal numbers.\n\n\nozs |&gt; \n  ggplot(aes(x = DesignatedOZ, y = PovertyRate, fill = DesignatedOZ)) +\n  geom_boxplot() + \n  scale_y_continuous(labels = scales::percent) +\n  labs(x = \"Opportunity Zone Eligible Tracts\", y = \"Poverty Rate\", fill = \"Tracts\")\n\n\n\n\n\n\n\n\nBy comparing the 50th percentile (the horizontal line inside each box) we can see that tracts designated as Opportunity Zones have a higher poverty rate compared with those not designated. The heights of the boxes themselves give us an indication of how closely around the median all values in the dataset are concentrated—the degree of dispersion or spread. The vertical lines are called whiskers and extend upward and downward to the lowest values that are not candidates for outlier status. An outlier is an unusual value that could potentially influence the results of an analysis. These are indicated with dots in the boxplot.\n\n\nDensity plot\nBy modifying the last code chunk, we can make a density plot to describe the distribution of poverty rate. A density plot can be understood as a smoothed version of the histogram, and provides a more direct view of of the shape and peaks in the data. The x-axis typically represents the range of values for the variable of interest, while the y-axis represents the probability density (how likely it is for the variable to take on a particular value within that range).\n\nozs |&gt; \n  ggplot(aes(x = PovertyRate, fill = DesignatedOZ)) +\n  geom_density() + \n  scale_x_continuous(labels = scales::percent) +\n  labs(x = \"Poverty Rate\", fill = \"Tracts\")\n\n\n\n\n\n\n\n\nIf you have noticed - in the code above, we didn’t provide a variable that what goes to the y-axis. Where does the value “density” come from?\nMany graphs, like boxplot, plot the raw values of your dataset. But other graphs, like histograms and density plots, calculate new values to plot. Here a density takes the count of data points at discrete poverty rate levels and smooths it out into a continuous curve. Then calculated values (probability density) go to the y-axis.\n\n\nCombinations of basic graphs to create composite views\nOne of the coolest thing about ggplot is that we can plot multiple geom_ on top of each other. For instance, we can combine the two plots above, to show both visually appealing curves and essential statistics (medians, quartiles, outliers, etc.) The following code uses two geom_(Check out geom_violin for more!), and introduces several new arguments for fine-tuning the cosmetics.\n\ntrim = FALSE: If TRUE (default), trim the tails of the violins to the range of the data. If FALSE, don’t trim the tails and show the complete distribution.\nalpha = 0.5: the transparency of the plotting area.\ncoord_flip(): whether the y axis is displayed horizonally or vertically.\nlegend.position = \"none\": the position of legend (“left”, “right”, “bottom”, “top”, or two-element numeric vector), or not showing the legend (“none”).\n\n\nozs |&gt; ggplot() +\n  geom_violin(aes(x = DesignatedOZ, y = PovertyRate, fill = DesignatedOZ), trim = FALSE, alpha = 0.5) +\n  geom_boxplot(aes(x = DesignatedOZ, y = PovertyRate), color = \"black\", width = .15, alpha = 0.8) +\n  scale_y_continuous(labels = scales::percent) +\n  labs(\n    x = \"Opportunity Zone Eligible Tracts\",\n    y = \"Poverty Rate\",\n    title = \"Distribution of Poverty Rate\"\n  ) +\n  coord_flip() +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\nA useful way to learn new arguments in ggplot is to take some of them out and see how it changes the plot.",
    "crumbs": [
      "Labs",
      "02 Opportunity Zones: tidyverse, ggplot2 packages"
    ]
  },
  {
    "objectID": "labs/lab2.html#relationship-between-two-variables",
    "href": "labs/lab2.html#relationship-between-two-variables",
    "title": "Exploratory Data Analysis with ",
    "section": "Relationship between two variables",
    "text": "Relationship between two variables\n\nScatter Plot\nWe are often interested in bivariate relationships or how two variables relate to one another. Scatterplots are often used to visualize the association between two continuous variables. They can reveal much about the nature of the relationship between two variables.\nLet’s use your subset of Massachusetts data to perform this part of analysis. (We could use the nationwide dataset, but there will be over 40,000 points showing on the graph, which will not be pleasing to the eye).\n\nozs_ma &lt;- ozs |&gt; filter(state == \"Massachusetts\") \n\nWe begin by creating a scatterplot of poverty rate and racial distribution. Note that we used theme_bw, which is a theme template for a cleaner look.\n\nozs_ma |&gt; \n  ggplot(aes(x = pctBlack, y = PovertyRate)) +\n  geom_point() +\n  labs(x = \"Proportion of Black Population\",\n       y = \"Poverty rate\",\n       title = \"Poverty rate vs. proportion of black population in Opportunity Zone eligible tracts\", \n       subtitle = \"State of Massachusetts\",\n       caption = \"Source: Urban Institute (2018)\") + \n  theme_bw()\n\n\n\n\n\n\n\n\nThere is a slight increase of slope as we move from left to right along the x-axis. However, there are all the points shown here. How can we distinguish between the two groups - Add a third “aesthetic element”, which is DesignatedOZ, and include the linear regression lines using geom_smooth.\n\nozs_ma |&gt; \n  ggplot(aes(x = pctBlack, y = PovertyRate, color = DesignatedOZ)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(x = \"Proportion of Black Population\",\n       y = \"Poverty rate\",\n       title = \"Poverty rate vs. proportion of black population in Opportunity Zone eligible tracts\", \n       subtitle = \"State of Massachusetts\",\n       caption = \"Source: Urban Institute (2018)\") + \n  theme_bw()",
    "crumbs": [
      "Labs",
      "02 Opportunity Zones: tidyverse, ggplot2 packages"
    ]
  },
  {
    "objectID": "labs/lab2.html#combinations-scatterplot-with-marginal-histograms",
    "href": "labs/lab2.html#combinations-scatterplot-with-marginal-histograms",
    "title": "Exploratory Data Analysis with ",
    "section": "Combinations: Scatterplot with marginal histograms",
    "text": "Combinations: Scatterplot with marginal histograms\nThis is just for fun - If you want to create more advanced statistical plots, there are ways to do that too. The following graph requires an additional package ggExtra. But the part within ggplot should look familiar now.\n\n#install.packages(\"ggExtra\")\nggExtra::ggMarginal(\n  ggplot(ozs_ma,\n         aes(x = pctBlack, y = PovertyRate, color = DesignatedOZ)) +\n    geom_point(na.rm = TRUE) +\n    theme_bw(),\n  type = \"histogram\",\n  groupFill = TRUE\n)",
    "crumbs": [
      "Labs",
      "02 Opportunity Zones: tidyverse, ggplot2 packages"
    ]
  },
  {
    "objectID": "labs/lab2.html#exercise-1",
    "href": "labs/lab2.html#exercise-1",
    "title": "Exploratory Data Analysis with ",
    "section": "Exercise 1",
    "text": "Exercise 1\nThe first exercise will ask you to create summary tables to analyze opportunity zones in Massachusetts. Please include the code you used, the tables you produced, and any explanatory text that you think would help clarify your results:\n\nIn Massachusetts, what are the average poverty rates for Opportunity Zones and non-Opportunity Zones?\nWhen you have the result of Q1 (the summary for Massachusetts), what are the corresponding situations by county in Massachusetts?\nReorganize your previous table, which county has the greatest disparity in poverty rate between designated and non-designated tracts?",
    "crumbs": [
      "Labs",
      "02 Opportunity Zones: tidyverse, ggplot2 packages"
    ]
  },
  {
    "objectID": "labs/lab2.html#exercise-2",
    "href": "labs/lab2.html#exercise-2",
    "title": "Exploratory Data Analysis with ",
    "section": "Exercise 2",
    "text": "Exercise 2\nFocus on your data in Massachusetts, now choose from the following variables: medhhincome, vacancyrate, unemprate, pctwhite, pctblack, pctHispanic, pctover64, HSorlower to help answer the following questions.\n\nSelect one of the variables, create a graphical representation that contrasts its distribution in designated tracts and in undesignated tracts in Massachusetts.\nSelect two variables, create a graphical representation that describes how they relate (or don’t relate) to each other, including the direction of this relationship..\nWhat can we say about the difference in demographic/economic conditions reflected by these graphs between designated and not designated tracts? Include in your document a few sentences of write-up. You can connect your findings with your summary tables above, and with some broader discussions about Opportunities Zones found here.",
    "crumbs": [
      "Labs",
      "02 Opportunity Zones: tidyverse, ggplot2 packages"
    ]
  },
  {
    "objectID": "labs/lab2.html#exercise-3",
    "href": "labs/lab2.html#exercise-3",
    "title": "Exploratory Data Analysis with ",
    "section": "Exercise 3",
    "text": "Exercise 3\nFirst, let’s use our familiar group_by + summarise process to calculate the average median house income by county in Massachusetts.\n\nozs_ma |&gt; \n  group_by(county, DesignatedOZ) |&gt;  \n  summarise(\n    Income = mean(medhhincome, na.rm=TRUE)) \n\n# A tibble: 25 × 3\n# Groups:   county [13]\n   county            DesignatedOZ   Income\n   &lt;chr&gt;             &lt;chr&gt;           &lt;dbl&gt;\n 1 Barnstable County designated     46717.\n 2 Barnstable County not_designated 61663.\n 3 Berkshire County  designated     35199 \n 4 Berkshire County  not_designated 51122.\n 5 Bristol County    designated     34573.\n 6 Bristol County    not_designated 42035.\n 7 Dukes County      not_designated 46816 \n 8 Essex County      designated     41358.\n 9 Essex County      not_designated 49966.\n10 Franklin County   designated     41711.\n# ℹ 15 more rows\n\n\nPlease pipe your summarized table to ggplot() for visualization. The geom function you should use here is geom_col.\n\nTake a few minutes to compare the bar chart you created and the one below:\n\n\nThere should be a few differences, which have enhanced the overall quality. How can you modify your code to replicate the bar chart in this image? In a new code chunk, please copy and paste your last bar chart code, and try your best to address the following questions.\n\nThe bars are put side-by-side instead of stacking on top of one another. If you don’t want a stacked bar chart, you can use the position argument in geom_col. There will be three options: “identity”, “dodge”, or “fill”.\nThe x-axis labels are titled to 45 degrees. How can I achieve this? Hint.\nThe labels on the y-axis are formatted in thousands with commas. This can be achieved by modifying the function scale_y_continuous(labels = scales::percent) we have seen above. Hint.\nLastly, the counties are not arranged alphabetically, but rather by the income values mapped to the y-axis, starting from large to small. How can I achieve this? Hint.\nPlease add the title, subtitle, x- and y-axis labels, and the data source annotation to your bar chart.\nPlease choose a theme template for your bar chart.\n\nFeel free to consult the R Graph Gallery and Aesthetic specifications for additional resources.",
    "crumbs": [
      "Labs",
      "02 Opportunity Zones: tidyverse, ggplot2 packages"
    ]
  }
]