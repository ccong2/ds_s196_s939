[
  {
    "objectID": "syllabus/index.html",
    "href": "syllabus/index.html",
    "title": "Applied Data Science for Cities",
    "section": "",
    "text": "Description\nUrban data science draws upon statistics, visualization, and computation to better understand and ultimately shape cities. This course emphasizes telling stories about cities using a set of descriptive approaches, quantitative and spatial analysis in R. We will learn how to describe community characteristics with small area census data, work with local administrative data, and think about how our analysis of quantitative data fit with other forms of data and engagement to fill in gaps in knowledge.\nLearning objectives:\n\nLearn - begin developing core data science competencies such as data wrangling, visualization, analysis, and communication;\nApply - apply concepts introduced in the readings and lectures to analyze datasets drawn from cities around the world;\nVisualize - manipulate tabular and geospatial data to produce intelligible and useful graphics for inclusion in documents and dissemination on the web;\nSynthesize - translate the results of visualization and analysis for use in decision-making and policy development.\n\n\nHow Will We Be Learning\nLecture: Class meetings are generally divided into lecture (Mondays) and laboratory sessions (Wednesdays) that focus on concepts and hand-on applications, respectively.\nLab: We will provide data science tutorials using R. Each lab tutorial aims to solve a specific urban data science problem in addition to building coding skills. Lab reports are due before the subsequent lab session and should be written independently.\nExtension readings: We provide a light material list that focuses on specific topics each week. These resources are meant to expand your knowledge and enhance your project completion capabilities.\nUrban data science project: The term project for the course will focus on integrating the tools of data science to explore a specific real-world planning issue or research question. This is a group project and students will define the scope of the project and identify specific deliverable(s) early in the semester. Reproducing an existing analysis or study using different datasets or an alternate study area is also acceptable for the term project.\n\n\nPrerequisites\nThis is a relatively fast-paced course so students can benefit from some prior knowledge working in R and RStudio. However, this is not a course prerequisite. Our first course sessions will focus on ensuring that we are all familiar with some of the basic work environment and methods which we'll make use of over the semester.\n\n\nAssessment\n\n\n\nAssignment\nWeight\n\n\nLab Reports\n50%\n\n\nProject Proposal\n10%\n\n\nProject Presentation\n30%\n\n\nAttendance\n10%\n\n\n\n\n\nKey Logistics\nLab Reports: You will be working on lab on most Wednesday classes and submit a short report before the next lab session. Details will be specified in each assignment submission page on Canvas.\nProject Proposal: Due Monday, Nov 18.\nThe purpose of this memo is to communicate the scope of your project (what you will do) and your strategies for collecting, visualizing, and analyzing data (how you will do it). A separate guideline will be distributed at the beginning of the course.\nProject Presentation: Week 8 (Dec 9 – Dec 11), class time. Presentations slides will due Dec 9.\n\n\nLate policy\nTo keep all students on a relatively level playing field, 5% will be deducted for late assignments, with an additional 5% deducted for each subsequent day. Late assignments two weeks after the due date will receive no credit and will not be accepted.\n\n\nSoftware\nWe use R and R Studio as the coding environment to develop analysis and applications. You will need to install both software on your personal computers from hereLinks to an external site. and hereLinks to an external site..\n\n\nCommunication\nPlan on using our class Slack channel, email, and office hours to get help with troubleshooting problems as they arise in your work. I also encourage you to work with others in the class to troubleshoot problems - it is highly likely that others in the class have encountered similar problems, and this also allows us to create a repository of our problems and responses.\nEmail: I check emails quite frequently, but I will not always be able to respond to emails right away. Please plan accordingly so that you don't miss deadlines.\nSlack: We have a Slack workspace that is accessible through Canvas for general communication, including homework Q&A, resource exchange, project collaboration, etc.\nOffice hours: Please consult the top of the syllabus for specific times. I will announce if there are any changes or exceptions. I'm happy to answer any specific coding questions, or chat and help shape the objective and scope of your projects.\n\n\nEthics\nAcademic Integrity: Violations to academic integrity are unacceptable at MIT and DUSP. Instances of misconduct include but are not limited to plagiarism, cheating, and deliberately unauthorized use of course data and material.\nCollaboration Policy: While team collaboration is encouraged, students should specify their roles and tasks in a project. A positive and constructive attitude toward teamwork is essential for the successful completion of the course.\nDiversity and Inclusion: MIT highly values a diverse, friendly, respectful, and inclusive learning environment among students, faculty, and staff. We welcome all individuals regardless of their origin, citizenship, gender identity, sexual orientation, or religious and political beliefs. Please contact me or departmental staff if you have any questions / considerations regarding this."
  },
  {
    "objectID": "labs/lab1.html",
    "href": "labs/lab1.html",
    "title": "Get started with ",
    "section": "",
    "text": "This exercise provides some more structured exploration and practice with Quarto Document. We will mix Markdown sections with code chunks, and experiment with working with a tabular dataset.\nLab 1 is not graded, and there’s no need to submit anything after you’ve completed it. But for labs after this one, you will complete them and submit your lab report, details will be specified in each lab document.",
    "crumbs": [
      "Labs",
      "01 Cambridge Building Energy: R, Quarto, dplyr essentials"
    ]
  },
  {
    "objectID": "labs/lab1.html#set-up-a-new-project",
    "href": "labs/lab1.html#set-up-a-new-project",
    "title": "Get started with ",
    "section": "Set up a New Project",
    "text": "Set up a New Project\nWe have talked about files paths in class, and the importance of setting a directory to work out of. Most of the time we can use setwd(), this works fine until when you want to move your working folder to another computer or share your work. If you write out a path in the code, it might not work on another computer if that directory does not exist.\nIn our labs, we are going to use R Projects to organize our works and make sure we don’t lose files. R projects organize all files related to a project in one place and setting up relative file paths. Let’s start by setting up a project for our exercise.\nLaunch RStudio, then click File - New Project… A dialog box will open up. Select New Directory, then New Project. Here, you can create a new folder to save everything related to this project. For example, I navigated to my D:/Fall24 folder and created a new folder there called Lab 1:\n\nClick the button “Create Project”. R will take a second to refresh. Then you will see in your Files tab that you have been directed to your current working directory: D:/Fall24/Lab 1. You will also see a .Rproj file in that folder.\n\nThe .Rproj file serves as a reference point that R uses to locate all files associated with the project. If you save all files related to Lab 1 in this folder, all relative paths remain intact and consistently applied.\nNote: In future sessions, I may provide you with a project folder containing data. As long as you launch RStudio by double-clicking the .Rproj file, you will be taken directly to the project’s home directory.",
    "crumbs": [
      "Labs",
      "01 Cambridge Building Energy: R, Quarto, dplyr essentials"
    ]
  },
  {
    "objectID": "labs/lab1.html#practice-formatting-text-with-quarto",
    "href": "labs/lab1.html#practice-formatting-text-with-quarto",
    "title": "Get started with ",
    "section": "Practice formatting text with Quarto",
    "text": "Practice formatting text with Quarto\nNow go to File - New File - Quarto Document to create a new Quarto document. The prompt shown below will appear. Type in a document title (e.g. Lab 1) and your name. Keep the radio button for HTML selected.\n\nYou will then see a template file. At the very top you will see the YAML (or “Yet Another Markdown Language”) header which begins and ends with three dashes ---. The YAML header determines how your document will be rendered to your desired output format. Now it specifies the title, author, output format and text editor.\nTo get an idea of how everything works, let’s click the “Render” button on top of your toolbar.\nWhen prompted, give this file a name, it will be saved in the folder where your “.Rproj” file is, as a .qmd file.\nYou will now see a formatted document in a web browser. Switch between your code and the document back and forth to see where each part of the code is placed in the rendered HTML file.\nNow we can add a new line to the YAML header:\n\ndate: &lt;insert the date that the file is created&gt;.\n\nRender it again and see where the difference is.\nThere can be other options specified in the YAML, particularly if you are rendering to a format other than HTML (such as pdf, or Word, see all formats).\nOn the very left of this toolbar, click the “Source” button to switch Markdown editing mode. These sections of text typically explain or provide context for the code and graphics, and they are formatted using Markdown syntax. For example:\n\n#: a header element.\n**: bold text.\n*: italic text.\n` : code blocks.\n\nOverall, the Visual interface looks pretty much like a Word document. There is a toolbar that allows you to make bold or italic text, create a bullet list, insert a link or an image, insert a code block, etc.\nNow let’s delete everything below the YAML header in the template file, so that we will start creating our own formatted report.\n\nYour practice\nIn 2014, the City of Cambridge passed a local ordinance on building energy use disclosure. Spend a few moments reviewing this website to become familiar with the ordinance (in general). Then, add a short paragraph below your YAML section that explain the following:\n\nWhat does the Building Energy Use Disclosure Ordinance require?\nWhat kind of data have been compiled and where to find them?\n\nYou may edit your text either in the “Source” or “Visual” panel, or toggle between them to get familiar with both. Make sure to make gratuitous use of bold, italics, bullet points, etc. in your text.\nWhen you finish, save your file and click Render again. You can immediately see your nicely formatted document in a web browser.",
    "crumbs": [
      "Labs",
      "01 Cambridge Building Energy: R, Quarto, dplyr essentials"
    ]
  },
  {
    "objectID": "labs/lab1.html#select-selects-a-subset-of-columns.",
    "href": "labs/lab1.html#select-selects-a-subset-of-columns.",
    "title": "Get started with ",
    "section": "Select: selects a subset of columns.",
    "text": "Select: selects a subset of columns.\nIn the energy dataset, we probably don’t need all of the 46 columns. So we can make it a smaller dataset by specifying a few columns to keep.\ndataset |&gt; select(Column1, Column2)\nInsert a new code chunk in your document like this one below. We will only keep 9 columns in the new dataset. You can type the pipe |&gt; operator in using Shift+Ctrl/Cmd+M.\n\nenergy &lt;- energy |&gt;\n  select(\n    `Data Year`,\n    `BEUDO Category`,\n    Owner,\n    `Year Built`,\n    `Primary Property Type - Self Selected`,\n    `Total GHG Emissions (Metric Tons CO2e)`,\n    `Total GHG Emissions Intensity (kgCO2e/ft2)`,\n    Longitude,\n    Latitude\n  ) \n\nSome of the column names are surrounded by backticks (`), that’s because they include special characters or spaces, which deviate from standard naming conventions. The use of backticks is a means of preserving these unique naming attributes. Just keep typing the column names, dplyr will populate the correct names for you.",
    "crumbs": [
      "Labs",
      "01 Cambridge Building Energy: R, Quarto, dplyr essentials"
    ]
  },
  {
    "objectID": "labs/lab1.html#filter-select-a-subset-of-rows",
    "href": "labs/lab1.html#filter-select-a-subset-of-rows",
    "title": "Get started with ",
    "section": "filter: Select a subset of rows",
    "text": "filter: Select a subset of rows\nNow let’s create a new dataset that only contains energy use records from MIT buildings.\ndataset |&gt; filter(&lt;condition&gt;)\nTake a look at how we achieve this using the following code:\n{r}\nenergy |&gt; \n  filter(Owner == \"MASSACHUSETTS INSTITUTE OF TECHNOLOGY\")\nViewing the result, you’ll notice that some entries are missing records for total GHG emissions, which appear as NA under the “Total GHG Emissions (Metric Tons CO2e)” column. If we want to simplify the dataset by keeping only the rows with valid GHG emission records, we can apply that as a filter condition too.\nProceed to insert a new code chunk in your document like the one below. Now we are filtering MIT buildings that have emission data, and we are assigning the result to a new variable “mit_energy”.\n\nmit_energy &lt;- energy |&gt; \n  filter(Owner == \"MASSACHUSETTS INSTITUTE OF TECHNOLOGY\") |&gt; \n  filter(!is.na(`Total GHG Emissions (Metric Tons CO2e)`))\n\nis.na() is a function commonly used to check whether each value in a column is missing (NA). The ! is a logical negation operator, so !is.na() checks for values that are not missing. It returns TRUE for non-missing values and FALSE for missing values.",
    "crumbs": [
      "Labs",
      "01 Cambridge Building Energy: R, Quarto, dplyr essentials"
    ]
  },
  {
    "objectID": "labs/lab1.html#group_by-summarise",
    "href": "labs/lab1.html#group_by-summarise",
    "title": "Get started with ",
    "section": "Group_by + Summarise",
    "text": "Group_by + Summarise\nSummarise is usually used in conjunction with group_by because the latter changes the scope from operating on the entire dataset to operating on it group-by-group. Go ahead and run the following code and observe the result:\n\nmit_energy |&gt; \n  group_by(`Data Year`) |&gt; \n  summarise(count = n())\n\n# A tibble: 8 × 2\n  `Data Year` count\n        &lt;dbl&gt; &lt;int&gt;\n1        2015   130\n2        2016   135\n3        2017   130\n4        2018   136\n5        2019   113\n6        2020   116\n7        2021   139\n8        2022   136\n\n\nWe now have a summary table that says, in our dataset, there are 130 records in 2015, 135 in 2016, and so on. We use group_by such that rows are grouped according to Data Year, which is the year when the energy record was taken. The result is then passed to summarise to count a total number of records per year. By default, the n() function creates a new column, which we here name as “count”.  \nBelow we are using the same group_by + summarise chain to calculate the average GHG emissions of all buildings, and the average GHG emission intensity. Pay attention to how we are giving new names to each of the new columns.\n\nmit_energy |&gt; \n  group_by(year = `Data Year`) |&gt; \n  summarise(count = n(),\n            avg_emission = mean(`Total GHG Emissions (Metric Tons CO2e)`),\n            avg_intensity = mean(`Total GHG Emissions Intensity (kgCO2e/ft2)`))\n\n# A tibble: 8 × 4\n   year count avg_emission avg_intensity\n  &lt;dbl&gt; &lt;int&gt;        &lt;dbl&gt;         &lt;dbl&gt;\n1  2015   130        1575.          13.4\n2  2016   135        1436.          13.2\n3  2017   130        1524.          13.5\n4  2018   136        1449.          13.1\n5  2019   113        1473.          12.6\n6  2020   116        1384.          10.9\n7  2021   139        1407.          11.4\n8  2022   136        1419.          11.4\n\n\n\nYour practice\nInsert a few new code chunks below this one to document your code and show your results. \n\nFrom the mit_energy dataset, create a subset of all non-residential buildings, which were built before the year 2000. (Hint: which function would you use?). How many such buildings are there?\nFrom the mit_energy dataset, compare the GHG emissions by property type (Hint: which column has that information?), and generate a table that shows the following results:\n\n\nYou can create this table mostly by modifying the sample code, however, there are a few small things you can experiment on:\n\nThe calculated average numbers in this table are rounded to 2 decimals, how to achieve that?\nThe table is arranged in descending order based on the “avg_emission” column, how to do that? (Hint)\n\nWe are already trying to ask questions and find hints of interesting stories from the dataset! If the results so far look interesting/surprising/expected to you, write a few sentences describing what you see from the analysis.\n\nLastly, and for fun, we will insert a map to complete your working document! The dataset we have includes “Longitude” and “Latitude” columns, which I love, because it indicates that location information is readily available and can be visualized.\nAdd the following code to your document, and you should be able to run it and see a map. (If your R says it can’t find mapview, run the line install.packages(\"mapview\"))\n\n\n#install.packages(\"mapview\")\nlibrary(mapview)\nmapview(\n  mit_energy,\n  xcol = \"Longitude\", ycol = \"Latitude\",\n  crs = 4326,\n  grid = FALSE\n)\n\n\n\n\n\n\nNow Save, Render your document again. You have now created a pretty, multi-media document using R!\n------\nIn this lab we have introduced how to create and develop a Quarto Document. We have also introduced some commonly-used dplyr functions including select, filter, group_by and summarise. This is the beginning of our data wrangling and leads to the work in Week 2.",
    "crumbs": [
      "Labs",
      "01 Cambridge Building Energy: R, Quarto, dplyr essentials"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Applied Data Science for Cities",
    "section": "",
    "text": "Urban science draws on statistics, visualization, and spatial analysis techniques to gain deeper insights into cities and actively contribute to their development. In this course, we’ll dive into the dynamic world of urban science by learning how to tell stories about cities and neighborhoods, covering a range of topics including demographic analysis, health and transportation, and using R as our primary quantitative analysis and interactive visualization tool.\n\n\n\n\n\n\n\nCourse Information\n\nSchedule: MW 9:30 - 11:00 AM, H2\nLocation: Mondays: 9-451; Wednesdays: 9-554\nCanvas Site: https://canvas.mit.edu/courses/27065"
  },
  {
    "objectID": "howto/setupr.html",
    "href": "howto/setupr.html",
    "title": "Applied Data Science for Cities",
    "section": "",
    "text": "Urban analytics draws upon statistics, visualization, and computation to better understand and ultimately shape cities. This course emphasizes telling stories about cities and neighborhoods covering a set of fundamental concepts of descriptive approaches, quantitative and spatial analysis in R, and principles of reproducible data analysis. Students learn to communicate the results of visualization and analysis for use in decision-making and policy development and to critique those processes.",
    "crumbs": [
      "Resources",
      "Set Up R and RStudio"
    ]
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "howto/git.html",
    "href": "howto/git.html",
    "title": "Applied Data Science for Cities",
    "section": "",
    "text": "Urban analytics draws upon statistics, visualization, and computation to better understand and ultimately shape cities. This course emphasizes telling stories about cities and neighborhoods covering a set of fundamental concepts of descriptive approaches, quantitative and spatial analysis in R, and principles of reproducible data analysis. Students learn to communicate the results of visualization and analysis for use in decision-making and policy development and to critique those processes.",
    "crumbs": [
      "Resources",
      "Start with GIT"
    ]
  },
  {
    "objectID": "howto/quartoweb.html",
    "href": "howto/quartoweb.html",
    "title": "Applied Data Science for Cities",
    "section": "",
    "text": "Urban analytics draws upon statistics, visualization, and computation to better understand and ultimately shape cities. This course emphasizes telling stories about cities and neighborhoods covering a set of fundamental concepts of descriptive approaches, quantitative and spatial analysis in R, and principles of reproducible data analysis. Students learn to communicate the results of visualization and analysis for use in decision-making and policy development and to critique those processes.",
    "crumbs": [
      "Resources",
      "Publish a Quarto Website"
    ]
  },
  {
    "objectID": "howto/startcode.html",
    "href": "howto/startcode.html",
    "title": "Applied Data Science for Cities",
    "section": "",
    "text": "Urban analytics draws upon statistics, visualization, and computation to better understand and ultimately shape cities. This course emphasizes telling stories about cities and neighborhoods covering a set of fundamental concepts of descriptive approaches, quantitative and spatial analysis in R, and principles of reproducible data analysis. Students learn to communicate the results of visualization and analysis for use in decision-making and policy development and to critique those processes.",
    "crumbs": [
      "Resources",
      "Quick Code Guides"
    ]
  },
  {
    "objectID": "labs/index.html",
    "href": "labs/index.html",
    "title": "Schedule Overview",
    "section": "",
    "text": "Schedule\nW01 (Oct 21 - Oct 25):\n\nCourse Overview\nLab1: Cambridge Building Energy: R, Quarto, dplyr essentials\n\nW02 (Oct 28 - Nov 1):\n\nExploratory Data Analysis\nLab 2: Opportunity Zones: tidyverse, ggplot2 packages\n\nW03 (Nov 4 - Nov 8):\n\nSpatial Analysis; Obtain data from multiple sources\nLab 3: Walkable Environment: sf, osmdata packages\n\nW04 (Nov 11 – Nov 15):\n\nCreate interactive graphs and maps\nLab 4: Airbnb in Chicago: plotly and leaflet packages\n\nW05 (Nov 18 – Nov 22):\n\nCensus Data and Demographic Analysis\nLab 5: Neighborhood Change: tidycensus, tidyr packages\n\nW06 (Nov 25 – Nov 29):\n\nWeb Storytelling I\nLab 6: Build ShinyApps\n\nW07 (Dec 2 – Dec 6):\n\nWeb Storytelling II\nLab 7 (optional): Quarto Website\n\nW08 (Dec 9 – Dec 11):\n\nPresentation",
    "crumbs": [
      "Labs",
      "Schedule"
    ]
  },
  {
    "objectID": "labs/lab1.html#summarise-create-a-summary-of-your-data",
    "href": "labs/lab1.html#summarise-create-a-summary-of-your-data",
    "title": "Get started with ",
    "section": "Summarise: Create a summary of your data",
    "text": "Summarise: Create a summary of your data\nGo ahead and run the following code and observe the result:\n\nmit_energy |&gt; \n  summarise(avg_emission = mean(`Total GHG Emissions (Metric Tons CO2e)`))\n\n# A tibble: 1 × 1\n  avg_emission\n         &lt;dbl&gt;\n1        1458.\n\n\nIt calculates the average of the column “Total GHG Emissions (Metric Tons CO2e)” of the entire dataset, and names the result “avg_emission”. The result says, of all MIT buildings, through all years, the average annual GHG emission is ~1458 MTCO2e.\nsummarise calculates summary statistics, like a total, mean, or count, across all values in the dataset. However, when used with group_by(), it calculates each group separately, collapsing each group into its own summary row.\nFor instance, below we calculate the average GHG emissions by Data Year, which is the year when the energy record was taken.\n\nmit_energy |&gt; \n  group_by(year = `Data Year`) |&gt; \n  summarise(avg_emission = mean(`Total GHG Emissions (Metric Tons CO2e)`))\n\n# A tibble: 8 × 2\n   year avg_emission\n  &lt;dbl&gt;        &lt;dbl&gt;\n1  2015        1575.\n2  2016        1436.\n3  2017        1524.\n4  2018        1449.\n5  2019        1473.\n6  2020        1384.\n7  2021        1407.\n8  2022        1419.\n\n\nThis says, in 2015, the average annual GHG emission was ~1575 MTCO2e., and in 2016, it was ~1436 MTCO2e., so on and so forth.\n\nYour practice\nInsert a few new code chunks below this one to document your code and show your results. \n\nFrom the mit_energy dataset, create a subset of all non-residential buildings, which were built before the year 2000. (Hint: which function would you use?). How many such buildings are there?\nFrom the mit_energy dataset, compare the GHG emissions by property type (Hint: which column has that information?), and generate a table that shows the following results:\n\n\nYou can create this table mostly by modifying the sample code, however, there are a few small things you can experiment on:\n\nThe calculated average numbers in this table are rounded to 2 decimals, how to achieve that?\nThe table is arranged in descending order based on the “avg_emission” column, how to do that? (Hint)\n\nWe are already trying to ask questions and find hints of interesting stories from the dataset! If the results so far look interesting/surprising/expected to you, write a few sentences describing what you see from the analysis.\n\nLastly, and for fun, we will insert a map to complete your working document! The dataset we have includes “Longitude” and “Latitude” columns, which I love, because it indicates that location information is readily available and can be visualized.\nCopy and paste the following code to your document, and you should be able to run it and see a map. (If your R says it can’t find mapview, run the line install.packages(\"mapview\"))\n\n\n#install.packages(\"mapview\")\nlibrary(mapview)\nmapview(\n  mit_energy,\n  xcol = \"Longitude\", ycol = \"Latitude\",\n  crs = 4326,\n  grid = FALSE\n)\n\n\n\n\n\n\nNow Save, Render your document again. You have now created a pretty, multi-media document using R!\n------\nIn this lab we have introduced how to create and develop a Quarto Document. We have also introduced some commonly-used dplyr functions including select, filter, group_by and summarise. This is the beginning of our data wrangling and leads to the work in Week 2.",
    "crumbs": [
      "Labs",
      "01 Cambridge Building Energy: R, Quarto, dplyr essentials"
    ]
  },
  {
    "objectID": "labs/lab2.html",
    "href": "labs/lab2.html",
    "title": "Exploratory Data Analysis with ",
    "section": "",
    "text": "This week’s Lab Exercise focuses on the dplyr package and the ggplot2 package. It also engages with data visualization best practices by demonstrating how to create and interpret a variety of graphics.\nExploratory data analysis (EDA) is a phase of a data science workflow that emphasizes getting to know the data before rushing to analyze it. EDA typically involves the creation and interpretation of summaries and graphics in order to gain insights that can inform more sophisticated analyses later on.",
    "crumbs": [
      "Labs",
      "02 Opportunity Zones: tidyverse, ggplot2 packages"
    ]
  },
  {
    "objectID": "labs/lab2.html#download-data-and-load-packages",
    "href": "labs/lab2.html#download-data-and-load-packages",
    "title": "Exploratory Data Analysis with ",
    "section": "Download data and load packages",
    "text": "Download data and load packages\nCreate a folder, for example, named “Lab 2”. Open RStudio and navigate to File &gt; New Project… When the dialog box will appear, choose Existing Directory. Proceed to create a new R project within your “Lab 2” folder.\nYou can create a new R file to implement the code from the tutorial; you will be asked to start a Quarto Document when you begin your exercises at the end of this tutorial.\nNow navigate to Urban Institute’s website about Opportunity Zones, find the link “Download tract-level data on all Opportunity Zones”, and download this dataset to your Lab 2 project folder. Rename the file if you need.\nTo stay organized, we should load packages at the beginning of our markdown document. These are the three packages we are going to use today. You may want to run install.packages() on readxl and DataExplorer if it’s the first time you use them.\n\nlibrary(readxl)\nlibrary(tidyverse)\nlibrary(DataExplorer)",
    "crumbs": [
      "Labs",
      "02 Opportunity Zones: tidyverse, ggplot2 packages"
    ]
  },
  {
    "objectID": "labs/lab2.html#data-cleaning",
    "href": "labs/lab2.html#data-cleaning",
    "title": "Exploratory Data Analysis with ",
    "section": "Data cleaning",
    "text": "Data cleaning\nThe mutate function in dplyr allows you to modify your dataset by either adding new columns, or updating values in existing columns. It’s a very flexible function because you can transform existing variables using a wide range of operations, such as arithmetic calculations, conditional expressions, or functions.\nFor example, the Urban Institute has coded the designated variable as either taking a value of 1 when designated, or NA when not. Since the NA and 1 here have no mathematical meaning, it would be easier to read if the column simply showed text like “Designated” or “Not Designated.” In the following code, we are updating the column DesignatedOZ.\n\nozs &lt;- data |&gt;\n  mutate(DesignatedOZ =\n           ifelse(is.na(DesignatedOZ), \n                  \"not_designated\", \"designated\"))\n\nThe ifelse(condition, \"not_designated\", \"designated\") is used to set the value of DesignatedOZ based on the condition: If DesignatedOZ is NA, it assigns the text “not_designated”. Otherwise, it assigns “designated”. After the modification, we can make a quick count of both types of tracts.\n\nozs |&gt; \n  count(DesignatedOZ) \n\n# A tibble: 2 × 2\n  DesignatedOZ       n\n  &lt;chr&gt;          &lt;int&gt;\n1 designated      8764\n2 not_designated 33414\n\n\nNote: A common point of confusion is the similarity between &lt;- (assign to) and |&gt;(pipe) and when to use them. To put it briefly:\n\nWhen to Use &lt;-: Use &lt;- to save our object. In the example above, we are keeping the original data intact, but created a new object called ozs.\nWhen Not to Use &lt;-: If you only want to view results without modifying the object.\n\nThere are a few columns (such as SE_Flag) that wont’ be very helpful for this analysis. We can select a subset of columns to work on. If there is a minus sign in front of the column names, that means to drop these specified columns.\n\nozs &lt;- \n  ozs |&gt; \n  select(-c(dec_score, SE_Flag, pctown, Metro, Micro, NoCBSAType))\n\nOne of the characteristics tracked in the Urban Institute data is the median household income for each tract (medhhincome). We might want to question whether there’s a difference in the median household income for designated and not-designated census tracts.\nHowever, if you scroll down to the bottom of the dataset in the data viewer, you will notice there are quite a few of NAs in the Census demographic columns.\nHow many missing values are there, and how many would be a hurdle for my analysis? It will be great to have a sense of completeness in terms of what proportion of a field actually holds data. Below we use is.na to check if each element in ozs is NA, and use colSums to sum up all TRUE values by column.\n\ncolSums(is.na(ozs))\n\n           geoid            state     DesignatedOZ           county \n               0               23                0               98 \n            Type       Population      medhhincome      PovertyRate \n               0              112              249              141 \n       unemprate         medvalue          medrent severerentburden \n             141             1106              395              189 \n     vacancyrate         pctwhite         pctBlack      pctHispanic \n             167              131              131              131 \n    pctAAPIalone       pctunder18        pctover64        HSorlower \n             131              131              131              132 \n      BAorhigher \n             132 \n\n\nAnother way to observe missing values in each column is to use plot_missing in the DataExplorer package.\n\nplot_missing(ozs)\n\n\n\n\n\n\n\n\nplot_missing calculates the proportion of missing values in a given variable, and makes some judgemental calls of whether the missing is significant, indicated by “Good”, “OK”, and “Bad”. (Feel feel to check out ?plot_missing in your console. What are the default ranges for these three categories?) Overall, most of our columns have a very small portion of missing values (less than 1%) and would not create significant representative issues. However, when performing calculations, we need to include the na.rm = TRUE argument, indicating that we are calculating based on the available 99%.",
    "crumbs": [
      "Labs",
      "02 Opportunity Zones: tidyverse, ggplot2 packages"
    ]
  },
  {
    "objectID": "labs/lab2.html#create-summary-tables",
    "href": "labs/lab2.html#create-summary-tables",
    "title": "Exploratory Data Analysis with ",
    "section": "Create Summary tables",
    "text": "Create Summary tables\nWe can calculate the average median household income for designated and not-designated census tracts. That is to collapse the stat summary of median household income summarise(mean(medhhincome)) into two groups group_by(DesignatedOZ) .\n\nozs |&gt; \n  group_by(DesignatedOZ) |&gt; \n  summarise(\n    Tracts = n(),\n    Income = mean(medhhincome, na.rm=TRUE))\n\n# A tibble: 2 × 3\n  DesignatedOZ   Tracts Income\n  &lt;chr&gt;           &lt;int&gt;  &lt;dbl&gt;\n1 designated       8764 33346.\n2 not_designated  33414 44446.\n\n\nWe can also put two columns in the group_by function, for instance, grouping first by state and then by eligibility, allowing for comparisons within each state.\n\nozs |&gt; \n  group_by(state, DesignatedOZ) |&gt; \n  summarise(\n    Income = mean(medhhincome, na.rm=TRUE))\n\n# A tibble: 108 × 3\n# Groups:   state [57]\n   state          DesignatedOZ   Income\n   &lt;chr&gt;          &lt;chr&gt;           &lt;dbl&gt;\n 1 Alabama        designated     30044.\n 2 Alabama        not_designated 36542.\n 3 Alaska         designated     49840.\n 4 Alaska         not_designated 54784.\n 5 American Samoa designated       NaN \n 6 Arizona        designated     34373.\n 7 Arizona        not_designated 40961.\n 8 Arkansas       designated     31254.\n 9 Arkansas       not_designated 37814.\n10 California     designated     36134.\n# ℹ 98 more rows\n\n\n“American Samoa” might have caught our attention at this step, because we’ve got NaN (not a number), indicating that all its values are NA. This prompts us to return to the dataset and further clean our data.\nAre there any other states where all economic variable values are NA, possibly meaning that we have no records for tracts in those areas?\n\nozs |&gt; \n  group_by(state) |&gt; \n  summarize(all_na = all(is.na(Population))) |&gt; \n  filter(all_na == TRUE)\n\n# A tibble: 5 × 2\n  state                    all_na\n  &lt;chr&gt;                    &lt;lgl&gt; \n1 American Samoa           TRUE  \n2 Guam                     TRUE  \n3 Northern Mariana Islands TRUE  \n4 Virgin Islands           TRUE  \n5 &lt;NA&gt;                     TRUE  \n\n\nThe all(is.na(Population)) function checks if all values in the Population column for that state are NA. If they are, all_na will be TRUE. If we aim to produce economic stats and these states are uninformative, we can choose to remove them from our dataset:\n\nozs &lt;- \n  ozs |&gt; \n  filter(!state %in% c(\"American Samoa\", \"Guam\", \"Northern Mariana Islands\", \"Virgin Islands\") & !is.na(state))\n\nThen perform the summary again:\n\nozs |&gt; \n  group_by(state, DesignatedOZ) |&gt; \n  summarise(\n    Income = mean(medhhincome, na.rm=TRUE))\n\n# A tibble: 103 × 3\n# Groups:   state [52]\n   state      DesignatedOZ   Income\n   &lt;chr&gt;      &lt;chr&gt;           &lt;dbl&gt;\n 1 Alabama    designated     30044.\n 2 Alabama    not_designated 36542.\n 3 Alaska     designated     49840.\n 4 Alaska     not_designated 54784.\n 5 Arizona    designated     34373.\n 6 Arizona    not_designated 40961.\n 7 Arkansas   designated     31254.\n 8 Arkansas   not_designated 37814.\n 9 California designated     36134.\n10 California not_designated 50858.\n# ℹ 93 more rows\n\n\nIt might be useful for us to reshape our summary table, arranging it in a way that each state has a single row with separate columns for designated and not-designated income value.\nFunctions pivot_wider() and pivot_longer() are useful for reshaping data. pivot_wider() adds columns to a dataset by transitioning content from rows to columns. pivot_longer() does the opposite - it makes a dataset longer by transitioning columns to rows.\nIn our case, let’s use pivot_wider() to transition our Designated and Not Designated rows into columns.\n\nozs |&gt; \n  group_by(state, DesignatedOZ) |&gt; \n  summarise(\n    Income = mean(medhhincome, na.rm=TRUE)) |&gt; \n  pivot_wider(names_from = DesignatedOZ, values_from = Income)\n\n# A tibble: 52 × 3\n# Groups:   state [52]\n   state                designated not_designated\n   &lt;chr&gt;                     &lt;dbl&gt;          &lt;dbl&gt;\n 1 Alabama                  30044.         36542.\n 2 Alaska                   49840.         54784.\n 3 Arizona                  34373.         40961.\n 4 Arkansas                 31254.         37814.\n 5 California               36134.         50858.\n 6 Colorado                 41138.         49601.\n 7 Connecticut              36760.         51389.\n 8 Delaware                 40971.         50143.\n 9 District of Columbia     38291.         62840.\n10 Florida                  31015.         40931.\n# ℹ 42 more rows\n\n\nAdd one more step, we can create a new column, to calculate and show the difference in income between designated and not designated tracts:\n\nozs |&gt; \n  group_by(state, DesignatedOZ) |&gt; \n  summarise(\n    Income = mean(medhhincome, na.rm=TRUE)) |&gt; \n  pivot_wider(names_from = DesignatedOZ, values_from = Income) |&gt; \n  mutate(Difference = designated - not_designated)\n\n# A tibble: 52 × 4\n# Groups:   state [52]\n   state                designated not_designated Difference\n   &lt;chr&gt;                     &lt;dbl&gt;          &lt;dbl&gt;      &lt;dbl&gt;\n 1 Alabama                  30044.         36542.     -6498.\n 2 Alaska                   49840.         54784.     -4944.\n 3 Arizona                  34373.         40961.     -6588.\n 4 Arkansas                 31254.         37814.     -6560.\n 5 California               36134.         50858.    -14724.\n 6 Colorado                 41138.         49601.     -8463.\n 7 Connecticut              36760.         51389.    -14628.\n 8 Delaware                 40971.         50143.     -9172.\n 9 District of Columbia     38291.         62840.    -24548.\n10 Florida                  31015.         40931.     -9916.\n# ℹ 42 more rows",
    "crumbs": [
      "Labs",
      "02 Opportunity Zones: tidyverse, ggplot2 packages"
    ]
  },
  {
    "objectID": "labs/lab2.html#distribution-of-one-variable",
    "href": "labs/lab2.html#distribution-of-one-variable",
    "title": "Exploratory Data Analysis with ",
    "section": "Distribution of one variable",
    "text": "Distribution of one variable\n\nBoxplot\nThe code below creates a boxplot to contrast the distribution of poverty rates between designated opportunity zones and undesignated zones. We are using grammars of the ggplot function introduced in class, then adding more features with the + operator and other functions listed in the package reference.\n\nozs |&gt; ggplot(): This is the main plotting function. ozs is your dataset we use.\ngeom_boxplot(): Recall that geometric layers are called geoms_*. It tells R what kind of geometry you want to use visualize the data.\naes(x = DesignatedOZ, y = PovertyRate): The aes() function is where you tell ggplot which variable goes on the x axis followed by which variable goes on the y axis.\nThe third aesthetic element is fill, which indicates the filled color of the boxplot. Accordingly, we use the fill argument in the labs function to set the name of the legend.\nWe used a new function scale_y_continuous to specify y axis properties. Here we are making sure the poverty rate are labeled as percentages. If you remove this line, they will by default show as decimal numbers.\n\n\nozs |&gt; \n  ggplot(aes(x = DesignatedOZ, y = PovertyRate, fill = DesignatedOZ)) +\n  geom_boxplot() + \n  scale_y_continuous(labels = scales::percent) +\n  labs(x = \"Opportunity Zone Eligible Tracts\", y = \"Poverty Rate\", fill = \"Tracts\")\n\n\n\n\n\n\n\n\nBy comparing the 50th percentile (the horizontal line inside each box) we can see that tracts designated as Opportunity Zones have a higher poverty rate compared with those not designated. The heights of the boxes themselves give us an indication of how closely around the median all values in the dataset are concentrated—the degree of dispersion or spread. The vertical lines are called whiskers and extend upward and downward to the lowest values that are not candidates for outlier status. An outlier is an unusual value that could potentially influence the results of an analysis. These are indicated with dots in the boxplot.\n\n\nDensity plot\nBy modifying the last code chunk, we can make a density plot to describe the distribution of poverty rate. A density plot can be understood as a smoothed version of the histogram, and provides a more direct view of of the shape and peaks in the data. The x-axis typically represents the range of values for the variable of interest, while the y-axis represents the probability density (how likely it is for the variable to take on a particular value within that range).\n\nozs |&gt; \n  ggplot(aes(x = PovertyRate, fill = DesignatedOZ)) +\n  geom_density() + \n  scale_x_continuous(labels = scales::percent) +\n  labs(x = \"Poverty Rate\", fill = \"Tracts\")\n\n\n\n\n\n\n\n\nIf you have noticed - in the code above, we didn’t provide a variable that what goes to the y-axis. Where does the value “density” come from?\nMany graphs, like boxplot, plot the raw values of your dataset. But other graphs, like histograms and density plots, calculate new values to plot. Here a density takes the count of data points at discrete poverty rate levels and smooths it out into a continuous curve. Then calculated values (probability density) go to the y-axis.\n\n\nCombinations of basic graphs to create composite views\nOne of the coolest thing about ggplot is that we can plot multiple geom_ on top of each other. For instance, we can combine the two plots above, to show both visually appealing curves and essential statistics (medians, quartiles, outliers, etc.) The following code uses two geom_(Check out geom_violin for more!), and introduces several new arguments for fine-tuning the cosmetics.\n\ntrim = FALSE: If TRUE (default), trim the tails of the violins to the range of the data. If FALSE, don’t trim the tails and show the complete distribution.\nalpha = 0.5: the transparency of the plotting area.\ncoord_flip(): whether the y axis is displayed horizonally or vertically.\nlegend.position = \"none\": the position of legend (“left”, “right”, “bottom”, “top”, or two-element numeric vector), or not showing the legend (“none”).\n\n\nozs |&gt; ggplot() +\n  geom_violin(aes(x = DesignatedOZ, y = PovertyRate, fill = DesignatedOZ), trim = FALSE, alpha = 0.5) +\n  geom_boxplot(aes(x = DesignatedOZ, y = PovertyRate), color = \"black\", width = .15, alpha = 0.8) +\n  scale_y_continuous(labels = scales::percent) +\n  labs(\n    x = \"Opportunity Zone Eligible Tracts\",\n    y = \"Poverty Rate\",\n    title = \"Distribution of Poverty Rate\"\n  ) +\n  coord_flip() +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\nA useful way to learn new arguments in ggplot is to take some of them out and see how it changes the plot.",
    "crumbs": [
      "Labs",
      "02 Opportunity Zones: tidyverse, ggplot2 packages"
    ]
  },
  {
    "objectID": "labs/lab2.html#relationship-between-two-variables",
    "href": "labs/lab2.html#relationship-between-two-variables",
    "title": "Exploratory Data Analysis with ",
    "section": "Relationship between two variables",
    "text": "Relationship between two variables\n\nScatter Plot\nWe are often interested in bivariate relationships or how two variables relate to one another. Scatterplots are often used to visualize the association between two continuous variables. They can reveal much about the nature of the relationship between two variables.\nLet’s use your subset of Massachusetts data to perform this part of analysis. (We could use the nationwide dataset, but there will be over 40,000 points showing on the graph, which will not be pleasing to the eye).\n\nozs_ma &lt;- ozs |&gt; filter(state == \"Massachusetts\") \n\nWe begin by creating a scatterplot of poverty rate and racial distribution. Note that we used theme_bw, which is a theme template for a cleaner look.\n\nozs_ma |&gt; \n  ggplot(aes(x = pctBlack, y = PovertyRate)) +\n  geom_point() +\n  labs(x = \"Proportion of Black Population\",\n       y = \"Poverty rate\",\n       title = \"Poverty rate vs. proportion of black population in Opportunity Zone eligible tracts\", \n       subtitle = \"State of Massachusetts\",\n       caption = \"Source: Urban Institute (2018)\") + \n  theme_bw()\n\n\n\n\n\n\n\n\nThere is a slight increase of slope as we move from left to right along the x-axis. However, there are all the points shown here. How can we distinguish between the two groups - Add a third “aesthetic element”, which is DesignatedOZ, and include the linear regression lines using geom_smooth.\n\nozs_ma |&gt; \n  ggplot(aes(x = pctBlack, y = PovertyRate, color = DesignatedOZ)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(x = \"Proportion of Black Population\",\n       y = \"Poverty rate\",\n       title = \"Poverty rate vs. proportion of black population in Opportunity Zone eligible tracts\", \n       subtitle = \"State of Massachusetts\",\n       caption = \"Source: Urban Institute (2018)\") + \n  theme_bw()",
    "crumbs": [
      "Labs",
      "02 Opportunity Zones: tidyverse, ggplot2 packages"
    ]
  },
  {
    "objectID": "labs/lab2.html#combinations-scatterplot-with-marginal-histograms",
    "href": "labs/lab2.html#combinations-scatterplot-with-marginal-histograms",
    "title": "Exploratory Data Analysis with ",
    "section": "Combinations: Scatterplot with marginal histograms",
    "text": "Combinations: Scatterplot with marginal histograms\nThis is just for fun - If you want to create more advanced statistical plots, there are ways to do that too. The following graph requires an additional package ggExtra. But the part within ggplot should look familiar now.\n\n#install.packages(\"ggExtra\")\nggExtra::ggMarginal(\n  ggplot(ozs_ma,\n         aes(x = pctBlack, y = PovertyRate, color = DesignatedOZ)) +\n    geom_point(na.rm = TRUE) +\n    theme_bw(),\n  type = \"histogram\",\n  groupFill = TRUE\n)",
    "crumbs": [
      "Labs",
      "02 Opportunity Zones: tidyverse, ggplot2 packages"
    ]
  },
  {
    "objectID": "labs/lab2.html#exercise-1",
    "href": "labs/lab2.html#exercise-1",
    "title": "Exploratory Data Analysis with ",
    "section": "Exercise 1",
    "text": "Exercise 1\nThe first exercise will ask you to create summary tables to analyze opportunity zones in Massachusetts. Please include the code you used, the tables you produced, and any explanatory text that you think would help clarify your results:\n\nIn Massachusetts, what are the average poverty rates for Opportunity Zones and non-Opportunity Zones?\nWhen you have the result of Q1 (the summary for Massachusetts), what are the corresponding situations by county in Massachusetts?\nReorganize your previous table, which county has the greatest disparity in poverty rate between designated and non-designated tracts?",
    "crumbs": [
      "Labs",
      "02 Opportunity Zones: tidyverse, ggplot2 packages"
    ]
  },
  {
    "objectID": "labs/lab2.html#exercise-2",
    "href": "labs/lab2.html#exercise-2",
    "title": "Exploratory Data Analysis with ",
    "section": "Exercise 2",
    "text": "Exercise 2\nFocus on your data in Massachusetts, now choose from the following variables: medhhincome, vacancyrate, unemprate, pctwhite, pctblack, pctHispanic, pctover64, HSorlower to help answer the following questions.\n\nSelect one of the variables, create a graphical representation that contrasts its distribution in designated tracts and in undesignated tracts in Massachusetts.\nSelect two variables, create a graphical representation that describes how they relate (or don’t relate) to each other, including the direction of this relationship..\nWhat can we say about the difference in demographic/economic conditions reflected by these graphs between designated and not designated tracts? Include in your document a few sentences of write-up. You can connect your findings with your summary tables above, and with some broader discussions about Opportunities Zones found here.",
    "crumbs": [
      "Labs",
      "02 Opportunity Zones: tidyverse, ggplot2 packages"
    ]
  },
  {
    "objectID": "labs/lab2.html#exercise-3",
    "href": "labs/lab2.html#exercise-3",
    "title": "Exploratory Data Analysis with ",
    "section": "Exercise 3",
    "text": "Exercise 3\nIn this part, we will make a Bar Chart. First, let’s use our familiar group_by + summarise process to calculate the average median house income by county in Massachusetts.\n\nozs_ma |&gt; \n  group_by(county, DesignatedOZ) |&gt;  \n  summarise(\n    Income = mean(medhhincome, na.rm=TRUE)) \n\n# A tibble: 25 × 3\n# Groups:   county [13]\n   county            DesignatedOZ   Income\n   &lt;chr&gt;             &lt;chr&gt;           &lt;dbl&gt;\n 1 Barnstable County designated     46717.\n 2 Barnstable County not_designated 61663.\n 3 Berkshire County  designated     35199 \n 4 Berkshire County  not_designated 51122.\n 5 Bristol County    designated     34573.\n 6 Bristol County    not_designated 42035.\n 7 Dukes County      not_designated 46816 \n 8 Essex County      designated     41358.\n 9 Essex County      not_designated 49966.\n10 Franklin County   designated     41711.\n# ℹ 15 more rows\n\n\nPlease pipe your summarized table to ggplot() for visualization. The geom function you should use here is geom_col.\n\nTake a few minutes to compare the bar chart you created and the one below:\n\n\nThere should be a few differences, which have enhanced the overall quality. How can you modify your code to replicate the bar chart in this image? In a new code chunk, please copy and paste your last bar chart code, and try your best to address the following questions.\n\nThe bars are put side-by-side instead of stacking on top of one another. If you don’t want a stacked bar chart, you can use the position argument in geom_col. There will be three options: “identity”, “dodge”, or “fill”.\nThe x-axis labels are titled to 45 degrees. How can I achieve this? Hint.\nThe labels on the y-axis are formatted in thousands with commas. This can be achieved by modifying the function scale_y_continuous(labels = scales::percent) we have seen above. Hint.\nLastly, the counties are not arranged alphabetically, but rather by the income values mapped to the y-axis, starting from large to small. How can I achieve this? Hint.\nPlease add the title, subtitle, x- and y-axis labels, and the data source annotation to your bar chart.\nPlease choose a theme template for your bar chart.\n\nFeel free to consult the R Graph Gallery and Aesthetic specifications for additional resources.",
    "crumbs": [
      "Labs",
      "02 Opportunity Zones: tidyverse, ggplot2 packages"
    ]
  },
  {
    "objectID": "labs/lab3.html",
    "href": "labs/lab3.html",
    "title": "Describe neighborhood dynamics with ",
    "section": "",
    "text": "In this lab, we will develop a place profile to quantify the physical and environmental characteristics of a given area. Planners are usually interested in understanding and enhance improving urban environments. Our approach will involve utilizing spatial data and conducting spatial analysis.\n\nTo put things into context, we will describe pedestrian-friendly built environment in Boston neighborhoods. Urban factors such as the presence of walking facilities, the density and variety of businesses, the number of residents and jobs, and streetscape elements like benches, storefronts, and shade all influence the extent and nature of walkable areas (Lai & Kontokosta, 2018; Frank et al., 2006; Owen et al., 2007; Ewing & Cervero, 2010).\n\nlibrary(sf)\nlibrary(tidyverse)\nlibrary(osmdata)\nlibrary(DT)\nlibrary(scales)\n\nFor the purpose of demonstrating how to gather, combine and analyze spatial data from different sources, we are going to develop three relatively simple indicators to measure the pedestrian environment in a Boston neighborhoods: sidewalk density, number of restaurants measured by points of interest (POI), and tree canopy coverage.",
    "crumbs": [
      "Labs",
      "03 Walkable Environment: sf, osmdata packages"
    ]
  },
  {
    "objectID": "labs/lab3.html#calculate-the-number-of-pois-by-neighborhood",
    "href": "labs/lab3.html#calculate-the-number-of-pois-by-neighborhood",
    "title": "Describe neighborhood dynamics with ",
    "section": "Calculate the number of POIs by neighborhood",
    "text": "Calculate the number of POIs by neighborhood\nWe now have a point shapefile (restaurants) and a polygon shapefile (neighborhood), and we want to count the number of points in each neighborhood. They are points, so we don’t need to look for overlapping/intersecting parts, st_join() will be sufficient.\n\nrestaurant_data &lt;- \n  st_join(restaurants, neighborhood) \n\nTake a look at what we’ve got. It’s still the same set of restaurants but each one has neighborhood information joined to it. Remove those fall outside of our neighborhood boundary, then count by neighborhood:\n\nrestaurant_data &lt;- \n  restaurant_data |&gt; \n  filter(!is.na(nbh_name)) |&gt; \n  count(nbh_name, name = \"restaurant\") |&gt; \n  st_drop_geometry()",
    "crumbs": [
      "Labs",
      "03 Walkable Environment: sf, osmdata packages"
    ]
  },
  {
    "objectID": "labs/lab4.html",
    "href": "labs/lab4.html",
    "title": "Creating Interactive Maps and Graphs with ",
    "section": "",
    "text": "Today we’re going to explore some new packages to enhance our visualizations. We will use an example of analyzing Airbnb data in Chicago to illustrate how to use these tools effectively.\n\nInteractive maps with leaflet\nplotly and adding interactivity through ggplotly\nIntroduce dashboards with flexdashboard\n\nCreate a project folder to hold your files. Once set up, I recommend starting your code in a simple R file to follow along with the tutorial. Later, you’ll be asked to identify which parts of the code are for the map and which are for the graph for submissions.\n\n# You may need to install plotly, leaflet, flexdashboard\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(plotly)\nlibrary(leaflet)\nlibrary(flexdashboard)",
    "crumbs": [
      "Labs",
      "04 Airbnb in Chicago: plotly and leaflet packages"
    ]
  },
  {
    "objectID": "labs/lab4.html#work-process",
    "href": "labs/lab4.html#work-process",
    "title": "Creating Interactive Maps and Graphs with ",
    "section": "Work Process",
    "text": "Work Process\n\nBasemap\nFirst, we initiate leaflet and add a basemap. With addTiles() you will add the default base map, and with setView() you will be able to set a center point and a zoom level. Run the following code to set our view to Chicago.\nleaflet() |&gt;\n  addTiles() |&gt;\n  setView(lng = -87.636483, lat = 41.862984, zoom = 10) \nBy default, addTiles() generates a basemap using OpenStreetMap tiles. They’re suitable, but the options of which basemap to use are extensive. Check out a lengthy list here. You can use addProviderTiles() instead to pick from among a set of third-party options. I’m partial to the washed simplicity of the “CartoDB.Positron” tiles, so I’ll use those in the maps that follow.\n\nleaflet() |&gt;\n  addProviderTiles(providers$CartoDB.Positron) |&gt;\n  setView(lng = -87.636483, lat = 41.862984,zoom = 10) \n\n\n\n\n\n\n\nCircle markers\nThen we can add all Airbnb listings as “circle markers” on the map. Circle markers offer more styling options than simple points, allowing you to define attributes such as: circle size (radius), whether to have storkes (stroke=TRUE) or fill (fill=TRUE), with what fill color (fillColor) and transparency (fillOpacity), etc.,\n\nleaflet() |&gt;\n  addProviderTiles(providers$CartoDB.Positron) |&gt;\n  setView(lng = -87.636483, lat = 41.862984,  zoom = 10) |&gt; \n  addCircleMarkers(data = airbnb,\n                   fill = TRUE,\n                   fillOpacity = 0.5,\n                   stroke = FALSE,\n                   radius = 1) \n\n\n\n\n\n\n\nColor\n\nFunction-Based Color Mapping\nRather than showing all listings in one color, we might want to differentiate listings by, for example, room type.\nIn ggplot2, when you specify color = &lt;variable name&gt;, ggplot automatically map colors based on your variable. However, Leaflet doesn’t know how to handle colors unless you tell it. You will need to create a color palette (pal) using color mapping functions to explicitly define how values in your variable correspond to colors.\n\n\nDefine pal using color mapping functions:\nIn the following line of code, we are defining our color palette pal using a color mapping function colorFactor(). This builds a relationship between the categorical values in the airbnb$room_type variable and a color ramp (“RdYlGn” – red, yellow, green).\npal &lt;- colorFactor(palette = \"RdYlGn\", domain = airbnb$room_type)\nWe used colorFactor() because variables in room_type are categorical. For numeric data, we will instead use colorBin() , which divides the range into “equal intervals”; and colorQuantile() , which creates quantiles for varying values.\n\n\nApplying pal to Map Elements\nOnce defined, you can use pal in multiple places on your map to consistently apply the color mapping. In the following code, the palette is passed to two things:\n1) fillColor = ~pal(room_type) - so that each room type will be filled with its corresponding color. The ~ is telling the system to treat room_type as a column of the airbnb dataset rather than a separate object.\n2) addLegend(pal = pal) so the that colors in the legend will show up accordingly.\n\npal &lt;- colorFactor(palette = \"RdYlGn\", domain = airbnb$room_type)\n\nleaflet() |&gt;\n  addProviderTiles(providers$CartoDB.Positron) |&gt;\n  setView(lng = -87.636483, lat = 41.862984,  zoom = 10) |&gt;\n  addCircleMarkers(data = airbnb,\n                   fillColor = ~pal(room_type),\n                   fillOpacity = 1,\n                   stroke = FALSE,\n                   radius = 1) |&gt;\n  addLegend(\n    position = 'topright',\n    pal = pal,\n    values = airbnb$room_type,\n    title = \"Room Type\"\n  )\n\n\n\n\n\n\n\n\nPolygons\nShapefiles can be added to leaflet through addPolygons() . For now, let’s download the Chicago boundary shapefile here. Once it’s downloaded, read it into R using appropriate sf functions.\nIn the last lab, we have been transforming all shapes to 2249, because we were performing spatial calculations, and need to use a local CRS like 2249 for Massachusetts to minimize distortion. For web mapping, however, EPSG:4326 (latitude/longitude) is always used to ensure global compatibility, as it’s the standard for GPS-based systems.\n\nchi_bnd &lt;- \n  st_read(\"../data/Chicago_City_Limits.shp\") |&gt; # change this to your path\n  st_transform(4326)\n\nNow we will add Chicago boundary to our map, while also adjusting the boundary color, width, and setting the fill color to transparent.\n\nleaflet() |&gt;\n  addProviderTiles(providers$CartoDB.Positron) |&gt;\n  setView(lng = -87.636483, lat = 41.862984,  zoom = 10) |&gt;\n  addCircleMarkers(data = airbnb,\n                   fillColor = ~pal(room_type),\n                   fillOpacity = 1,\n                   stroke = FALSE,\n                   radius = 1) |&gt;\n  addPolygons(data = chi_bnd,\n              color = \"blue\",\n              fill = FALSE,\n              weight = 1) |&gt;\n  addLegend(\n    position = 'topright',\n    pal = pal,\n    values = airbnb$room_type,\n    title = \"Room Type\"\n  )\n\n\n\n\n\n\n\nPopup labels\nPop-ups show information when clicking a map item. In our example, each circle marker represents one Airbnb listing. We can request the map to show an attribute of this listing (such as name) when clicking it.\n\naddCircleMarkers(data = airbnb,\n                 fillOpacity = 0.5,\n                 stroke = FALSE,\n                 radius = 1,\n                 popup = ~name) \n\nOr, displays more than one attributes, such as name, host_name and price:\n\naddCircleMarkers(data = airbnb,\n                 fillOpacity = 0.5,\n                 stroke = FALSE,\n                 radius = 1,\n                 popup = ~paste(name, host_name, price)) \n\nSame as before, we use ~ to indicate these are data attributes rather than individual objects.\nTry adding the popup argument to your addCircleMarkers() function. You will see that information are displayed in one continuous row, which is not easy to read. But we can fix this by defining a label format outside of the plotting process. We can even control the appearance of pop-up information using line breaks &lt;br&gt; or making text bold. For example:\n\npopup_format &lt;-\n  paste0(\"&lt;b&gt;Name:&lt;/b&gt;\", airbnb$name, \"&lt;br&gt;\",\n         \"&lt;b&gt;Host Name: &lt;/b&gt;\", airbnb$host_name,\"&lt;br&gt;\",\n         \"&lt;b&gt;Price: &lt;/b&gt;\", \"$\", airbnb$price, \"&lt;br&gt;\"\n  )\n\nleaflet() |&gt;\n  addProviderTiles(providers$CartoDB.Positron) |&gt;\n  setView(lng = -87.636483, lat = 41.862984,  zoom = 10) |&gt;\n  addCircleMarkers(data = airbnb,\n                   fillColor = ~pal(room_type),\n                   fillOpacity = 1,\n                   stroke = FALSE,\n                   radius = 1,\n                   popup = popup_format) |&gt;\n  addPolygons(data = chi_bnd,\n              color = \"blue\",\n              fill = FALSE,\n              weight = 1) |&gt;\n  addLegend(\n    position = 'topright',\n    pal = pal,\n    values = airbnb$room_type,\n    title = \"Room Type\"\n  )\n\n\n\n\n\nWhat we have created so far already resembles the map on Inside Airbnb’s visualization. For the last step, we will add a layer control to turn on and off layer groups as we wish!\n\n\nLayer Control\nIn the following code, we are assigning each item we added into a group. Then these group names are passed to the addLayersControl() for users to toggle on and off.\nAdd these layer control lines to your code - be careful to match all parentheses correctly.\n\nleaflet() |&gt;\n  addProviderTiles(providers$CartoDB.Positron,\n                   group = \"CartoDB Positron\") |&gt;\n  addProviderTiles(\"Esri.WorldImagery\", \n                   group = \"ESRI World Imagery\") |&gt;\n  addProviderTiles(providers$CartoDB.DarkMatter, \n                   group = \"CartoDB Dark\") |&gt;\n  setView(lng = -87.636483, lat = 41.862984,  zoom = 10) |&gt;\n  addCircleMarkers(data = airbnb,\n                   fillColor = ~pal(room_type),\n                   fillOpacity = 1,\n                   stroke = FALSE,\n                   radius = 1,\n                   popup = popup_format, \n                   group = \"Airbnb Listings\") |&gt;\n  addPolygons(data = chi_bnd,\n              color = \"blue\",\n              fill = FALSE,\n              weight = 1, \n              group = \"Chicago Boundary\") |&gt;\n  addLegend(\n    position = 'topright',\n    pal = pal,\n    values = airbnb$room_type,\n    title = \"Room Type\") |&gt; \n  addLayersControl(\n    baseGroups = c(\"CartoDB Positron\", \n                   \"ESRI World Imagery\",\n                   \"CartoDB Dark\"),\n    overlayGroups = c(\"Airbnb Listings\", \"Chicago Boundary\")\n  )\n\n\n\n\n\nNow, you should have a complete code for the Leaflet map. Make sure it runs properly and produces the map you want. You can then save it as a file, such as leaflet.R.",
    "crumbs": [
      "Labs",
      "04 Airbnb in Chicago: plotly and leaflet packages"
    ]
  },
  {
    "objectID": "labs/lab4.html#median-room-price-by-type",
    "href": "labs/lab4.html#median-room-price-by-type",
    "title": "Creating Interactive Maps and Graphs with ",
    "section": "Median Room Price by Type",
    "text": "Median Room Price by Type\nThis graph displays the median price by room type. It starts with a static ggplot graph, which is then passed to ggplotly for interactivity.\n\ng &lt;- airbnb |&gt; \n  group_by(room_type) |&gt; \n  summarise(median_price = median(price, na.rm = TRUE)) |&gt; \n  ggplot() + \n    geom_col(aes(x = room_type, y = median_price), \n             fill = \"#62A0CA\", width = 0.5) + \n  theme_bw()+\n  labs(x = \"Room Type\", y = \"Median Price\")\n\nggplotly(g)",
    "crumbs": [
      "Labs",
      "04 Airbnb in Chicago: plotly and leaflet packages"
    ]
  },
  {
    "objectID": "labs/lab4.html#proportion-of-airbnb-listings-by-type",
    "href": "labs/lab4.html#proportion-of-airbnb-listings-by-type",
    "title": "Creating Interactive Maps and Graphs with ",
    "section": "Proportion of Airbnb Listings by Type",
    "text": "Proportion of Airbnb Listings by Type\nPlotly follows an extremely similar grammar as ggplot2, with aesthetic arguments such as x, y, color, size, and functions for adding geometric objects including add_markers(), add_shape(), add_hline(). I’d perfer to convert almost any ggplot graphs into plotly to save me from having to remember a new set of functions. However, there’s one type of graph where I’d go directly to plotly: pie or donut charts. ggplot2 is less flexible with circular graphs since it’s more focused on x and y axes.\nThis graph shows the count of listings for each room type. The room type goes to labels(read: groups or legend items) and the number of listings for each type goes to values.\n\nairbnb |&gt; \n  count(room_type) |&gt; \n  plot_ly() |&gt;  \n  add_pie(labels = ~room_type, \n          values = ~n,\n          hole = 0.6)\n\n\n\n\n\nFor your interest in digging deeper into plotly, check out plotly R graph library and its ggplot2 integration examples.",
    "crumbs": [
      "Labs",
      "04 Airbnb in Chicago: plotly and leaflet packages"
    ]
  },
  {
    "objectID": "labs/lab4.html#introduction-to-flexdashboard",
    "href": "labs/lab4.html#introduction-to-flexdashboard",
    "title": "Creating Interactive Maps and Graphs with ",
    "section": "Introduction to flexdashboard",
    "text": "Introduction to flexdashboard\nCrafting a fundamental flexdashboard does not require additional web development knowledge because it uses simple templates in R code. From your RStudio interface, let’s go to File - New File - R Markdown.\nIn the popped-up window, select “From Template” - “Flex Dashboard”\n\nYou have a new opened file that serves as the template of the dashboard. Save this file as “flexdashboard.Rmd”\nGo ahead click “Knit” - “Knit to flex_dashboard”. You now should see a blank dashboard. Any code you put into the “Chart A”, “Chart B” and “Chart C” sections will be displayed in their corresponding spaces!",
    "crumbs": [
      "Labs",
      "04 Airbnb in Chicago: plotly and leaflet packages"
    ]
  },
  {
    "objectID": "labs/lab4.html#populate-a-dashboard-template",
    "href": "labs/lab4.html#populate-a-dashboard-template",
    "title": "Creating Interactive Maps and Graphs with ",
    "section": "Populate a dashboard template",
    "text": "Populate a dashboard template\nJust below the YAML header of flexdashboard.Rmd, there is a code chuck named setup and marked include = FALSE. Here you can supply any data related to package loading and data preparation. Make sure you include here any of your scripts related to loading packages and reading data:\n\nNow you only need to identify and isolate the code that we produced today to populate the respective three chart sections.\nIn other words, you should copy the code you’ve worked through building a leaflet map, and paste them in the blank code chunk under “Chart A”.\nThen copy and paste the codes under “Median Room Price by Type” and “Proportion of Airbnb Listings by type” to the flexdashboard sections “Chart B” and “Chart C”.\nWhen you are ready, knit the document again, and a dashboard should appear!\nFor your reference, I have my flexdashboard.Rmd uploaded here.",
    "crumbs": [
      "Labs",
      "04 Airbnb in Chicago: plotly and leaflet packages"
    ]
  },
  {
    "objectID": "labs/lab4.html#make-your-own-dashboard",
    "href": "labs/lab4.html#make-your-own-dashboard",
    "title": "Creating Interactive Maps and Graphs with ",
    "section": "Make your own dashboard",
    "text": "Make your own dashboard\nCongratulations on getting your dashboard up and running! For this week’s assignment, choose a U.S. city other than Chicago and create a flexdashboard that includes one leaflet map and no less than two charts to describe its Airbnb listings.\nYou should be able to reuse most of today’s code, but be sure to make the necessary changes to the Leaflet map for the new city.\nThere are a number of things we can stylish your dashboard. Feel free to refer to the flexdashboard walk-through, where you can find things such as choosing another layout, adjusting chart width, adding a tabset, etc.",
    "crumbs": [
      "Labs",
      "04 Airbnb in Chicago: plotly and leaflet packages"
    ]
  },
  {
    "objectID": "labs/lab5.html",
    "href": "labs/lab5.html",
    "title": "Download and analyze Census data with ",
    "section": "",
    "text": "In this lab, we will use decennial census data and American Community Survey (ACS) data to examine long-term population trends in neighborhoods.\nThe Nathalie P. Voorhees Center has conducted research on neighborhood and community improvement in the City of Chicago. In their Three Cities report, they calculate the average per capita income for each census tract in Chicago for the years 1970, 1980, 1990, 2000, 2010, and 2017, and then compare it to the regional weighted average income. They have found Chicago has become more segregated by income over time and is losing its middle class.\n\nIn the following image, the census tracts are classified into “three cities”: those that have increased their share of regional income by 20% or more, those that have changed by less than 20%, and those that have decreased by 20% or more. The study also summarizes socio-demographic characteristics based on these changes.\n\nWe will take the idea of the Voorhees Center’s study to examine population change in Chicago. We’ll look at population by census tracts over multiple years to see if we can categorize tracts as gaining, losing, or staying stable in population.\nThese are the packages we are going to use today.\n\n# You may need to install the packages: tidycensus, tigris and gt\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(tidycensus)\nlibrary(tigris)\nlibrary(gt)",
    "crumbs": [
      "Labs",
      "05 Neighborhood Change: tidycensus, tidyr packages"
    ]
  },
  {
    "objectID": "labs/lab5.html#obtain-acs-data-in-2022-get_acs",
    "href": "labs/lab5.html#obtain-acs-data-in-2022-get_acs",
    "title": "Download and analyze Census data with ",
    "section": "Obtain ACS data in 2022 (get_acs)",
    "text": "Obtain ACS data in 2022 (get_acs)\nWe will first use tidycensus to download data for 2022, you’ll need a Census API key (Here is where you can request one from the census bureau). After you’ve signed up for an API key, be sure to activate the key from the email you receive from the Census Bureau so it works correctly.\nDeclaring install = TRUE when calling census_api_key() will install the key for use in future R sessions, which may be convenient for many users.\ncensus_api_key(\"yourkeyinquotationmarks\", install = TRUE)\nTo complete the API call for ACS data, you will typically need to specify:\n\nA desired geography to look up (for example, tract- or block group-level data)\nThe variables we want to look up (we have the complete variable list for 2022 5-year ACS, you can also retrieve this list by calling load_variables())\nThe state and county names to define your study region.\nThe census year you want data for.\nThe survey you’d like to use (such as ACS 1-, or 5-year data)\n\nThe following code returns the number of total population by tract in Cook County, Illinois in 2022. The table I consult is B01001: Sex by Age.\n\nF_2022 &lt;- get_acs(\n  geography = \"tract\",\n  state = \"IL\",\n  county = \"Cook\",\n  variables = \"B01001_001\",\n  geometry = TRUE,\n  year = 2022)\n\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |=                                                                     |   1%\n  |                                                                            \n  |=                                                                     |   2%\n  |                                                                            \n  |==                                                                    |   3%\n  |                                                                            \n  |===                                                                   |   4%\n  |                                                                            \n  |=====                                                                 |   7%\n  |                                                                            \n  |======                                                                |   8%\n  |                                                                            \n  |======                                                                |   9%\n  |                                                                            \n  |=======                                                               |  10%\n  |                                                                            \n  |========                                                              |  12%\n  |                                                                            \n  |=========                                                             |  13%\n  |                                                                            \n  |==========                                                            |  14%\n  |                                                                            \n  |==========                                                            |  15%\n  |                                                                            \n  |===========                                                           |  15%\n  |                                                                            \n  |=============                                                         |  18%\n  |                                                                            \n  |=================                                                     |  24%\n  |                                                                            \n  |===================                                                   |  26%\n  |                                                                            \n  |====================                                                  |  28%\n  |                                                                            \n  |====================                                                  |  29%\n  |                                                                            \n  |=====================                                                 |  30%\n  |                                                                            \n  |======================                                                |  32%\n  |                                                                            \n  |========================                                              |  35%\n  |                                                                            \n  |=========================                                             |  35%\n  |                                                                            \n  |==========================                                            |  37%\n  |                                                                            \n  |===========================                                           |  39%\n  |                                                                            \n  |============================                                          |  40%\n  |                                                                            \n  |=============================                                         |  41%\n  |                                                                            \n  |===============================                                       |  44%\n  |                                                                            \n  |===============================                                       |  45%\n  |                                                                            \n  |==================================                                    |  49%\n  |                                                                            \n  |===================================                                   |  50%\n  |                                                                            \n  |====================================                                  |  52%\n  |                                                                            \n  |======================================                                |  54%\n  |                                                                            \n  |=======================================                               |  55%\n  |                                                                            \n  |=======================================                               |  56%\n  |                                                                            \n  |===================================================                   |  73%\n  |                                                                            \n  |=====================================================                 |  75%\n  |                                                                            \n  |=======================================================               |  78%\n  |                                                                            \n  |========================================================              |  80%\n  |                                                                            \n  |==========================================================            |  83%\n  |                                                                            \n  |===========================================================           |  85%\n  |                                                                            \n  |=============================================================         |  87%\n  |                                                                            \n  |==============================================================        |  88%\n  |                                                                            \n  |==============================================================        |  89%\n  |                                                                            \n  |=================================================================     |  93%\n  |                                                                            \n  |==================================================================    |  95%\n  |                                                                            \n  |======================================================================| 100%\n\n\nNow you might have seen our first problem: we don’t have the option for getting tract-level data for cities. We can only obtained data for Cook County, where Chicago is located, but not for the City of Chicago directly. We will come back to it later in our exercise to cut out tracts within Chicago’s spatial boundary. That’s why we’ve included the argument geometry = TRUE, which will be needed for our spatial processing.\n\ngeometry = TRUE returns a simple features (sf) object with spatial information saved for each tract. The returned table will include a geometry variable that enables spatial analysis.",
    "crumbs": [
      "Labs",
      "05 Neighborhood Change: tidycensus, tidyr packages"
    ]
  },
  {
    "objectID": "labs/lab5.html#decennial-data-get_decennial",
    "href": "labs/lab5.html#decennial-data-get_decennial",
    "title": "Download and analyze Census data with ",
    "section": "Decennial data (get_decennial)",
    "text": "Decennial data (get_decennial)\nIn addition to get_acs(), tidycensus also provides get_decennial() for retrieving data from the US Decennial Census for the years 2000, 2010, and 2020. The function uses similar arguments, as shown below:\n\nF_2010 &lt;- get_decennial(\n  geography = \"tract\", \n  state = \"IL\",\n  county = \"Cook\",\n  variables = \"P001001\",\n  geometry = TRUE,\n  year = 2010)\n\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |==                                                                    |   3%\n  |                                                                            \n  |===                                                                   |   4%\n  |                                                                            \n  |=====                                                                 |   8%\n  |                                                                            \n  |===========                                                           |  15%\n  |                                                                            \n  |=============                                                         |  19%\n  |                                                                            \n  |==============                                                        |  20%\n  |                                                                            \n  |================                                                      |  22%\n  |                                                                            \n  |=================                                                     |  24%\n  |                                                                            \n  |==================                                                    |  25%\n  |                                                                            \n  |====================                                                  |  28%\n  |                                                                            \n  |=====================                                                 |  30%\n  |                                                                            \n  |========================                                              |  34%\n  |                                                                            \n  |=========================                                             |  36%\n  |                                                                            \n  |===========================                                           |  38%\n  |                                                                            \n  |=============================                                         |  42%\n  |                                                                            \n  |===============================                                       |  44%\n  |                                                                            \n  |===============================                                       |  45%\n  |                                                                            \n  |=================================                                     |  47%\n  |                                                                            \n  |==================================                                    |  48%\n  |                                                                            \n  |==================================                                    |  49%\n  |                                                                            \n  |===================================                                   |  51%\n  |                                                                            \n  |====================================                                  |  52%\n  |                                                                            \n  |======================================                                |  54%\n  |                                                                            \n  |=======================================                               |  56%\n  |                                                                            \n  |=========================================                             |  58%\n  |                                                                            \n  |==========================================                            |  60%\n  |                                                                            \n  |===========================================                           |  62%\n  |                                                                            \n  |============================================                          |  63%\n  |                                                                            \n  |=============================================                         |  64%\n  |                                                                            \n  |==============================================                        |  66%\n  |                                                                            \n  |=======================================================               |  78%\n  |                                                                            \n  |========================================================              |  80%\n  |                                                                            \n  |==========================================================            |  82%\n  |                                                                            \n  |==========================================================            |  84%\n  |                                                                            \n  |===========================================================           |  84%\n  |                                                                            \n  |============================================================          |  86%\n  |                                                                            \n  |=============================================================         |  87%\n  |                                                                            \n  |==============================================================        |  88%\n  |                                                                            \n  |===============================================================       |  90%\n  |                                                                            \n  |================================================================      |  92%\n  |                                                                            \n  |=================================================================     |  93%\n  |                                                                            \n  |==================================================================    |  94%\n  |                                                                            \n  |===================================================================   |  96%\n  |                                                                            \n  |====================================================================  |  97%\n  |                                                                            \n  |======================================================================| 100%",
    "crumbs": [
      "Labs",
      "05 Neighborhood Change: tidycensus, tidyr packages"
    ]
  },
  {
    "objectID": "labs/lab5.html#historic-data-in-1990-and-1970",
    "href": "labs/lab5.html#historic-data-in-1990-and-1970",
    "title": "Download and analyze Census data with ",
    "section": "Historic Data in 1990 and 1970",
    "text": "Historic Data in 1990 and 1970\nUnfortunately we cannot continue to use get_decennial() for the 1990 and 1970 data. The main reason is that the Census Bureau has removed the API endpoints for earlier years. Additionally, Census geographies—which approximate neighborhoods based on population—can change over time. Some tracts stay the same, but others may split as populations grow or merge when populations decline.\nAnalysts often face the challenge of geographic inconsistencies when working with longitudinal data. The Census Bureau’s geographic relationship files can help show the relationships between geographies over time. To make it easier for researchers, Brown University’s Longitudinal Tract Database offers demographic estimates for previous decennial census years - 1970, 1980, 1990, and 2000 - adjusted to match the 2010 boundaries. This means the tract areas and IDs match the 2010 data, with attribute values adjusted based on area or population share to reflect boundary changes over the years.\nLet’s get familiar with LTDB data and how to use them. Go to the data download page. You will need to enter your email address and certify that you will follow terms of use for the data.\nReview the data standard descriptions a bit to see what datasets are available. In our analysis, we will only use the Standard Full Data Files for population counts.\nIn “Download Standard Full Data Files”, select the year 1990, click the download button. Then do the same for 1970. Save the two .csv files into your project folder.\nNow we can import these two datasets using read_csv. Simultaneously, we’ll use filter() to extract only the portion for Cook County.\nF_1990 &lt;- \n  read_csv(\"LTDB_Std_1990_fullcount.csv\") |&gt;\n  filter(county == \"Cook County\" & state == \"IL\")\n\nF_1970 &lt;- read_csv(\"LTDB_Std_1970_fullcount.csv\")|&gt;\n  filter(county == \"Cook County\" & state == \"IL\")",
    "crumbs": [
      "Labs",
      "05 Neighborhood Change: tidycensus, tidyr packages"
    ]
  },
  {
    "objectID": "labs/lab5.html#compare-population-changes",
    "href": "labs/lab5.html#compare-population-changes",
    "title": "Download and analyze Census data with ",
    "section": "Compare population changes",
    "text": "Compare population changes\nWe have compiled population data in one table. To make this table more informative, let’s calculate the percentage change in population for each tract, and classify tracts based on whether their population has significantly decreased, increased, or remained largely unchanged, following the Voohver Center’s approach.\n\nTry to insert your own code to achieve the following, before you read along to the next chunk:\n\nCalculate the population change from 1970 to 2022 by computing the percentage increase (POP2022 - POP1970) / POP1970. Use mutate() for this and create a new column called “change”.\nCreate another column called “status” to classify the changes. Label changes less than 10% as “Low Change”. Label changes greater than 10% as “Growing”, and less than -10% as “Declining”.\nNote that we also have missing values (NAs) in change due to differences in census tracts, where some tracts in 2020 did not exist in earlier years. To account for that I included a fourth category in status named “Uncategorized”.\n\nHow to populate values based on multiple conditions? The traditional and more straightforward approach is using the if_else() statement. Combining it with mutate, it looks something like this:\n# Only for showing, do not run\npop_data |&gt; \n  mutate(status = ifelse(change &lt; -0.1, \"declining\",\n                ifelse(change &gt; 0.1, \"growing\",\n                ifelse(is.na(change), \"uncategorized\", \"low change\"))))\nHowever, dplyr offers a more concise solution. Check out case_when, which simplifies handling multiple conditions by eliminating the need for nested if else statements within multiple parentheses.\n\npop_data &lt;- pop_data |&gt; \n  mutate(change = (POP2022 - POP1970)/POP1970)|&gt; \n  mutate(status = case_when(\n    is.na(change) ~ \"Uncategorized\",\n    change &gt; 0.10 ~ \"Group 1 - Growing\",\n    change &lt; -0.10 ~ \"Group 2 - Declining\",\n    TRUE ~ \"Group 3 - Low Change\"\n  ))",
    "crumbs": [
      "Labs",
      "05 Neighborhood Change: tidycensus, tidyr packages"
    ]
  },
  {
    "objectID": "labs/lab5.html#create-summary-tables",
    "href": "labs/lab5.html#create-summary-tables",
    "title": "Download and analyze Census data with ",
    "section": "Create summary tables",
    "text": "Create summary tables\nWhen you are ready preparing the dataset in the last step. we can use group_by() and summarise() to find out how many tracts are in each category.\n\npop_data |&gt; \n  group_by(status) |&gt;    \n  summarise(num_tracts = n())\n\n# A tibble: 4 × 2\n  status               num_tracts\n  &lt;chr&gt;                     &lt;int&gt;\n1 Group 1 - Growing           392\n2 Group 2 - Declining         621\n3 Group 3 - Low Change        273\n4 Uncategorized                46\n\n\nSpeaking of summary tables, gt() is another powerful tool for creating presentable and customizable tables with ease. For instance, we can add a few more lines in the previous code to enhance this table by adding titles and coloring. There are also many other styling options available.\n\npop_data |&gt; \n  group_by(Status = status) |&gt; \n  summarise(`Number of Tracts` = n()) |&gt; \n  gt() |&gt; \n  tab_header(title = \"Change in Population, 1970-2022\",\n             subtitle = \"Cook County, IL\") |&gt; \n  tab_options(column_labels.background.color = 'dodgerblue4')\n\n\n\n\n\n\n\nChange in Population, 1970-2022\n\n\nCook County, IL\n\n\nStatus\nNumber of Tracts\n\n\n\n\nGroup 1 - Growing\n392\n\n\nGroup 2 - Declining\n621\n\n\nGroup 3 - Low Change\n273\n\n\nUncategorized\n46\n\n\n\n\n\n\n\nPopulation of each census tract, with a trend line showing the average population for each group:\nThe code is more detailed than needed for your exercise; feel free to skip ahead or refer to the relevant parts\n\npop_data_summary &lt;- pop_data |&gt; \n  filter(!is.na(change)) |&gt; \n  pivot_longer(col = POP1970:POP2022, \n               names_to = \"year\", values_to = \"population\") |&gt;\n  mutate(year = str_replace(year, \"POP\", \"\"))\n\npop_data_summary|&gt;\n  group_by(status, year) |&gt; \n  mutate(mean_population = mean(population, na.rm = TRUE)) |&gt; \n  ungroup() |&gt; \n  ggplot() +\n    geom_point(aes(x = year, y = population, color = status), \n               alpha = 0.2, size = 1) +\n    geom_line(aes(x = year, y = mean_population, color = status, \n                  group = status), linewidth = 1) +\n    facet_wrap(~status)+\n    labs(title = \"Cook County Population Trends by Census Tract and Group Over Time\")+\n    theme_classic()+\n    theme(legend.position=\"none\")",
    "crumbs": [
      "Labs",
      "05 Neighborhood Change: tidycensus, tidyr packages"
    ]
  }
]