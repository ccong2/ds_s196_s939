[
  {
    "objectID": "syllabus/index.html",
    "href": "syllabus/index.html",
    "title": "Applied Data Science for Cities",
    "section": "",
    "text": "Description\nUrban data science draws upon statistics, visualization, and computation to better understand and ultimately shape cities. This course emphasizes telling stories about cities using a set of descriptive approaches, quantitative and spatial analysis in R. We will learn how to describe community characteristics with small area census data, work with local administrative data, and think about how our analysis of quantitative data fit with other forms of data and engagement to fill in gaps in knowledge.\nLearning objectives:\n\nGain familiarity with R as a data analysis, mapping, and storytelling tool.\nBe able to identify relevant open datasets in support of your research question.\nArticulate the main arguments for your data analysis product and how it supports planning and decision-making regarding an issue of local/national significance.\n\n\n\nHow Will We Be Learning\nLecture: Class meetings are generally divided into lecture (Mondays) and laboratory sessions (Wednesdays) that focus on concepts and hand-on applications, respectively.\nLab: We will provide data science tutorials using R. Each lab tutorial aims to solve a specific urban data science problem in addition to building coding skills. Lab reports are due before the subsequent lab period and should be written independently.\nExtension readings: We provide a light material list that focuses on specific topics each week. These resources are meant to expand your knowledge and enhance your project completion capabilities.\nUrban data science project: The term project for the course will focus on integrating the tools of data science to explore a specific real-world planning issue or research question. This is a group project and students will define the scope of the project and identify specific deliverable(s) early in the semester. Reproducing an existing analysis or study using different data or an alternate study area is also acceptable for the term project.\n\n\nPrerequisites\nThis is a relatively fast-paced course so students can benefit from some prior knowledge working in R and RStudio. However, this is not a course prerequisite. Our first course sessions will focus on ensuring that we are all familiar with some of the basic work environment and methods which we’ll make use of over the semester.\n\n\nAssessment\n\n\n\n\n\n\n\nAssignment\nWeight\n\n\n\n\nLab Reports\nPackage Introduction\nProject Scope Memo\nProject Presentation\nAttendance\n40%\n10%\n10%\n30%\n10%\n\n\n\n\n\nKey Logistics\nLab Exercises: You will be working on lab on most Wednesday classes and submit a short report before the subsequent class. Details will be specified in each of the assignments distributed at the beginning of the lab session.\nPackage Introduction: Each student is expected to introduce one package to the class that is relevant to your research interests. The objectives are (1) to foster a collaborative approach to acquiring familiarity with the ever-increasing new packages, and (2) to identify valuable existing R packages applicable to your study. A separate guideline will be distributed at the beginning of the course.\nProject Scope Memo: Due Friday, Nov 17.\nThe purpose of this memo is to communicate the scope of your project (what you will do) and your strategies for collecting, visualizing, and analyzing data (how you will do it). A separate guideline will be distributed at the beginning of the course.\nProject Presentation: Week 8 (Dec 11 – Dec 13), class time.\n\n\nLate policy\nTo keep all students on a relatively level playing field, a late assignment will be accepted up until one week after the original due date for a loss of a half-letter grade (e.g., an A becomes an A-). After one week, late assignments will receive no credit and will not be accepted.\n\n\nSoftware\nWe use R and R Studio as the coding environment to develop analysis and applications. You can install the software on your personal computers from here and here.\n\n\nCommunication\nPlan on using our class Slack channel, email, and office hours to get help with troubleshooting problems as they arise in your work. I also encourage you to work with others in the class to troubleshoot problems - it is highly likely that others in the class have encountered similar problems, and this also allows us to create a repository of our problems and responses.\nEmail: I check emails quite frequently, but I will not always be able to respond to emails right away. Please plan accordingly so that you don’t miss deadlines.\nSlack: We have a Slack workspace that is accessible through Canvas for general communication, including homework Q&A, resource exchange, project collaboration, etc.\nOffice hours: Please consult the top of the syllabus for specific times. I will announce if there are any changes or exceptions. I’m happy to answer any specific coding questions, or chat and help shape the objective and scope of your projects.\n\n\nEthics\nAcademic Integrity: Violations to academic integrity are unacceptable at MIT and DUSP. Instances of misconduct include but are not limited to plagiarism, cheating, deliberately unauthorized use of course data and material.\nCollaboration Policy: While team collaboration is encouraged, students should specify their role and tasks in a project. A positive and constructive attitude for teamwork is essential for a successful completion of the course.\nDiversity and Inclusion: MIT highly values a diverse, friendly, respectful, and inclusive learning environment among students, faculty, and staff. We welcome all individuals regardless of their origin, citizenship, gender identity, sexual orientation, religious and pollical beliefs. Please contact me or departmental staff if you have any question / consideration regarding this."
  },
  {
    "objectID": "labs/lab6.html",
    "href": "labs/lab6.html",
    "title": "Build ShinyApp with Flexdashboard and ",
    "section": "",
    "text": "As a brief review, a ShinyApp comprises two essential components: the User Interface (UI) and the Server. The UI dictates the app’s appearance, defining the layout and visual elements, while the Server handles the underlying logic and data processing, enabling dynamic, real-time interactions.\nA ShinyApp typically starts from a three-line template:\n\n# library(shiny)\n\nui &lt;- fluidPage(\n  # *Input functions\n  # *Output functions\n)\n\nserver &lt;- function(input, output) {\n  # output$id &lt;- render*function(input$id)\n}\n\nshinyApp(ui, server)\n\nIn ui, you can define the…\n\nType of interface (e.g. fluidPage),\nStructure (panels, layout, etc.),\nComponents that acquire user inputs (choose a corresponding Input function). Each input component must have a unique ID.\nComponents that will display outputs (choose a corresponding Output function). Each output component must have a unique ID.\n\nIn server, most reactivity happens. The server works as an open, running function that takes the input and output defined and acquired from ui, and then produces the result in concert with a render function.\nThe relationship between the UI and the Server is established by shinyApp(ui, server) so that the app will react to the user input (ui) by re-rendering (server) continuously while the app is running."
  },
  {
    "objectID": "labs/lab6.html#preparation",
    "href": "labs/lab6.html#preparation",
    "title": "Build ShinyApp with Flexdashboard and ",
    "section": "Preparation",
    "text": "Preparation\nLet’s copy and paste the files you used for creating your flexdashboard for lab 4 to your current working directory. This is what my current working directory looks like, it contains my Rproj. file, my flexdashboard.Rmd, and a data folder. In my data folder, I have the airbnb.rds that I used for the flexdashboard.\n\nKnit your flexdashboard and make sure it works. Suppose that you have created a two-column dashboard with one map on the left and two charts on the right. We will work from here and add reactive features."
  },
  {
    "objectID": "labs/lab6.html#customize-your-flexdashboard-structure",
    "href": "labs/lab6.html#customize-your-flexdashboard-structure",
    "title": "Build ShinyApp with Flexdashboard and ",
    "section": "Customize your flexdashboard structure",
    "text": "Customize your flexdashboard structure\nYou’ll first need to modify 5 places in your flexdashboard.Rmd to restructure it to work with Shiny.\n\n\nAdd runtime: shiny to the YAML header at the top of the document. This specifies that the Shiny package will be used to handle reactive content.\nLoad the libraries. The packages you used may be different from what is shown above. But you can simply add library(shiny) to the list of packages you’ve already used in your flexdashboard.\nAdd a new code chunk {r data} where we will load and work with local data. Include the code chunk options message=FALSE, warning=FALSE, results='hide' .\nCreate a sidebar column. You can simply copy and paste a Column header and the dashed-line divider from your code. But make sure to add the attribute .sidebar. I have also changed the column width to 200 pixels.\nSlightly adjust the column width for your maps and charts. I have changed the two columns to 500 and 300 pixels, respectively. The total number of pixels of your sidebar + two columns should be 1,000."
  },
  {
    "objectID": "labs/lab6.html#load-and-prepare-your-data",
    "href": "labs/lab6.html#load-and-prepare-your-data",
    "title": "Build ShinyApp with Flexdashboard and ",
    "section": "Load and prepare your data",
    "text": "Load and prepare your data\nJust like a normal .Rmd, your Shiny app code will run line by line or chunk by chunk. To keep it organized, we are dedicating one code chunk for loading libraries, and the second one for working with global data. In your code chunk {r data}, read in your airbnb data:\nairbnb &lt;- readRDS(\"data/airbnb.rds\")\nIf your flexdashboard involves downloading census geography data, the best practice is to relocate those lines of code here as well."
  },
  {
    "objectID": "labs/lab6.html#work-with-the-implicit-ui-and-server",
    "href": "labs/lab6.html#work-with-the-implicit-ui-and-server",
    "title": "Build ShinyApp with Flexdashboard and ",
    "section": "Work with the implicit UI and Server",
    "text": "Work with the implicit UI and Server\nBuilding on top of a flexdashboard framework, the process of constructing our app is simplified. We don’t need to explicitly define a ui and a server - the connection between inputs and outputs happens implicitly. What it means to us is that we only need to:\n\nIn our sidebar column, directly enter input functions\nIn your Chart A-C areas, directly enter render functions\n\nYour initial structure should look like this:"
  },
  {
    "objectID": "labs/lab6.html#fill-in-your-input-functions",
    "href": "labs/lab6.html#fill-in-your-input-functions",
    "title": "Build ShinyApp with Flexdashboard and ",
    "section": "Fill in your input functions",
    "text": "Fill in your input functions\nFill in the sliderInput and selectInput with their required arguments. I’m giving them an inputId of price_range and nbh_name, respectively.\n\nsliderInput(inputId = \"price_range\", label = \"Select a price range\",\n                  min = 0, max = 1000, value = c(0, 500))\nselectInput(inputId = \"nbh_name\", label = \"Select a neigborhood\",\n              choices = unique(airbnb$neighborhood))"
  },
  {
    "objectID": "labs/lab6.html#complete-your-render-functions",
    "href": "labs/lab6.html#complete-your-render-functions",
    "title": "Build ShinyApp with Flexdashboard and ",
    "section": "Complete your render functions",
    "text": "Complete your render functions\nConsider one of your Plotly charts as an example. First, wrap all the code you used to create this chart into the curly braces of renderPlotly({}).\nWhenever you choose a neighborhood, it results in the filtering of a subset from the original airbnb data. To make your original code for creating the bar chart responsive to your inputs, you will need to incorporate a few lines like the following (right after airbnb |&gt;), calling the input IDs.\n\nrenderPlotly({\n  g &lt;- airbnb |&gt; \n        filter(neighborhood == input$nbh_name, \n        price &gt;= input$price_range[1],       \n        price &lt; input$price_range[2]) |&gt;  \n    group_by(room_type) |&gt; \n    summarise(median_price = median(price)) |&gt; \n    ggplot() + \n      geom_col(aes(x = room_type, y = median_price), fill = \"#62A0CA\", width = 0.5) + \n    theme_bw()+\n    labs(x = \"Room Type\", y = \"Median Price\")+\n    theme(axis.text.x = element_text(size = 7))\n    ggplotly(g)\n})\n\nFor your Leaflet map, there are a few additional things to do. But first, wrap all your code related to creating your leaflet map into the curly braces of renderLeaflet({}).\n\nThe dataset for your map should also react to your input, requiring a similar filtering process. I’m assigning the filtered results to a new object, df_map, replacing the original airbnb object that exists at multiple places in this code chunk. Here it requires your careful decisions about where to perform replacements and where not to.\n\n\ndf_map &lt;- airbnb |&gt; \n    filter(neighborhood == input$nbh_name,\n    price &gt;= input$price_range[1],\n    price &lt; input$price_range[2])\n\n\nAdjust the setView function to zoom in on the selected neighborhood rather than the entire region. My approach is using the average longitude and latitude of the subset data.\n\n\n# Add these two lines before leaflet()\nmid_long &lt;- mean(df_map$longitude, na.rm = TRUE)\nmid_lat &lt;- mean(df_map$latitude, na.rm = TRUE)\n\n# Replace the original lat and lon which represents the center of your region\nleaflet() |&gt;\n  ...\n  setView(lng = mid_long, lat = mid_lat, zoom = 14)\n\nGive it a try! Now your Shiny dashboard should work. Save your file, then click “Run Document”. If some errors show up, that’s fine, and we will figure it out! You can find my final code here for troubleshooting."
  },
  {
    "objectID": "labs/lab4.html",
    "href": "labs/lab4.html",
    "title": "Creating Interactive Graphs and Maps with ",
    "section": "",
    "text": "Today we’re going to explore a couple of new packages and how they can be used to enhance our visualizations. We will use an example to show how to use these tools effectively:\n\nplotly and adding interactivity through ggplotly\nInteractive maps with leaflet\nIntroduce dashboards with flexdashboard\n\nIn terms of creating good and effective data analyses, mastering the ‘fundamentals’ is more important than pursuing ‘advanced’ tools. Step one is always to have a good understanding of the data you are representing. Interactivity tools contribute to improving interpretation, but they cannot replace proficiency in the subject matter.\n\n# You may need to install plotly, leaflet, flexdashboard\n\nlibrary(tidyverse)\nlibrary(plotly)\nlibrary(leaflet)\nlibrary(sf)\nlibrary(tigris)"
  },
  {
    "objectID": "labs/lab4.html#data-cleaning",
    "href": "labs/lab4.html#data-cleaning",
    "title": "Creating Interactive Graphs and Maps with ",
    "section": "Data Cleaning",
    "text": "Data Cleaning\nIn class, we covered two important data-cleaning steps. We first removed the dollar signs from the price column to facilitate our subsequent numerical analyses. We then converted the character data in the last_review column into the date format to manage temporal-related analyses. We used the stringr package and the lubridate package, respectively.\n\nairbnb &lt;- data |&gt; \n  mutate(price = str_replace(string = price, \n                             pattern = \"\\\\$\", \n                             replacement = \"\")) |&gt; \n  mutate(price =  str_replace(price, \",\", \"\")) |&gt; \n  mutate(price = as.numeric(price))\n\n\nairbnb &lt;- airbnb |&gt; \n  mutate(last_review = ymd(last_review)) |&gt; \n  mutate(last_review_year = year(last_review),\n         last_review_month = month(last_review))\n\nAfter these two data-cleaning steps, please save a copy of this dataset to your data folder. It will make it easier for us to import it into another script shortly.\nsaveRDS(airbnb, \"data/airbnb.rds\")"
  },
  {
    "objectID": "labs/lab4.html#chart-b-median-room-price-by-type",
    "href": "labs/lab4.html#chart-b-median-room-price-by-type",
    "title": "Creating Interactive Graphs and Maps with ",
    "section": "Chart B: Median Room Price by Type",
    "text": "Chart B: Median Room Price by Type\n\ng &lt;- airbnb |&gt; \n  group_by(room_type) |&gt; \n  summarise(median_price = median(price)) |&gt; \n  ggplot() + \n    geom_col(aes(x = room_type, y = median_price), fill = \"#62A0CA\", width = 0.5) + \n  theme_bw()+\n  labs(x = \"Room Type\", y = \"Median Price\")\n  \n\nggplotly(g)"
  },
  {
    "objectID": "labs/lab4.html#chart-c-donut-chart-of-last-reviews",
    "href": "labs/lab4.html#chart-c-donut-chart-of-last-reviews",
    "title": "Creating Interactive Graphs and Maps with ",
    "section": "Chart C: Donut Chart of Last Reviews",
    "text": "Chart C: Donut Chart of Last Reviews\n\nairbnb |&gt; \n  group_by(last_review_year) |&gt; \n  summarise(count_review = n()) |&gt; \n  plot_ly() |&gt;  \n  add_pie(labels = ~last_review_year, \n          values = ~count_review,\n          hole = 0.6)"
  },
  {
    "objectID": "labs/lab4.html#work-process",
    "href": "labs/lab4.html#work-process",
    "title": "Creating Interactive Graphs and Maps with ",
    "section": "Work Process",
    "text": "Work Process\n\nBasemap\nFirst, we initiate leaflet and add a basemap. We are adding a basemap created by CartoDB. Alternatively, there are all the other web map options from ESRI, OpenStreetMap, etc.\nThe setView defines the initial center and zoom level of a map, where larger zoom values result in a more detailed view.\n\nleaflet() |&gt;\n  addProviderTiles(providers$CartoDB.Positron) |&gt;\n  setView(lng = -87.636483, lat = 41.862984,  zoom = 10) \n\n\n\n\n\n\n\nCircle markers\nThen we can add the Airbnb listings as circle markers on the map, where we can define:\n\nthe circle size (radius),\nwhether to have fill (fill=TRUE), with what color (fillColor) and transparency (fillOpacity),\nwhether to have strokes (stroke=TRUE), stroke width (weight), and with what color (color) and transparency (opacity),\n\netc., etc….\n\nleaflet() |&gt;\n  addProviderTiles(providers$CartoDB.Positron) |&gt;\n  setView(lng = -87.636483, lat = 41.862984,  zoom = 10) |&gt; \n  addCircleMarkers(data = airbnb,\n                   fill = TRUE,\n                   fillOpacity = 0.5,\n                   stroke = FALSE,\n                   radius = 1) \n\n\n\n\n\n\n\nPopup labels\nTo enable popup tooltips to display information upon user clicks, we need to define the appearance of labels using formatted text. In this text listing_popup, each line shows “item name: airbnb attribute values”. This label format is then added into addCircleMarkers as the input of the popup argument.\nThe HTML tags (&lt;br&gt; and &lt;b&gt;…&lt;b&gt;) may look unfamiliar but they suggest that this string is intended for display in an HTML context, where line breaks and bold formatting are relevant.\n\nlisting_popup &lt;-\n  paste0(airbnb$name, \"&lt;br&gt;\",\n    \"&lt;b&gt;Neighborhood: &lt;/b&gt;\", airbnb$neighborhood,\"&lt;br&gt;\",\n    \"&lt;b&gt;Room Type: &lt;/b&gt;\", airbnb$room_type, \"&lt;br&gt;\",\n    \"&lt;b&gt;Price: &lt;/b&gt;\", airbnb$price, \"&lt;br&gt;\",\n    \"&lt;b&gt;Average Rating: &lt;/b&gt;\", airbnb$review_scores_rating\n  )\n\nleaflet() |&gt;\n  addProviderTiles(providers$CartoDB.Positron) |&gt;\n  setView(lng = -87.636483, lat = 41.862984,  zoom = 10) |&gt; \n  addCircleMarkers(data = airbnb,\n                   fillOpacity = 0.5,\n                   stroke = FALSE,\n                   radius = 1,\n                   popup = listing_popup) \n\n\n\n\n\n\n\nColor\nLeaflet uses color mapping functions to assign colors to categorical variables and numeric variables, respectively. In this following line, we create a “relationship” between the categorical values in the airbnb$room_type variable and a sequential color palette RdYlGn (“red-yellow-green”).\npal &lt;- colorFactor(palette = \"RdYlGn\", domain = airbnb$room_type)\nThen this “relationship” pal is passed to two things:\n1) fillColor = ~pal(room_type). You again encounter the use of the tilde (~) symbol, indicating that this input represents a relationship on a given variable (room type).\n2) addLegend(pal = pal) so the that colors in the legend will show up accordingly.\n\npal &lt;- colorFactor(palette = \"RdYlGn\", domain = airbnb$room_type)\n\nleaflet() |&gt;\n  addProviderTiles(providers$CartoDB.Positron) |&gt;\n  setView(lng = -87.636483, lat = 41.862984,  zoom = 10) |&gt; \n  addCircleMarkers(data = airbnb,\n                   fillColor = ~pal(room_type),\n                   fillOpacity = 1,\n                   stroke = FALSE,\n                   radius = 1,\n                   popup = listing_popup) |&gt; \n  addLegend(\n    position = 'topright',\n    pal = pal,\n    values = airbnb$room_type,\n    title = \"Room Type\"\n  )\n\n\n\n\n\n\n\nPolygons\nLet’s grab a tigris place boundary for Chicago, and add it to our map:\n\noptions(tigris_use_cache=TRUE)\nchi_boundary &lt;- places(state = \"IL\") |&gt; filter(NAME == \"Chicago\")\n\nIn this additional function addPolygons, we can adjust the boundary color and fill color:\n\nleaflet() |&gt;\n  addProviderTiles(providers$CartoDB.Positron) |&gt;\n  setView(lng = -87.636483, lat = 41.862984,  zoom = 10) |&gt; \n  addCircleMarkers(data = airbnb,\n                   fillColor = ~pal(room_type),\n                   fillOpacity = 1,\n                   stroke = FALSE,\n                   radius = 1,\n                   popup = listing_popup) |&gt;\n  addPolygons(data = chi_boundary,\n              color = \"blue\",\n              fill = FALSE,\n              weight = 1) |&gt; \n  addLegend(\n    position = 'topright',\n    pal = pal,\n    values = airbnb$room_type,\n    title = \"Room Type\"\n  )\n\n\n\n\n\nWhat we have created so far already resembles Inside Airbnb’s visualization. For the last step, we will add a layer control to turn on and off layer groups as we wish.\n\n\nLayer Control\nbaseGroups: We added two more basemaps using addProviderTiles, organizing each as a group named “ESRI World Imagery,” “CartoDB Dark,” and “CartoDB Positron” correspondingly. These group names are passed to the addLayersControl for users to toggle on and off.\noverlayGroups: This argument organizes layers on top of the base map. In this case, we added a group argument in both the addCircleMarkers and addPolygons functions, giving each a name, then passed their names to addLayersControl. Similar to the case of basemaps, users can toggle on and off the points and the polygon layers with their respective names.\noptions: In this example, collapsed = TRUE means that the layers control will initially be collapsed or hidden, showing a cleaner interface.\n\nleaflet() |&gt;\n  addProviderTiles(\"Esri.WorldImagery\", group = \"ESRI World Imagery\") |&gt; \n  addProviderTiles(providers$CartoDB.DarkMatter, group = \"CartoDB Dark\") |&gt; \n  addProviderTiles(providers$CartoDB.Positron) |&gt;\n  setView(lng = -87.636483, lat = 41.862984,  zoom = 10) |&gt; \n  addCircleMarkers(data = airbnb,\n                   fillColor = ~pal(room_type),\n                   fillOpacity = 1,\n                   stroke = F,\n                   radius = 1,\n                   popup = listing_popup,\n                   group = \"Airbnb Listings\") |&gt;\n  addPolygons(data = chi_boundary,\n              color = \"blue\",\n              fill = FALSE,\n              weight = 1,\n              group = \"Chicago Boundary\") |&gt; \n  addLegend(\n    position = 'topright',\n    pal = pal,\n    values = airbnb$room_type,\n    title = \"Room Type\"\n  ) |&gt; \n  addLayersControl(\n    baseGroups = c(\"ESRI World Imagery\", \"CartoDB Dark\", \"CartoDB Positron\"),\n    overlayGroups = c(\"Chicago Boundary\", \"Airbnb Listings\"),\n    options = layersControlOptions(collapsed = TRUE)\n  )"
  },
  {
    "objectID": "labs/lab4.html#chart-a-leaflet-map",
    "href": "labs/lab4.html#chart-a-leaflet-map",
    "title": "Creating Interactive Graphs and Maps with ",
    "section": "Chart A: Leaflet Map",
    "text": "Chart A: Leaflet Map\nThis is the complete code for the leaflet map we have created:\n\nlisting_popup &lt;-\n  paste0(airbnb$name, \"&lt;br&gt;\",\n    \"&lt;b&gt;Neighborhood: &lt;/b&gt;\", airbnb$neighborhood,\"&lt;br&gt;\",\n    \"&lt;b&gt;Room Type: &lt;/b&gt;\", airbnb$room_type, \"&lt;br&gt;\",\n    \"&lt;b&gt;Price: &lt;/b&gt;\", airbnb$price, \"&lt;br&gt;\",\n    \"&lt;b&gt;Average Rating: &lt;/b&gt;\", airbnb$review_scores_rating\n  )\n\npal &lt;- colorFactor(palette = \"RdYlGn\", domain = airbnb$room_type)\n\noptions(tigris_use_cache=TRUE)\nchi_boundary &lt;- places(state = \"IL\") |&gt; filter(NAME == \"Chicago\")\n\n\nleaflet() |&gt;\n  addProviderTiles(\"Esri.WorldImagery\", group = \"ESRI World Imagery\") |&gt;\n  addProviderTiles(providers$CartoDB.DarkMatter, group = \"CartoDB Dark\") |&gt;\n  addProviderTiles(providers$CartoDB.Positron) |&gt;\n  setView(lng = -87.636483, lat = 41.862984,  zoom = 10) |&gt; \n  addCircleMarkers(data = airbnb,\n                   fillColor = ~pal(room_type),\n                   fillOpacity = 1,\n                   stroke = F,\n                   radius = 1,\n                   popup = listing_popup,\n                   group = \"Airbnb Listings\") |&gt;\n  addPolygons(data = chi_boundary,\n              color = \"blue\",\n              fill = FALSE,\n              weight = 1,\n              group = \"Chicago Boundary\") |&gt; \n  addLegend(\n    position = 'topright',\n    pal = pal,\n    values = airbnb$room_type,\n    title = \"Room Type\"\n  ) |&gt; \n  addLayersControl(\n    baseGroups = c(\"ESRI World Imagery\", \"CartoDB Dark\", \"CartoDB Positron\"),\n    overlayGroups = c(\"Chicago Boundary\", \"Airbnb Listings\"),\n    options = layersControlOptions(collapsed = TRUE)\n  )"
  },
  {
    "objectID": "labs/lab4.html#create-your-own-dashboard",
    "href": "labs/lab4.html#create-your-own-dashboard",
    "title": "Creating Interactive Graphs and Maps with ",
    "section": "Create your own dashboard",
    "text": "Create your own dashboard\nJust below the YAML header, there is a code chuck named setup and marked include = FALSE. Here you can supply any data related to package loading and data preparation. Make sure you include here any of your scripts related to loading packages and reading data:\n\nNow you only need to identify and isolate the code that we produced today to populate the respective three chart sections. In other words, you should copy all the code we’ve worked through under the heading “Chart A: Leaflet Map”, and paste them in the blank code chunk under “Chart A”. Then copy and paste all the codes under “Chart B: Median Room Price by Type” and “Chart C: Donut Chart of Last Reviews” to the flexdashboard sections Chart B and Chart C.\nWhen you are ready, knit the document again, and a dashboard should appear!"
  },
  {
    "objectID": "labs/lab2.html",
    "href": "labs/lab2.html",
    "title": "Exploratory Data Analysis with ",
    "section": "",
    "text": "This week’s Lab Exercise focuses on the dplyr package and the ggplot2 package. It also begins to engage with data visualization best practices by demonstrating how to create and interpret a variety of graphics.\nExploratory data analysis (EDA) is a phase of a larger data science workflow that emphasizes getting to know the data before rushing to analyze it. EDA typically involves the creation and interpretation of graphics in order to build familiarity and gain fundamental insights that can inform more sophisticated analyses later on. There are several overarching goals of exploratory data analysis, including:\n\nTo determine if there are any problems with your dataset.\nTo determine whether the question you are asking can be answered by the data that you have.\nTo begin formulating an answer to your question."
  },
  {
    "objectID": "labs/lab2.html#download-data-and-load-packages",
    "href": "labs/lab2.html#download-data-and-load-packages",
    "title": "Exploratory Data Analysis with ",
    "section": "Download data and load packages",
    "text": "Download data and load packages\nNow please navigate to Urban Institute’s website about Opportunity Zones, find the link “Download tract-level data on all Opportunity Zones”, and download this dataset to your “data” folder within your Lab 2 project folder.\nStart a new .qmd (or .Rmd) file and remove the template texts.\nTo stay organized, we should load packages at the beginning of our markdown document. These are the three packages we are going to use today.\n\nlibrary(readxl)\nlibrary(tidyverse)\nlibrary(DataExplorer)"
  },
  {
    "objectID": "labs/lab2.html#initial-data-cleaning",
    "href": "labs/lab2.html#initial-data-cleaning",
    "title": "Exploratory Data Analysis with ",
    "section": "Initial data cleaning",
    "text": "Initial data cleaning\nThe Urban Institute has coded the designated variable as either taking a value of 1 when designated, or NA when not. We can recode the NA values in DesignatedOZ for legibility. In the following code, we use the dplyr function: mutate to modify DesignatedOZ in place. We replaced the numbers with texts since the NA and 1 here have no mathematical meaning.\n\nozs &lt;- ozs |&gt; mutate(DesignatedOZ = \n                ifelse(is.na(DesignatedOZ), \"Not Designated\", \"Designated\"))\n\nThis modification will make it much easier for us to make a quick count of both types of areas.\n\nozs |&gt; \n  count(DesignatedOZ) \n\n# A tibble: 2 × 2\n  DesignatedOZ       n\n  &lt;chr&gt;          &lt;int&gt;\n1 Designated      8764\n2 Not Designated 33414\n\n\nOr, a cross-tabulation of both types of areas by state.\n\nozs |&gt; \n  count(state, DesignatedOZ)\n\n# A tibble: 108 × 3\n   state          DesignatedOZ       n\n   &lt;chr&gt;          &lt;chr&gt;          &lt;int&gt;\n 1 Alabama        Designated       158\n 2 Alabama        Not Designated   677\n 3 Alaska         Designated        25\n 4 Alaska         Not Designated    43\n 5 American Samoa Designated        16\n 6 Arizona        Designated       168\n 7 Arizona        Not Designated   702\n 8 Arkansas       Designated        85\n 9 Arkansas       Not Designated   435\n10 California     Designated       879\n# ℹ 98 more rows\n\n\nThere are a few columns (such as SE_Flag) that we won’t need for this analysis. We can use select in dplyr function to select a subset of columns to work on. select allows you to retain specified columns. If there is a minus sign in front, that means to drop these specified columns.\n\nozs &lt;- ozs |&gt; select(-c(dec_score, SE_Flag, pctown, Metro, Micro, NoCBSAType))\n\nOne of the characteristics tracked in the Urban Institute data is the median household income for each tract. We might question whether there’s a difference in the median household income for designated and not-designated census tracts. Let’s see if we can calculate the mean of the median household income values:\n\nmean(ozs$medhhincome)\n\n[1] NA\n\n\nIt returns NA. But it doesn’t indicate an issue with our code. The reason for this is the presence of missing values in this column. When dealing with numerical variables, NA values affect calculation results because we can’t decide if the missing values are larger or smaller than the others and we don’t know how to calculate them.\nTake another look at the ozs data. Sort the medhhincome column by clicking the little triangles appearing on the column name. Drag down to the bottom of the dataset, then you will see quite a few of NAs in the Census demographic columns.\nHow many missing values are there, and how many would be a hurdle for my analysis? It will be great to have a sense of completeness in terms of what proportion of a field actually holds data. DataExplorer is a handy tool that offers functions to quickly understand the completeness of datasets.\n\nDataExplorer::plot_missing(ozs)\n\n\n\n\nplot_missing calculates the proportion of missing values in a given variable, and makes some judgemental calls of whether the missing is significant, indicated by “Good”, “OK”, and “Bad”. (Check out the documentation by typing ?plot_missing in your console. What are the default ranges for these three categories?) Overall, most of our columns have a very small percentage of missing values (less than 1%) and would not create significant representative issues. However, when performing calculations, we need to include the na.rm = TRUE argument, indicating that we are calculating based on the available 99%.\n\nmean(ozs$medhhincome, na.rm = TRUE)\n\n[1] 42152.76\n\n\nNote: R can do the same thing in different ways. This is especially true as you get further and further into the weeds - some niche packages are able to perform general tasks, in more or less verbose ways. For example, in dplyr language, you may do the same calculation like this:\n\nozs |&gt; summarise(meanvalue = mean(medhhincome, na.rm = TRUE))\n\n# A tibble: 1 × 1\n  meanvalue\n      &lt;dbl&gt;\n1    42153."
  },
  {
    "objectID": "labs/lab2.html#exercise-1",
    "href": "labs/lab2.html#exercise-1",
    "title": "Exploratory Data Analysis with ",
    "section": "Exercise 1",
    "text": "Exercise 1\nWhile we are now examining the nationwide dataset, you are going to conduct some focused analysis for Massachusetts. Please insert a new code chunk below to create a new object ozs_ma that extracts all tracts in Massachusetts. Then add some more code chunks and texts to “catch” your responses to the following questions.\n\nWhich of the variables (columns) are continuous and which are categorical (e.g., character)?\n\nRecall that a variable is categorical if it can only take one of a small set of values and continuous if it can take any of an infinite set of ordered values.\nWhich function or approach did you use to answer this question?\n\nIn Massachusetts, how many eligible census tracts are designated as Opportunity Zones, and how many are not designated?\n\nWhich function or approach did you use to answer this question?\n\nChoose one of the following variables: medhhincome, vacancyrate, unemprate, pctwhite, pctblack, pctHispanic, pctover64, HSorlower . What are the average values of your variable for both designated and not designated tracts in Massachusetts?\n\nWhich function or approach did you use to answer this question?"
  },
  {
    "objectID": "labs/lab2.html#boxplot",
    "href": "labs/lab2.html#boxplot",
    "title": "Exploratory Data Analysis with ",
    "section": "Boxplot",
    "text": "Boxplot\nNow we are ready to create some visual representations of our data. The code below creates a boxplot to contrast the distribution of poverty rates between designated opportunity zones and undesignated zones. We are using what should now be familiar conventions to construct the graphic beginning with the ggplot function, then adding more features with the + operator and other functions listed in the package reference.\n\nggplot(data = ozs): This is the main plotting function. ozs is your dataset we use.\ngeom_boxplot(): Recall that geometric layers are called geoms. It tells R what kind of geometry you want to use visualize the data.\naes(x = DesignatedOZ, y = PovertyRate): The aes() function is where you tell ggplot which variable goes on the x axis followed by which variable goes on the y axis.\nThe third aesthetic element is fill, which indicates the filled color of the boxplot. Accordingly, we use the fill argument in the labs function to set the name of the legend.\nWe used a new function scale_y_continuous to specify y axis properties. Here we are making sure the poverty rate are labeled as percentages. If you remove this line, they will by default show as decimal numbers.\n\n\nggplot(data = ozs) +\n  geom_boxplot(aes(x = DesignatedOZ, y = PovertyRate, fill = DesignatedOZ), na.rm = TRUE) + \n  scale_y_continuous(labels = scales::percent) +\n  labs(x = \"Opportunity Zone Eligible Tracts\", y = \"Poverty Rate\", fill = \"Tracts\")\n\n\n\n\nBy comparing the 50th percentile (the horizontal line inside each box) we can see that tracts designated as Opportunity Zones have a higher poverty rate compared with those not designated. The heights of the boxes themselves give us an indication of how closely around the median all values in the dataset are concentrated—the degree of dispersion or spread. The vertical lines are called whiskers and extend upward and downward to the lowest values that are not candidates for outlier status. An outlier is an unusual value that could potentially influence the results of an analysis. These are indicated with dots in the boxplot.\nOutliers can be legitimate values, and require the exercise of judgment on the part of the analyst and may be removed or excluded from the analysis or imputed. There are a variety of criteria that have been offered, but you will address this question from a more statistical perspective in your other courses that introduce linear methods."
  },
  {
    "objectID": "labs/lab2.html#density-plot",
    "href": "labs/lab2.html#density-plot",
    "title": "Exploratory Data Analysis with ",
    "section": "Density plot",
    "text": "Density plot\nBy modifying the last code chunk, we can make a density plot to describe the distribution of poverty rate. A density plot can be understood as a smoothed version of the histogram, and provides a more direct view of of the shape and peaks in the data. The x-axis typically represents the range of values for the variable of interest, while the y-axis represents the probability density (how likely it is for the variable to take on a particular value within that range).\n\nIn the code below, we didn’t provide a variable that what goes to the y-axis. Where does the value “density” come from? Many graphs, like boxplot, plot the raw values of your dataset. But other graphs, like histograms and density plots, calculate new values to plot. Here a density takes the count of data points at discrete poverty rate levels and smooths it out into a continuous curve. Then calculated values (probability density) go to the y-axis.\n\n\nggplot(data = ozs) +\n  geom_density(aes(x = PovertyRate, fill = DesignatedOZ), na.rm = TRUE) + \n  scale_x_continuous(labels = scales::percent) +\n  labs(x = \"Poverty Rate\", fill = \"Tracts\")"
  },
  {
    "objectID": "labs/lab2.html#combinations-of-basic-graphs-to-create-composite-views",
    "href": "labs/lab2.html#combinations-of-basic-graphs-to-create-composite-views",
    "title": "Exploratory Data Analysis with ",
    "section": "Combinations of basic graphs to create composite views",
    "text": "Combinations of basic graphs to create composite views\nOne of the coolest thing about ggplot is that we can plot multiple geom_ on top of each other. For instance, we can combine the two plots above, to show both visually appealing curves and essential statistics (medians, quartiles, outliers, etc.) The following code uses two geom_(Check out geom_violin for more!), and introduces several new and helpful arguments for fine-tuning the cosmetics.\n\ntrim = FALSE: If TRUE (default), trim the tails of the violins to the range of the data. If FALSE, don’t trim the tails and show the complete distribution.\nalpha = 0.5: the transparency of the plotting area.\ncoord_flip(): whether the y axis is displayed horizonally or vertically.\nlegend.position = \"none\": the position of legend (“left”, “right”, “bottom”, “top”, or two-element numeric vector), or not showing the legend (“none”).\n\n\nggplot(ozs) +\n  geom_violin(aes(x = DesignatedOZ, y = PovertyRate, fill = DesignatedOZ), trim = FALSE, alpha = 0.5) +\n  geom_boxplot(aes(x = DesignatedOZ, y = PovertyRate), colour = \"black\", width = .15, alpha = 0.8) +\n  scale_y_continuous(labels = scales::percent) +\n  labs(\n    x = \"Opportunity Zone Eligible Tracts\",\n    y = \"Poverty Rate\",\n    title = \"Distribution of Poverty Rate\"\n  ) +\n  coord_flip() +\n  theme(legend.position = \"none\")"
  },
  {
    "objectID": "labs/lab2.html#exercise-2",
    "href": "labs/lab2.html#exercise-2",
    "title": "Exploratory Data Analysis with ",
    "section": "Exercise 2",
    "text": "Exercise 2\nFocus on your selected variable in Exercise 1 and your data specific to Massachusetts. Explore the distribution of your variable of interest.\n\nCreate one graphical representation that contrasts the results in designated tracts and in undesignated tracts.\nInterpret the graphic you have created and include 2-3 sentences of text explanation, for example:\n\nIs it a more flat/homogeneous distribution, or a more skewed distribution in terms of fewer observations belonging to the higher-value groups? Does that align with your expectation for this variable?\nWhat can we say about the difference in demographic/economic conditions between designated and not designated census tracts in Massachusetts?\nFeel free to consult the R Graph Gallery and Aesthetic specifications for additional resources."
  },
  {
    "objectID": "labs/lab2.html#scatterplot",
    "href": "labs/lab2.html#scatterplot",
    "title": "Exploratory Data Analysis with ",
    "section": "Scatterplot",
    "text": "Scatterplot\nWe are often interested in bivariate relationships or how two variables relate to one another. Scatterplots are often used to visualize the association between two continuous variables. They can reveal much about the nature of the relationship between two variables.\nLet’s use your subset of Massachusetts data to perform this part of analysis. We can definitely use the entire dataset, but there will be over 40,000 points showing on the graph, which will not be pleasing to the eye.\n\nozs_ma &lt;- ozs |&gt; filter(state == \"Massachusetts\") \n\nWe begin by creating a scatterplot of poverty rate and unemployment rate. Note that we used theme_bw, which is a theme template for a cleaner look.\n\nggplot(ozs_ma) +\n  geom_point(aes(x = unemprate, y = PovertyRate)) +\n  labs(x = \"Unemployment rate\",\n       y = \"Poverty rate\",\n       title = \"Poverty rate vs. unemployment rate in Opportunity Zone eligible tracts\", \n       subtitle = \"State of Massachusetts\",\n       caption = \"Source: Urban Institute (2018)\") + \n  theme_bw()\n\n\n\n\nIt is generally easy to recognize patterns in a graphical display. As we move from left to right along the x-axis (i.e., as the unemployment rate creases), the amount of poverty rate reported also increases.\nAs a complement to a scatterplot, we can use the stats::cor function to calculate the (Pearson by default, see ?cor for other options) correlation between any continuous variables in the dataset. The DataExplorer package is also designed to help us quickly understand patterns in our data. We demonstrate both in the following code.\nIf you are unfamiliar with reading a correlation matrix, the values range between -1 and 1 where:\n\n-1 indicates a perfectly negative linear correlation between two variables\n0 indicates no linear correlation between two variables\n1 indicates a perfectly positive linear correlation between two variables\n\n\nozs_ma |&gt; select(Population:medrent, pctwhite:BAorhigher) |&gt; \n  stats::cor(use = \"complete.obs\")\n\n                Population medhhincome PovertyRate  unemprate    medvalue\nPopulation    1.0000000000   0.1971310 -0.17219731 -0.1255690  0.05060663\nmedhhincome   0.1971310072   1.0000000 -0.74280330 -0.5823090  0.45672297\nPovertyRate  -0.1721973144  -0.7428033  1.00000000  0.5848120 -0.15673933\nunemprate    -0.1255690086  -0.5823090  0.58481204  1.0000000 -0.32627434\nmedvalue      0.0506066324   0.4567230 -0.15673933 -0.3262743  1.00000000\nmedrent       0.1434824928   0.6494977 -0.32058250 -0.4139894  0.60946764\npctwhite      0.0007742062   0.4334837 -0.58166274 -0.4583927  0.01479243\npctBlack      0.0305972149  -0.2081070  0.24067220  0.3002354  0.07662521\npctHispanic  -0.0610330071  -0.4470198  0.53817440  0.4107687 -0.24108925\npctAAPIalone  0.1179507879   0.1360442  0.05777953 -0.1516819  0.38403023\npctunder18    0.0075944281  -0.3886837  0.28931873  0.4263952 -0.48597184\npctover64    -0.0200934534   0.1116477 -0.40837729 -0.2236285 -0.08790793\nHSorlower    -0.0510755304  -0.6192728  0.38428550  0.5080771 -0.59138685\nBAorhigher    0.0317046620   0.5837037 -0.24483958 -0.4850706  0.71630822\n                 medrent      pctwhite    pctBlack pctHispanic pctAAPIalone\nPopulation    0.14348249  0.0007742062  0.03059721 -0.06103301   0.11795079\nmedhhincome   0.64949767  0.4334836763 -0.20810703 -0.44701977   0.13604425\nPovertyRate  -0.32058250 -0.5816627414  0.24067220  0.53817440   0.05777953\nunemprate    -0.41398938 -0.4583927470  0.30023542  0.41076874  -0.15168191\nmedvalue      0.60946764  0.0147924343  0.07662521 -0.24108925   0.38403023\nmedrent       1.00000000  0.0592656294 -0.01942961 -0.21839927   0.37309846\npctwhite      0.05926563  1.0000000000 -0.64391155 -0.72603098  -0.14608318\npctBlack     -0.01942961 -0.6439115534  1.00000000  0.05756055  -0.07150764\npctHispanic  -0.21839927 -0.7260309801  0.05756055  1.00000000  -0.16352352\npctAAPIalone  0.37309846 -0.1460831806 -0.07150764 -0.16352352   1.00000000\npctunder18   -0.47107334 -0.4866220916  0.28987714  0.52756318  -0.29072583\npctover64    -0.20439363  0.5440272257 -0.22702697 -0.42430798  -0.23991183\nHSorlower    -0.59290214 -0.4609030805  0.16871183  0.56358387  -0.25180077\nBAorhigher    0.67357448  0.3278345358 -0.19790460 -0.42390213   0.38422347\n               pctunder18    pctover64   HSorlower   BAorhigher\nPopulation    0.007594428 -0.020093453 -0.05107553  0.031704662\nmedhhincome  -0.388683691  0.111647722 -0.61927277  0.583703714\nPovertyRate   0.289318732 -0.408377288  0.38428550 -0.244839584\nunemprate     0.426395199 -0.223628488  0.50807715 -0.485070585\nmedvalue     -0.485971837 -0.087907931 -0.59138685  0.716308223\nmedrent      -0.471073339 -0.204393629 -0.59290214  0.673574479\npctwhite     -0.486622092  0.544027226 -0.46090308  0.327834536\npctBlack      0.289877141 -0.227026967  0.16871183 -0.197904602\npctHispanic   0.527563183 -0.424307981  0.56358387 -0.423902133\npctAAPIalone -0.290725828 -0.239911831 -0.25180077  0.384223470\npctunder18    1.000000000 -0.248306018  0.68045309 -0.718007353\npctover64    -0.248306018  1.000000000 -0.14789121 -0.003268322\nHSorlower     0.680453091 -0.147891209  1.00000000 -0.923184800\nBAorhigher   -0.718007353 -0.003268322 -0.92318480  1.000000000\n\n\n\nozs_ma |&gt; select(Population:medrent, pctwhite:BAorhigher) |&gt; \n  na.omit() |&gt; \n  DataExplorer::plot_correlation()\n\n\n\n\nAn additional note to the code above is that we selected several continuous variables that we want to inspect, and removed NA values (use = \"complete.obs\") so that the correlation values can be correctly calculated.\nWhat can we do if we are interested in statistical associations between categorical variables? The typical approach is to use a chi-squared test (see the chisq.test function in the stats package) in conjunction with visual tools like barcharts. We won’t delve deeply into the statistical aspect, but we will learn the useful bar plots for displaying summary statistics of categorical data."
  },
  {
    "objectID": "labs/lab2.html#bar-plot",
    "href": "labs/lab2.html#bar-plot",
    "title": "Exploratory Data Analysis with ",
    "section": "Bar plot",
    "text": "Bar plot\nIn the following code, you can see our familiar group_by + summarise process used to calculate the average median house income by county in Massachusetts. This summarized table is then piped to ggplot() for visualization.\nIf you are unsure about how the output was generated, you can highlight and run part of the code and observate the intermediate results. For example, you can run\n\nozs_ma |&gt; group_by(county, DesignatedOZ) |&gt;summarise( Tracts = n(), Income = mean(medhhincome, na.rm=TRUE)),\n\nsee what it gives you, then add one more line at a time to observe the changes accordingly.\n\nozs_ma |&gt; \n  group_by(county, DesignatedOZ) |&gt;  \n  summarise(\n    Tracts = n(),\n    Income = mean(medhhincome, na.rm=TRUE)) |&gt; \n  ggplot() +\n  geom_col(aes(x = county, y = Income, fill = DesignatedOZ))"
  },
  {
    "objectID": "labs/lab2.html#exercise-3",
    "href": "labs/lab2.html#exercise-3",
    "title": "Exploratory Data Analysis with ",
    "section": "Exercise 3",
    "text": "Exercise 3\nTake a few minutes to read this bar chart below:\n\nThere are a few obvious changes in this graph compared with what we just created before. Overall, it looks nicer…How can we modify our code above to replicate the bar chart in this image? You’ll notice that you can achieve most of the features by tweaking our previous examples, plus a little bit more exploration. In a new code chunk, please copy and paste our last bar chart code, and try your best to address the following questions.\n\nThe stacking is performed automatically by the position= argument in geom_col(). More explainations here. If you don’t want a stacked bar chart, you can use one of the three other options: “identity”, “dodge”, or “fill”.\nThe x-axis labels are titled to 45 degrees. How can I achieve this? Hint.\nThe labels on the y-axis are formatted in thousands with commas. This can be achieved by modifying the function scale_y_continuous(labels = scales::percent) we have seen before. Hint.\nLastly, the counties are not arranged alphabetically, but rather by the income values mapped to the y-axis, starting from large to small. How can I achieve this? Hint.\nPlease add the title, subtitle, x- and y-axis labels, and the data source annotation to your bar chart.\nThe background looks much cleaner. Please choose a theme template for your bar chart."
  },
  {
    "objectID": "labs/lab2.html#optional-scatterplot-with-marginal-histograms",
    "href": "labs/lab2.html#optional-scatterplot-with-marginal-histograms",
    "title": "Exploratory Data Analysis with ",
    "section": "Optional: Scatterplot with marginal histograms",
    "text": "Optional: Scatterplot with marginal histograms\nThis requires a new package ggExtra. But the other syntax should be familiar now.\n\n#install.packages(\"ggExtra\")\np &lt;- ggplot(ozs_ma) + \n  geom_point(aes(x = pctBlack, y = PovertyRate, color = DesignatedOZ)) + \n  theme_bw()\nggExtra::ggMarginal(p, type = \"histogram\", groupFill = TRUE)"
  },
  {
    "objectID": "labs/index.html",
    "href": "labs/index.html",
    "title": "Schedule Overview",
    "section": "",
    "text": "Schedule\nW01 (Oct 23 - Oct 27):\n\nCourse Overview\nLab1: Cambridge Building Energy: R, Quarto, dplyr essentials\n\nW02 (Oct 30 - Nov 3):\n\nExploratory Data Analysis\nLab 2: Opportunity Zones: tidyverse, ggplot2 packages\n\nW03 (Nov 6 - Nov 10):\n\nCensus Data and Demographic Analysis\nLab 3: Racial Distribution: tidycensus, tidyr packages\n\nW04 (Nov 13 – Nov 17):\n\nCreate interactive graphs and maps\nLab 4: Airbnb in Chicago: plotly and leaflet packages\n\nW05 (Nov 20 – Nov 22):\n\nSpatial Analysis; Obtain data from multiple sources\nLab 5: Walkable Environment: sf, osmdata packages\n\nW06 (Nov 27 – Dec 1):\n\nWeb Storytelling I\nLab 6: Build ShinyApp with flexdashboard\n\nW07 (Dec 4 – Dec 8):\n\nWeb Storytelling II\nWork on projects\n\nW08 (Dec 11 – Dec 13):\n\nPresentation"
  },
  {
    "objectID": "howto/setupr.html",
    "href": "howto/setupr.html",
    "title": "Applied Data Science for Cities",
    "section": "",
    "text": "Urban analytics draws upon statistics, visualization, and computation to better understand and ultimately shape cities. This course emphasizes telling stories about cities and neighborhoods covering a set of fundamental concepts of descriptive approaches, quantitative and spatial analysis in R, and principles of reproducible data analysis. Students learn to communicate the results of visualization and analysis for use in decision-making and policy development and to critique those processes."
  },
  {
    "objectID": "howto/setupr.html#this-website-is-under-construction-please-check-back-later-for-updates",
    "href": "howto/setupr.html#this-website-is-under-construction-please-check-back-later-for-updates",
    "title": "Applied Data Science for Cities",
    "section": "This website is under construction, please check back later for updates! 😄",
    "text": "This website is under construction, please check back later for updates! 😄"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Applied Data Science for Cities",
    "section": "",
    "text": "Urban science draws on statistics, visualization, and spatial analysis techniques to gain deeper insights into cities and actively contribute to their development. In this course, we’ll dive into the dynamic world of urban science by learning how to tell stories about cities and neighborhoods, covering a range of topics including demographic analysis, health and transportation, and using R as our primary quantitative analysis and interactive visualization tool.\n\n\n\n\n\n\n\nCourse Information\n\nClass Time: M, W: 9:30-11:00 AM\nLocation: Building 9-450\nCanvas Site: https://canvas.mit.edu/courses/23126"
  },
  {
    "objectID": "labs/lab1.html",
    "href": "labs/lab1.html",
    "title": "Get started with ",
    "section": "",
    "text": "This practice exercise provides some more structured exploration and practice with Quarto Document (R Markdown format). We will mix Markdown sections with code chunks, build familiarity with basic data types, and experiment with importing a tabular dataset. Because this is an in-class exercise, there is nothing you need to submit—the goal is to apply what we have read and seen in the lectures."
  },
  {
    "objectID": "labs/lab1.html#select-selects-a-subset-of-columns.",
    "href": "labs/lab1.html#select-selects-a-subset-of-columns.",
    "title": "Get started with ",
    "section": "Select: selects a subset of columns.",
    "text": "Select: selects a subset of columns.\nWhile in base R, we do:\ndataset[, c(\"Column1\", \"Column2\")]\nIn dplyr, we do:\ndataset |&gt; select(Column1, Column2)\nIn other words, we can simply insert the column names into the select function, without worrying about syntax like indexing, concatenation (c()), and the quotation marks.\nIn the energy dataset, we probably don’t need all of the 46 columns. So we can make it a smaller dataset by specifying a few columns to keep. Insert a new code chunk in your document like this one below. Here we are using the pipe |&gt; operator to “chain together” lines of code. You can type this symbol in using Shift+Ctrl/Command+M.\n\nenergy &lt;- energy |&gt;\n  select(\n    `Data Year`,\n    `BEUDO Category`,\n    Owner,\n    `Year Built`,\n    `Primary Property Type - Self Selected`,\n    `Total GHG Emissions (Metric Tons CO2e)`,\n    `Total GHG Emissions Intensity (kgCO2e/ft2)`,\n    Longitude,\n    Latitude\n  ) \n\nSome of the column names are surrounded by backticks (`), that’s because they include special characters or spaces, which deviate from standard naming conventions. The use of backticks is a means of preserving these unique naming attributes. Just keep typing the column names, dplyr will populate the correct names for you."
  },
  {
    "objectID": "labs/lab1.html#filter-select-a-subset-of-rows",
    "href": "labs/lab1.html#filter-select-a-subset-of-rows",
    "title": "Get started with ",
    "section": "filter: Select a subset of rows",
    "text": "filter: Select a subset of rows\nIn base R, we do this to pick observations by their values:\ndataset[dataset$place == “Boston\", ]\nIn dplyr, we do:\ndataset |&gt; filter(place == “Boston\")\nAgain, a simpler and more understandable syntax.\nNow let’s create a new dataset that only contains energy use records from MIT buildings and that are not missing the total GHG emission attribute. Take a look at how we achieve this using the following code, then proceed to insert a new code chunk in your document like the one below:\n\nmit_energy &lt;- energy |&gt; \n  filter(Owner == \"MASSACHUSETTS INSTITUTE OF TECHNOLOGY\") |&gt; \n  filter(!is.na(`Total GHG Emissions (Metric Tons CO2e)`))"
  },
  {
    "objectID": "labs/lab1.html#mutate-create-or-modify-columns",
    "href": "labs/lab1.html#mutate-create-or-modify-columns",
    "title": "Get started with ",
    "section": "mutate: create or modify columns",
    "text": "mutate: create or modify columns\nWe can change the values in a column either based on specified values or certain transformations. For example, below we are showing how to replace the full name of the institute with “MIT”.\n\nmit_energy |&gt; mutate(Owner = \"MIT\")\n\n# A tibble: 891 × 9\n   `Data Year` `BEUDO Category` Owner `Year Built` Primary Property Type - Sel…¹\n         &lt;dbl&gt; &lt;chr&gt;            &lt;chr&gt;        &lt;dbl&gt; &lt;chr&gt;                        \n 1        2015 Non-Residential  MIT           1994 College/University           \n 2        2017 Residential      MIT           1963 College/University           \n 3        2021 Non-Residential  MIT           2020 Office                       \n 4        2021 Non-Residential  MIT           1983 College/University           \n 5        2017 Non-Residential  MIT           1916 College/University           \n 6        2021 Non-Residential  MIT           1994 College/University           \n 7        2018 Non-Residential  MIT           1931 College/University           \n 8        2021 Non-Residential  MIT           1956 College/University           \n 9        2020 Residential      MIT           1999 Multifamily Housing          \n10        2016 Residential      MIT           1963 College/University           \n# ℹ 881 more rows\n# ℹ abbreviated name: ¹​`Primary Property Type - Self Selected`\n# ℹ 4 more variables: `Total GHG Emissions (Metric Tons CO2e)` &lt;dbl&gt;,\n#   `Total GHG Emissions Intensity (kgCO2e/ft2)` &lt;dbl&gt;, Longitude &lt;dbl&gt;,\n#   Latitude &lt;dbl&gt;\n\n\nYou can also use mutate to add new columns to your data frame that are calculated from existing columns. Here we are showing how to create a new column for building ages.\n\nmit_energy |&gt; mutate(Building_Age = 2023 - `Year Built`)\n\n# A tibble: 891 × 10\n   `Data Year` `BEUDO Category` Owner        `Year Built` Primary Property Typ…¹\n         &lt;dbl&gt; &lt;chr&gt;            &lt;chr&gt;               &lt;dbl&gt; &lt;chr&gt;                 \n 1        2015 Non-Residential  MASSACHUSET…         1994 College/University    \n 2        2017 Residential      MASSACHUSET…         1963 College/University    \n 3        2021 Non-Residential  MASSACHUSET…         2020 Office                \n 4        2021 Non-Residential  MASSACHUSET…         1983 College/University    \n 5        2017 Non-Residential  MASSACHUSET…         1916 College/University    \n 6        2021 Non-Residential  MASSACHUSET…         1994 College/University    \n 7        2018 Non-Residential  MASSACHUSET…         1931 College/University    \n 8        2021 Non-Residential  MASSACHUSET…         1956 College/University    \n 9        2020 Residential      MASSACHUSET…         1999 Multifamily Housing   \n10        2016 Residential      MASSACHUSET…         1963 College/University    \n# ℹ 881 more rows\n# ℹ abbreviated name: ¹​`Primary Property Type - Self Selected`\n# ℹ 5 more variables: `Total GHG Emissions (Metric Tons CO2e)` &lt;dbl&gt;,\n#   `Total GHG Emissions Intensity (kgCO2e/ft2)` &lt;dbl&gt;, Longitude &lt;dbl&gt;,\n#   Latitude &lt;dbl&gt;, Building_Age &lt;dbl&gt;\n\n\nThere is no &lt;- operator in these two code chunks. We are observing the results but the resulting tables are not saved in an object."
  },
  {
    "objectID": "labs/lab1.html#group_by-summarise",
    "href": "labs/lab1.html#group_by-summarise",
    "title": "Get started with ",
    "section": "Group_by + Summarise",
    "text": "Group_by + Summarise\nSummarise is usually used in conjunction with group_by because the latter changes the scope from operating on the entire dataset to operating on it group-by-group. Go ahead and run the following code:\n\nmit_energy |&gt; \n  group_by(`Data Year`) |&gt; \n  summarise(count = n())\n\n# A tibble: 7 × 2\n  `Data Year` count\n        &lt;dbl&gt; &lt;int&gt;\n1        2015   129\n2        2016   134\n3        2017   130\n4        2018   134\n5        2019   111\n6        2020   115\n7        2021   138\n\n\nWe use group_by such that observations (i.e., rows) are grouped according to Data Year, which is the year when the energy record was taken. The result is then passed to summarise to generate a total number of records per year. By default, the n() function creates a new attribute (i.e., column), which we here name as “count”.  \nBelow we are using the same group_by + summarise chain to calculate the average GHG emissions of all buildings, and the average GHG emission intensity (use the column Total GHG Emissions Intensity (kgCO2e/ft2)). Note that we are now giving new names to each of the new columns.\n\nmit_energy |&gt; \n  group_by(year = `Data Year`) |&gt; \n  summarise(count = n(),\n            avg_emission = mean(`Total GHG Emissions (Metric Tons CO2e)`),\n            avg_intensity = mean(`Total GHG Emissions Intensity (kgCO2e/ft2)`))\n\n# A tibble: 7 × 4\n   year count avg_emission avg_intensity\n  &lt;dbl&gt; &lt;int&gt;        &lt;dbl&gt;         &lt;dbl&gt;\n1  2015   129        1585.          13.4\n2  2016   134        1445.          13.3\n3  2017   130        1523.          13.5\n4  2018   134        1472.          13.2\n5  2019   111        1522.          12.8\n6  2020   115        1396.          11.0\n7  2021   138        1415.          11.5\n\n\n\nYour practice\nInsert a few new code chunks below this one to document your code and show your results. \n\nFrom the mit_energy dataset, create a subset of all non-residential buildings, which were built before the year 2000. (Hint: which function would you use?). How many such buildings are there?\nFrom the mit_energy dataset, compare the GHG emissions by property type (Hint: which column has that information?), and generate a table that shows the following results:\n\n\nYou can create this table mostly by modifying the last code chunk (labeled “annual_mean”), however, there are a few small things you can experiment on:\n\nThe calculated average numbers in this table are rounded to 2 decimals, how to achieve that?\nThe table is arranged in descending order based on the “avg_emission” column, how to do that? (Hint)\n\nWe are already trying to ask questions and find hints of interesting stories from the dataset, which is what Exploratory Data Analysis (EDA) is all about. If the results so far look interesting/surprising/expected to you, write a few sentences describing what you see from the analysis.\n\nLastly, we will insert a map to complete your working document! This dataset includes “Longitude” and “Latitude” columns, which I like because it indicates that location information is available and can be visualized.\nAdd the following code to your document, and you should be able to run it and see a map. (If your R says it can’t find mapview, run the line install.packages(\"mapview\"))\n\n\n#install.packages(\"mapview\")\nlibrary(mapview)\nmapview(\n  mit_energy,\n  xcol = \"Longitude\", ycol = \"Latitude\",\n  crs = 4326,\n  grid = FALSE\n)\n\n\n\n\n\n\n\nNow Save, Render your document again. You have now created a pretty, multi-media document using R!\n------\nIn this lab we have introduced how to create and develop a Quarto Document. We have also introduced some commonly-used dplyr funtions including select, filter, mutate, group_by and summarise. This is the beginning of our data wrangling and leads to the work in Week 2."
  },
  {
    "objectID": "labs/lab3.html",
    "href": "labs/lab3.html",
    "title": "Download and analyze Census data with ",
    "section": "",
    "text": "Today we have a two-part lab, in which we are going to use American Community Survey (ACS) data to approach a classic planning question: what is the racial distribution like in my study area? We will address this question by creating both visual representations and numerical indicators, two potential research directions involving Census data in R.\nIn the first part, we are going to focus our study on the City of Chicago, which is situated in Cook County, Illinois. The tract-level Census data is available for states and counties but not for individual cities. Therefore, we will need to extract data for Chicago from the broader dataset encompassing Cook County, taking advantage of the spatial data object returned by tidycensus.\nIn the second part, we will examine racial segregation in Chicago. Using available Census data, we can apply segregation indicators to assess the degree of racial group segregation within given regions. This involves continuously using and extending the fundamental dplyr data wrangling skills.\n\n# You may need to install the packages: tigris, sf and gt\nlibrary(tidyverse)\nlibrary(tidycensus)\nlibrary(tigris)\nlibrary(sf)\nlibrary(gt)"
  },
  {
    "objectID": "labs/lab3.html#construct-multiple-variables",
    "href": "labs/lab3.html#construct-multiple-variables",
    "title": "Download and analyze Census data with ",
    "section": "Construct multiple variables",
    "text": "Construct multiple variables\nIn most cases, our data requirements from the Census won’t be limited to just two variables. A common practice is to save a number of variables (possibly spanning multiple tables) as a single object and then insert this object into the download call. See the following code. Notice that we can also rename the IDs into something more descriptive during this process.\n\nvars &lt;- c(\n  White = \"B03002_003\",\n  Black = \"B03002_004\",\n  AIAN = \"B03002_005\",\n  Asian = \"B03002_006\",\n  NHPI = \"B03002_007\",\n  Hispanic = \"B03002_012\",\n  Totpop = \"B03002_001\"\n)\n\nGo ahead and perform the get_acs with the input vars.\n\ncook_race &lt;- get_acs(\n  geography = \"tract\",\n  state = \"IL\",\n  county = \"Cook\",\n  variables = vars,\n  output = \"wide\", \n  geometry = TRUE) \n\nWe can use dplyr::select to slim down this dataset by excluding the margin-of-error columns, we can select columns whose names have a pattern that ends_with(\"E\").\n\ncook_race &lt;- \n  cook_race |&gt; \n  select(GEOID, NAME, ends_with(\"E\"))"
  },
  {
    "objectID": "labs/lab3.html#modify-geographies",
    "href": "labs/lab3.html#modify-geographies",
    "title": "Download and analyze Census data with ",
    "section": "Modify Geographies",
    "text": "Modify Geographies\nPlaces like Chicago cannot be directly inserted into the get_acs argument as it is not a county. How is Chicago located within Cook County exactly? We can use another census-related package tigris for a visual check of the place boundaries. tigris fetches census geographics. You can think of it as a programmatic way of downloading TIGER/Line Shapefiles.\nThe function place fetches census-designated place. The city of Chicago is one of such places in Illinois.\n\noptions(tigris_use_cache=TRUE) # This is to allow tigris to use caching for downloaded data so that we don't need to fetch the same data again every time you need it.\n\n# Download the boundary of Chicago. \nchi_boundary &lt;- places(state = \"IL\") |&gt; \n  filter(NAME == \"Chicago\")\n\nNow let’s plot the two shapefiles together to see the overlay:\n\nggplot()+\n  geom_sf(data = cook_race)+\n  geom_sf(data = chi_boundary, color = \"blue\", linewidth = 0.5, fill = NA)\n\n\n\n\nBoth the Census table and geography we have obtained are sf objects. An extensive vignette with examples using sf may be found here. In this lab, we will only use the st_intersection function to perform a geometric intersection between two spatial objects.\nIn the following code, we will extract tract-level racial data that fall within the boundary of the City of Chicago. The modified result will be saved to a new name chicago_race.\n\nchicago_race &lt;- \n  st_intersection(cook_race, chi_boundary)\n\nst_intersection returns a result that combines the attributes of both input tables. That is why we have a table that has 26 columns as opposed to 10. Let’s do another round of select , then this dataset is good to go.\n\nchicago_race &lt;- chicago_race |&gt; select(GEOID:TotpopE)\n\nPause here and use saveRDS function to save the chicago_race dataset to your data folder. We want to keep an intermediate result so that you can always come back. A quick check - it now has 862 rows and 10 columns.\nsaveRDS(chicago_race, \"data/&lt;your file name&gt;.rds\")\nIf you were wondering what the file type .rds is, it’s a more flexible way to save and load R objects compared to other formats like CSV or plain text files, which may lose some data structure information (such as spatial information).\nNote: At this point, you can also save chicago_race as a shapefile, which is fully compatible with GIS software. Even if you aren’t conducting extensive spatial analysis within R, you can choose to download data in R and then transfer it to your GIS software of choice for further geospatial processing and visualization. The function is st_write(chicago_race, \"data/&lt;your file name&gt;.shp\")"
  },
  {
    "objectID": "labs/lab3.html#join-another-census-data",
    "href": "labs/lab3.html#join-another-census-data",
    "title": "Download and analyze Census data with ",
    "section": "Join another Census data",
    "text": "Join another Census data\nWide format data has unique GEOIDs, and thus can be easily joined with another table. A simple measure of racial wealth disparity is to look at how income levels and racial backgrounds are spread among tracts. We can go back to download median household income data and join it with our existing racial dataset.\n\ncook_income &lt;- get_acs(\n  geography = \"tract\",\n  state = \"IL\",\n  county = \"Cook\",\n  variables = \"B19013_001\",\n  output = \"wide\") \n\nWe are going to use a new function left_join from the dplyr package. You can read about different joining types and illustrative examples here. But simply put, we are trying to combine two datasets (x and y) based on a shared column. A left join keeps all the rows from x. If a value in the key column in x doesn’t exist in y, the row will contain NA values for all the y columns.\n\njoined_data &lt;- \n  left_join(chicago_race, cook_income, by=\"GEOID\")\n\n\nggplot(joined_data)+\n  geom_point(aes(x=WhiteE/TotpopE, y=B19013_001E), cex = .5, alpha = .2, na.rm = TRUE) +\n  labs(title = \"Tract Median Household Income vs White Population\", x = \"Percent White\", y = \"Median Household Income\") +\n  scale_x_continuous(labels = scales::percent)+\n  scale_y_continuous(labels = scales::comma)+\n  theme_minimal()\n\n\n\n\nIf you are curious, you can modify the plot to show the relationship between income and the percentage of other racial groups."
  },
  {
    "objectID": "labs/lab3.html#reshape-data-from-wide-to-long-format",
    "href": "labs/lab3.html#reshape-data-from-wide-to-long-format",
    "title": "Download and analyze Census data with ",
    "section": "Reshape data from wide to long format",
    "text": "Reshape data from wide to long format\nWe now have estimates of each group captured in separate columns, along with a total population column. We could make six new columns to calculate and store the percentages for racial groups, but it would make the dataset a bit cumbersome.\nWe have two functions in tidyr to help us transform the dataset structure: pivot_wider and pivot_longer. For example, using pivot_longer, we can stack the six variables and make multiple rows for each unique identifier.\nThe essential arguments of pivot_longer are:\n\ncols: the portion of the dataset you want to modify. For instance, here we will leave the TotpopE as its own column and only modify the racial group columns.\nnames_to and values_to: conceptually we need two new columns, one for stacking the racial group names and another for the corresponding values. Here we are giving the former a name called “racial_groups”, and the latter a name called “estimate”. The names are totally your choice.\n\n\nchi_race_long &lt;- chicago_race |&gt; \n  pivot_longer(\n    cols = c(WhiteE:HispanicE),\n    names_to = \"racial_groups\",\n    values_to = \"estimate\"\n  )\n\nFrom here, calculating the population rate will be much easier. What we will do in the following code are:\n\nuse mutate to calculate the race-specific population rate (divide the racial group population column by the total population column),\nuse select to select the variables we need and remove the others\n\nMake sure to examine each row and figure out how they impact the results.\n\nchi_race_long &lt;- \n  chi_race_long |&gt; \n  mutate(percent = round((estimate/TotpopE)*100, 2)) |&gt; \n  select(GEOID, NAME, racial_groups, TotpopE, estimate, percent) \n\nThe racial_groups column has a character “E” at the end of each racial group name, carried over from the original Census data. This is something we can fix using a string modification function str_sub. We will keep all characters from the beginning (the start of the string) up to the second-to-last character. So, it essentially removes the last character from each string.\n\nchi_race_long &lt;- chi_race_long |&gt; \n  mutate(racial_groups = str_sub(racial_groups, end = -2))\n\nA quick check of what your dataset chi_race_long should look like at this step. If you are unsure about your result, you can read your stored RDS data again, then run the code chunks labeled “data-wrangling-1” through “data-wrangling-3”.\n\nchi_race_long |&gt; head(3) |&gt; gt()\n\n\n\n\n\n  \n    \n    \n      GEOID\n      NAME\n      racial_groups\n      TotpopE\n      estimate\n      percent\n      geometry\n    \n  \n  \n    17031230200\nCensus Tract 2302, Cook County, Illinois\nWhite\n1979\n709\n35.83\nlist(c(-87.713305, -87.711901, -87.706877, -87.706765, -87.70686, -87.70855, -87.71018, -87.713417, -87.713305, 41.910037, 41.910059, 41.910147, 41.910714, 41.913716, 41.913705, 41.91363, 41.913589, 41.910037))\n    17031230200\nCensus Tract 2302, Cook County, Illinois\nBlack\n1979\n164\n8.29\nlist(c(-87.713305, -87.711901, -87.706877, -87.706765, -87.70686, -87.70855, -87.71018, -87.713417, -87.713305, 41.910037, 41.910059, 41.910147, 41.910714, 41.913716, 41.913705, 41.91363, 41.913589, 41.910037))\n    17031230200\nCensus Tract 2302, Cook County, Illinois\nAIAN\n1979\n0\n0.00\nlist(c(-87.713305, -87.711901, -87.706877, -87.706765, -87.70686, -87.70855, -87.71018, -87.713417, -87.713305, 41.910037, 41.910059, 41.910147, 41.910714, 41.913716, 41.913705, 41.91363, 41.913589, 41.910037))"
  },
  {
    "objectID": "labs/lab3.html#create-a-faceted-map",
    "href": "labs/lab3.html#create-a-faceted-map",
    "title": "Download and analyze Census data with ",
    "section": "Create a faceted map",
    "text": "Create a faceted map\nIn our long-format data table, racial group names are consolidated within a single column. We can create a map that visualizes the distribution of the percent attribute while categorizing it based on the racial_groups attribute.\n\nchi_race_long |&gt;\n  ggplot() +\n    geom_sf(aes(fill = percent, color = percent)) +\n    facet_wrap( ~ racial_groups) +\n    coord_sf(datum = NA) + \n    theme_minimal()\n\n\n\n\nThe facet_wrap in the code takes one categorical variable as its input (racial_groups) and breaks the data into subsets accordingly. The ~ here is a shorthand for “relative to”, meaning I’m gonna split the whole data based on their relationship to racial_groups."
  },
  {
    "objectID": "labs/lab3.html#exercise-1",
    "href": "labs/lab3.html#exercise-1",
    "title": "Download and analyze Census data with ",
    "section": "Exercise 1",
    "text": "Exercise 1\nNow that you have some new tools that you are familiar with, please start a new .qmd or .Rmd Document, and try your hand at the following:\n1) Create a dataset for the City of Chicago that shows the percentage of households that are experiencing housing cost burden (i.e. spending 30 percent or more of income on housing costs), grouped by housing ownership (owner-occupied or renter-occupied).\n\nThe universe of this characteristic is reflected in the B25106 table.\nIf we want a count of all owner-occupied households experiencing cost burden, we need to sum these variables: B25106_006, B25106_010, B25106_014, B25106_018, B25106_022. If we want a count of all renter-occupied households experiencing cost burden, we need to sum up B25106_028, B25106_032, B25106_036, B25106_040, B25106_044.\nThe total number of owner-occupied households is B25106_002; The total number of renter-occupied households is B25106_024.\n\n2) Create maps depicting the percentages of households experiencing housing cost burdens in both owner-occupied and renter-occupied housing.\n\nYou can choose to create two individual maps, or one faceted map. But in either case, your calculated percentage values should go to the geom_sf(fill = ) argument.\nVisually compare the housing cost burden maps and the racial distribution maps, and briefly provide your interpretations."
  },
  {
    "objectID": "labs/lab3.html#indicators-for-measuring-segregation",
    "href": "labs/lab3.html#indicators-for-measuring-segregation",
    "title": "Download and analyze Census data with ",
    "section": "Indicators for measuring segregation",
    "text": "Indicators for measuring segregation\n\nDissimilarity\nDissimilarity is a measure of evenness between two populations. Conceptually, dissimilarity represents the proportion of a group that would need to move in order to create a uniform distribution of population.\nA dissimilarity score of 0 would mean a completely even distribution of two populations (no segregation). A dissimilarity score of 1 would mean a completely segregated minority population (100% of the minority population would need to move to achieve an even distribution among the population. Dissimilarity is calculated as follows:\n\\[ D = 0.5\\Sigma_{i=1}^{n}|\\frac{w_{i}}{W}-\\frac{b_{i}}{B}| \\]\nIf we are interested in measuring non-Hispanic Black-White Dissimilarity, then:\n\n\\(w_{i}\\) is the White population of tract i\n\\(W\\) is the total number of White population in the region;\n\\(b{i}\\) is the Black population in tract i;\n\\(B\\) is the total number of Black population in the region;\n\nWith the dataset structure we have constructed, calculating D can be broken down into a series of codable parts: 1) Calculate fractions inside the absolute value for each row; 2) use abs() to find the absolute value; 3) use group_by() to sum up the data for each region; 4) multiply the numbers by 0.5.\n\ndissimilarity &lt;- chi_race_region |&gt; \n  mutate(Dissimilarity = abs(Black / sum_Black - White / sum_White)) |&gt; \n  group_by(Region) |&gt; \n  summarise(Dissimilarity = round(0.5 * sum(Dissimilarity), 3))\n\nCreate a more presentable table:\n\ndissimilarity |&gt;  gt()\n\n\n\n\n\n  \n    \n    \n      Region\n      Dissimilarity\n    \n  \n  \n    CENTRAL\n0.489\n    FAR SOUTH\n0.829\n    NORTH\n0.542\n    NORTHWEST\n0.449\n    SOUTHEAST\n0.702\n    SOUTHWEST\n0.827\n    WEST\n0.772\n  \n  \n  \n\n\n\n\ngt is powerful tool for creating elegant and customizable tables with ease. For instance, we can add a few more lines to enhance this table by adding colorization based on the values.\n\ndissimilarity |&gt;\n  gt() |&gt;\n  tab_header(\"Black-White Dissimilarity in Chicago Regions\") |&gt;\n  data_color(\n    columns = `Dissimilarity`,\n    colors = scales::col_numeric(\n    palette = c(\"white\", \"darkred\"),\n    domain  = c(0, 1)\n    )\n  )\n\n\n\n\n\n  \n    \n      Black-White Dissimilarity in Chicago Regions\n    \n    \n    \n      Region\n      Dissimilarity\n    \n  \n  \n    CENTRAL\n0.489\n    FAR SOUTH\n0.829\n    NORTH\n0.542\n    NORTHWEST\n0.449\n    SOUTHEAST\n0.702\n    SOUTHWEST\n0.827\n    WEST\n0.772\n  \n  \n  \n\n\n\n\n\n\nInteraction\nA second common measure of segregation is interaction, which assesses isolation levels rather than spatial distribution directly. Such indices typically estimate the likelihood of individuals from one group encountering or interacting with members of another group, based upon their distribution within areal subunits (tracts). For instance, non-Hispanic Black-White interaction is calculated as follows:\n\\[\nI=\\Sigma_{i=1}^{n}(\\frac{b_{i}}{B})(\\frac{w_{i}}{t_{i}})\n\\]\nwhere:\n\n\\(b_{i}\\) is the Black population of tract i;\nB is the Black population of the region;\n\\(w_{i}\\) is the White population of tract i;\n\\(t_{i}\\) is the total population of tract i.\n\n\ninteraction &lt;- chi_race_region |&gt; \n  mutate(Interaction = (Black/sum_Black) * (White/Totpop)) |&gt; \n  group_by(Region) |&gt; \n  summarise(Interaction = sum(Interaction, na.rm = TRUE) |&gt; round(3))\n\nThe maximum value of I depends both on the distribution of ethnic groups AND on the proportion of minorities in the city. But generally speaking, higher values mean two groups are more likely to be evenly distributed, and lower values mean a more segregated minority population.\nCreate a similar table for the non-Hispanic Black-White interaction index:\n\ninteraction |&gt;\n  gt() |&gt;\n  tab_header(\"Black-White Interaction in Chicago Regions\") |&gt;\n  data_color(\n    columns = `Interaction`,\n    colors = scales::col_numeric(\n    palette = c(\"darkred\", \"white\"),\n    domain  = c(0, 1)\n    )\n  )\n\n\n\n\n\n  \n    \n      Black-White Interaction in Chicago Regions\n    \n    \n    \n      Region\n      Interaction\n    \n  \n  \n    CENTRAL\n0.423\n    FAR SOUTH\n0.053\n    NORTH\n0.449\n    NORTHWEST\n0.405\n    SOUTHEAST\n0.038\n    SOUTHWEST\n0.049\n    WEST\n0.071"
  },
  {
    "objectID": "labs/lab3.html#exercise-2",
    "href": "labs/lab3.html#exercise-2",
    "title": "Download and analyze Census data with ",
    "section": "Exercise 2",
    "text": "Exercise 2\nHow would you interpret these dissimilarity and interaction values? Where are the indices the highest within the region, and where are they the lowest? Does the result seem to confirm the racial distribution map we produced in the first part?\nThe same methods can be used for any other two groups, such as Nonwhite-White, or Latino-White. You have the option to experiment with these values, but for this lab, I only request interpretation/diagnosis using the measure of segregation 😄."
  },
  {
    "objectID": "labs/lab5.html",
    "href": "labs/lab5.html",
    "title": "Describe neighborhood dynamics with ",
    "section": "",
    "text": "Geospatial data analysis figures prominently in cities and urban analytics. In this lab, we will introduce essential tools and processes to facilitate spatial analysis, along with demonstrating how to obtain and integrate data from open data sources.\nTo put things into context, we are going to describe walkable environment in Boston neighborhoods. Walkability is regarded as whether characteristics of the built environment may or may not support residents to walk for either leisure or access destinations (Leslie et al., 2007). Some studies have further pinpointed several variables within a framework of three D’s supporting walkability: Density, Diversity, and Design (Cervero and Kockelman, 1997, Ewing et al., 2008), and later, five D’s that further include Destination accessibility and Diversity (Ewing & Cervero, 2010).\n\nlibrary(sf)\nlibrary(tidyverse)\nlibrary(leaflet)\nlibrary(osmdata)\nlibrary(scales)"
  },
  {
    "objectID": "labs/lab5.html#density-of-sidewalks",
    "href": "labs/lab5.html#density-of-sidewalks",
    "title": "Describe neighborhood dynamics with ",
    "section": "Density of sidewalks",
    "text": "Density of sidewalks\nFrom the front page of Analyze Boston, search for “sidewalk”, you will find the Sidewalk Centerline shapefile. Download it to your data folder, unzip it and read it in R. What CRS is this shapefile using?\n\nsidewalk &lt;- st_read(\"data/Sidewalk_Centerline.shp\")\n\nIf we intersect the sidewalk and neighborhood shapefiles, we should be able to calculate the total length of sidewalk segments in each neighborhood. Similar to st_area, st_length is another geometry measurement function and calculates segment length.\nIn the following code, we are going to:\n\nintersect the sidewalk and neighborhood spatial objects, i.e. break up line segments at neighborhood boundaries;\ncalculate the lengths of sidewalk segments after the intersection;\nconvert the units to numeric values;\ngroup the line segments by neighborhood, summing up the sidewalk lengths within each neighborhood.\n\n\nsidewalk_data &lt;- \n  st_intersection(sidewalk, neighborhood) |&gt; \n  mutate(length = as.numeric(st_length(geometry))) |&gt; \n  group_by(nbh_name) |&gt; \n  summarise(sidewalk_length = sum(length)) \n\nTo calculate sidewalk coverage we need to join sidewalk_length with neighborhood, using the shared attribute nbh_name in these two datasets. Recall that operations on attributes do not remove their geometry - but we can explicitly drop it using st_drop_geometry()\n\nsidewalk_data &lt;-  sidewalk_data |&gt; st_drop_geometry() |&gt; \n  left_join(neighborhood, by = \"nbh_name\") \n\nThen we’ll calculate sidewalk density using another pipeline:\n\nsidewalk_data &lt;- \n  sidewalk_data |&gt; \n  mutate(sidewalk_coverage = sidewalk_length/nbh_area) |&gt; \n  select(nbh_name, sidewalk_coverage)\n\nAt this point, you can remove some datasets that you don’t anticipate using again.\n\nrm(sidewalk)"
  },
  {
    "objectID": "labs/lab5.html#calculate-the-number-of-pois-by-neighborhood",
    "href": "labs/lab5.html#calculate-the-number-of-pois-by-neighborhood",
    "title": "Describe neighborhood dynamics with ",
    "section": "Calculate the number of POIs by neighborhood",
    "text": "Calculate the number of POIs by neighborhood\nOnce the three OSM variables are generated, we will count the number of points within each neighborhood. In the following code, st_intersects returns a list of 24 (the number of neighborhoods), in which each element contains IDs of the points falling within this polygon. We then apply lengths() to the elements of st_intersects to count the number of IDs. (lengths is a base R function, it’s not calculating spatial lengths, but getting the length of elements of a list).\n\n# The `bind_cols` brings multiple vectors together to make a data frame\nosm_data &lt;- bind_cols(neighborhood$nbh_name,\n  lengths(st_intersects(neighborhood, restaurant)),\n  lengths(st_intersects(neighborhood, fastfood)),\n  lengths(st_intersects(neighborhood, retail)),\n  lengths(st_intersects(neighborhood, supermarket))\n)\n\ncolnames(osm_data) &lt;- c(\"nbh_name\", \"restaurant\", \"fastfood\", \"retail\", \"supermarket\")\n\nThen we will sum them up to have the total number of business-related POIs in each neighborhood, then factor in the population size to normalize the raw count.\n\nosm_data &lt;- osm_data |&gt; \n  left_join(neighborhood, by = \"nbh_name\") |&gt; \n  mutate(poi_density = ifelse(population&gt;0, \n                              (restaurant+fastfood+retail+supermarket)/(population/1000), \n                              NA)) |&gt; \n  select(nbh_name, poi_density)"
  },
  {
    "objectID": "labs/lab5.html#assemble-results",
    "href": "labs/lab5.html#assemble-results",
    "title": "Describe neighborhood dynamics with ",
    "section": "Assemble results",
    "text": "Assemble results\nNow we will put the three parts of our result (population density, sidewalk coverage, and POI data) in one place. The three datasets we have created share the common attribute nbh_name. We can perform left_join() on two datasets at a time, but an alternative strategy is to use the purrr package’s reduce() function to combine multiple datasets at one time.\n\ndata &lt;- list(neighborhood, sidewalk_data, osm_data) |&gt; \n  reduce(left_join, by = \"nbh_name\")\n\nWe have finally compiled the necessary dataset that incorporates all the indicators we have identified! Now let’s remove intermediary variables in the working environment, keeping only the data and prepare to create the index.\n\nrm(list=setdiff(ls(), \"data\"))\ndata &lt;- data |&gt; select(-c(population, nbh_area))"
  },
  {
    "objectID": "labs/lab5.html#standardize-indicators",
    "href": "labs/lab5.html#standardize-indicators",
    "title": "Describe neighborhood dynamics with ",
    "section": "Standardize indicators",
    "text": "Standardize indicators\nSince our indicators end up having very different scales, we can’t interpret anything about the magnitude by comparing these values to each other. In many statistical analyses, such as multivariate linear regression, standardizing your data is usually an essential first step to prevent certain variables from dominating the analysis due to their larger numerical values.\nZ-score standardization converts data from various distributions into a common unit of measurement, which is the number of standard deviations from the mean. This conversion enables data points, regardless of their original spread, to be expressed in the same way.\nA z-score can be calculated by subtracting from a given observation the mean of all observations and then dividing by the standard deviation of all observations. R has us covered here - the scale function does exactly the same thing. Here we use a mutate_at function to perform the same alteration upon multiple variables we select. Take a look at the documentation for mutate_at, the list argument allows you to modify existing columns in place.\n\nscore &lt;-\n  data |&gt;\n  mutate_at(\n    vars(pop_density, sidewalk_coverage, poi_density),\n    list(scale)\n  )"
  },
  {
    "objectID": "labs/lab5.html#specify-the-directions",
    "href": "labs/lab5.html#specify-the-directions",
    "title": "Describe neighborhood dynamics with ",
    "section": "Specify the directions",
    "text": "Specify the directions\nHigh values in indicators may represent beneficial or detrimental conditions - in some cases, higher values are “good”, and in some cases, lower values are “good”. When we are creating an additive index, we can transform values so that they are moving in the same direction by simply switching the sign on the values that need to be reversed (e.g. multiply by -1). However, we don’t need to adjust directions in our case because our indicators move in the same direction (i.e. high values mean a more walkable environment)."
  },
  {
    "objectID": "labs/lab5.html#aggregate-the-index",
    "href": "labs/lab5.html#aggregate-the-index",
    "title": "Describe neighborhood dynamics with ",
    "section": "Aggregate the index",
    "text": "Aggregate the index\nNow we are going to generate our composite index, where standardized values are simply added together.\n\nscore &lt;- score |&gt;\n  rowwise() |&gt;\n  mutate(\n    index = sum(pop_density, sidewalk_coverage, poi_density))|&gt; \n  ungroup()\n\nYou may have noticed rowwise(). Typically, if we were to ask dplyr to mutate by providing a sum, it would do so by column. rowwise() modifies this and asks for something to happen across a data observation (row) instead of by column.\nWe could analyze the index value and interpret it as is, but for an index, it’s reasonable to anticipate a value falling within the range of 0 to 100. We can use rescale to put our values into any specified range.\n\nscore &lt;-\n  score |&gt;\n  mutate(index = rescale(index, to = c(0, 100)))"
  },
  {
    "objectID": "labs/lab5.html#check-the-result",
    "href": "labs/lab5.html#check-the-result",
    "title": "Describe neighborhood dynamics with ",
    "section": "Check the result",
    "text": "Check the result\nWe can make a quick map to examine our results. Although, we don’t have to over-interpret our results because any method to evaluate walkability is a difficult one. OpenStreetMap also exhibits issues such as missing names, data availability biases, inconsistent categorization, etc. This study chose a more pragmatic method to demonstrate the capabilities of R in data management and spatial analysis.\n\nggplot(score) + \n  geom_sf(aes(fill = index))+\n  scale_fill_continuous()\n\n\n\n\n\nOptional:\nIn response to suggestions - here are some other approaches for presenting our results! For example, a formatted and sortable table:\n\n# Rescore the sub-dimensional indices to 0-100 as well\nscore &lt;-\n  score |&gt;\n  mutate_at(vars(pop_density, sidewalk_coverage, poi_density),\n    ~rescale(., to = c(0, 100))) |&gt; \n  st_drop_geometry()\n\nlibrary(gt)\nscore|&gt; \n  modify_if(~is.numeric(.), ~round(., 2)) |&gt; \n  gt() |&gt; \n  tab_style(\n    locations = cells_body(columns = nbh_name),\n    style = cell_text(weight = \"bold\")\n  ) |&gt;\n  opt_interactive(\n    use_compact_mode = TRUE,\n    use_text_wrapping = FALSE\n  )\n\n\n\n\n\n\n\n\n\nOr, we can make some radar charts:\n\nnbh = \"Downtown\"\n# The radar chart requires only value columns\ntemp &lt;- score |&gt; \n  filter(nbh_name == nbh)|&gt; \n  select(pop_density, sidewalk_coverage, poi_density)\n\n# I also have to manually add 2 new row: the max and min of each indicator\ntemp &lt;- rbind(rep(100, 3), rep(0, 3), temp) \n\nlibrary(fmsb)\nradarchart(temp, title = nbh)"
  }
]